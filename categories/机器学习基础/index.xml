<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习基础 on LeoDreamer</title>
        <link>https://LeoDreamer2004.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link>
        <description>Recent content in 机器学习基础 on LeoDreamer</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>LeoDreamer</copyright>
        <lastBuildDate>Tue, 11 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://LeoDreamer2004.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习基础(3) —— 基于后验概率最大化准则的分类模型</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/bayers/</link>
        <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/bayers/</guid>
        <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://LeoDreamer2004.github.io/styles/notes.css&#34; &gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-3.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;后验概率最大化准则&#34;&gt;后验概率最大化准则
&lt;/h2&gt;&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对训练样本集 $D=\{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathcal{X}$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_K\}$, 将 $x$ 的类别预测为 $c_i$ 的 &lt;strong&gt;风险&lt;/strong&gt; 为&lt;/p&gt;
$$
R(Y=c_i | x) = \sum_{j=1}^K \lambda_{ij} P(Y=c_j | x)
$$&lt;p&gt;其中 $\lambda_{ij}$ 是将属于 $c_j$ 的样本预测为 $c_i$ 的损失. &lt;strong&gt;最优预测&lt;/strong&gt; $\hat{y}$ 是使得风险最小的类别, 即&lt;/p&gt;
$$
\hat{y} = \arg\min_{c_i} R(Y=c_i | x)
$$&lt;/div&gt;
&lt;p&gt;假设采用 $0-1$ 损失函数, 易知&lt;/p&gt;
$$
R(Y=c_i | x) = 1 - P(Y=c_i | x)
$$&lt;p&gt;即输入 $x$ 的最优预测 $\hat{y}$ 为使得后验概率 $P(y | x)$ 最大的类别.&lt;/p&gt;
&lt;h2 id=&#34;逻辑斯蒂回归模型&#34;&gt;逻辑斯蒂回归模型
&lt;/h2&gt;&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}=\mathbb{R}^n, \mathcal{Y}=\{c_1,c_2\}$, 逻辑斯蒂回归模型是如下的后验概率分布:&lt;/p&gt;
$$
\begin{aligned}
P(Y=c_1 | x) &amp;= \frac{\exp(w \cdot x + b)}{1+\exp(w \cdot x + b)} \\
P(Y=c_2 | x) &amp;= \frac{1}{1+\exp(w \cdot x + b)
}
\end{aligned}
$$&lt;p&gt;其中 $w,b$ 是模型参数.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;按照后验概率最大化准则, 显然当 $w \cdot x + b &gt; 0$ 时, 预测为 $c_1$, 否则预测为 $c_2$.&lt;/p&gt;
&lt;p&gt;对于多类分类任务, 仍然可以使用逻辑斯蒂回归模型:&lt;/p&gt;
$$
\begin{aligned}
p(y=c_i | x) &amp;= \frac{\exp(w_i \cdot x + b_i)}{\sum_{j=1}^{K-1} \exp(w_j \cdot x + b_j)}, \quad i=1,2,\cdots,K-1 \\
p(y=c_K | x) &amp;= \frac{1}{\sum_{j=1}^{K-1} \exp(w_j \cdot x + b_j)}
\end{aligned}
$$&lt;p&gt;给定 $D=\{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{0,1\}$, 用 $\theta=(w,b)$ 表示二项逻辑斯蒂回归模型的参数, 令&lt;/p&gt;
$$
p(x;\theta) = p(Y=1 | x;\theta)
$$&lt;p&gt;则考虑似然函数为&lt;/p&gt;
$$
\begin{aligned}
L(\theta) &amp;= \prod_{i=1}^n p(x_i;\theta)^{y_i} (1-p(x_i;\theta))^{1-y_i} \\
\log L(\theta) &amp;= \sum_{i=1}^n y_i \log p(x_i;\theta) + (1-y_i) \log (1-p(x_i;\theta)) \\
&amp;= \sum_{i=1}^N y_i(w \cdot x_i + b) - \log(1+\exp(w \cdot x_i + b))
\end{aligned}
$$&lt;p&gt;对 $w,b$ 求偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial \log L(\theta)}{\partial w} &amp;= \sum_{i=1}^n x_i(y_i - p(x_i;\theta)) = 0\\
\frac{\partial \log L(\theta)}{\partial b} &amp;= \sum_{i=1}^n (y_i - p(x_i;\theta)) = 0
\end{aligned}
$$&lt;h2 id=&#34;朴素贝叶斯分类器&#34;&gt;朴素贝叶斯分类器
&lt;/h2&gt;&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title thm&#34;&gt;定理&lt;span class=&#34;subtitle&#34;&gt;贝叶斯公式&lt;/p&gt;
$$
\begin{aligned}
P(Y=c_i | x) &amp;= \frac{P(x | Y=c_i) P(Y=c_i)}{P(x)} \\
&amp;= \frac{P(x | Y=c_i) P(Y=c_i)}{\sum_{j=1}^K P(x | Y=c_j) P(Y=c_j)}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;朴素贝叶斯假定特征之间相互独立, 即&lt;/p&gt;
$$
p(X^1=x^1, X^2=x^2, \cdots, X^n=x^n | Y=c_k) = \prod_{j=1}^n p(X^j=x^j | Y=c_k)
$$&lt;p&gt;对于输入实例 $x=(x^1,x^2,\cdots,x^n)$, 则后验概率&lt;/p&gt;
$$
p(Y=c_k|x)=\frac{\left( \prod_{i=1}^n p(X^i=x^i | Y=c_k) \right) P(Y=c_k)}{\sum_{j=1}^K \left( \prod_{i=1}^n p(X^i=x^i | Y=c_j) \right) P(Y=c_j)}
$$&lt;p&gt;分母是固定的, 只需比较分子的大小即可. 但是一旦某个特征取值和分类没有同时出现, 后验概率直接为 $0$, 为了避免这种情况, 通常引入一些平滑技术:&lt;/p&gt;
$$
p_{\lambda}(Y=c_k) = \frac{\sum_{j=1}^NI(y_j=c_k)+\lambda}{N+K\lambda}
$$&lt;p&gt;$\lambda=1$ 时称为 Laplace 平滑.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-3.pdf&#34; &gt;本节作业链接&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(2) —— 支持向量机</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/</link>
        <pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/</guid>
        <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://LeoDreamer2004.github.io/styles/notes.css&#34; &gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-2.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;线性可分支持向量机&#34;&gt;线性可分支持向量机
&lt;/h2&gt;&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于一个数据集 $D$, 如果能找到一个超平面 $H: w^Tx + b = 0$, 将数据分为两类. 即对任意 $(x_i, y_i) \in D$, 若 $y_i = 1$, 则 $w^Tx_i + b \geq 0$; 若 $y_i = -1$, 则 $w^Tx_i + b &lt; 0$. 则称 $D$ 是 &lt;strong&gt;线性可分的&lt;/strong&gt; , 超平面 $H$ 是 $D$ 的一个 &lt;strong&gt;分离超平面&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;最优超平面不仅要能够将数据分开, 还要使得两类数据点到超平面的距离尽可能远.&lt;/p&gt;
&lt;p&gt;考虑到 $w,b$ 任意缩放都不影响超平面的位置, 我们可以规定 $w^Tx + b = 1$ 为最近的正类数据点满足的方程. 此时距离为 $1/{\|w\|}$, 要最大化这个量, 即化归成凸二次规划问题:&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b} \frac{1}{2} \|w\|^2 \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1, \quad i = 1, 2, \cdots, n
\end{aligned}
$$&lt;p&gt;只要 $D$ 是线性可分的, 上述问题一定有解且唯一. 对应的分类决策函数&lt;/p&gt;
$$
f(x) = \text{sign}(w^Tx + b)
$$&lt;p&gt;称为 &lt;strong&gt;线性可分支持向量机&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;引入 Lagrange 乘子 $\alpha_i \geq 0$:&lt;/p&gt;
$$
L(w, b, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b) - 1)
$$&lt;p&gt;对 $w, b$ 求偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
&amp; w = \sum_{i=1}^n \alpha_i y_i x_i \\
&amp; 0 = \sum_{i=1}^n \alpha_i y_i
\end{aligned}
$$&lt;p&gt;代入 $L(w, b, \alpha)$, 得到对偶问题:&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title&#34;&gt; 线性可分对偶问题 &lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i \cdot x_j \\
&amp; \text{s.t.} \quad \alpha_i \geq 0, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;由 KKT 条件, 最优解一定满足&lt;/p&gt;
$$
\begin{aligned}
\alpha_i(y_i(w \cdot x_i + b) - 1) &amp;= 0 \\
y_i(w \cdot x_i + b) - 1 &amp;\geq 0 \\
\alpha_i &amp;\geq 0 \\
\end{aligned}
$$&lt;p&gt;由于 $\alpha_i$ 不全为 $0$, 存在 $j$ 使得 $y_j(w \cdot x_j + b) = 1$, 由此&lt;/p&gt;
$$
b = y_j - w \cdot x_j = y_j - \sum_{i=1}^n \alpha_i y_i x_i \cdot x_j
$$&lt;p&gt;乘上 $\alpha_jy_j$ 做累和, 有&lt;/p&gt;
$$
0=\sum_{j=1}^n \alpha_jy_jb = \sum_{j=1}^n \alpha_j - \| w \|^2
$$&lt;p&gt;上式中 $\alpha_i=0$ 的 $i$ 也成立, 因为都是 $0$ 不影响结果. 注意到 $w = \sum_{i=1}^n \alpha_i y_i x_i$ 也只收到 $\alpha_i &gt; 0$ 的影响, 而这些项的点都落在间隔边界&lt;/p&gt;
$$
H_1: w \cdot x + b = 1, \quad H_2: w \cdot x + b = -1
$$&lt;p&gt;上, 称这些点 $x_i$ 为 &lt;strong&gt;支持向量&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;支持向量机的留一误差&lt;/p&gt;
$$
\hat{R}_{\text{loo}} = \frac{1}{n} \sum_{i=1}^n I(f_{D-\{x_i\}}(x_i) \neq y_i)
$$&lt;p&gt;则 $\hat{R}_{\text{loo}} \le N_{SV}/n$, 其中 $N_{SV}$ 为支持向量的个数.&lt;/p&gt;
&lt;h2 id=&#34;线性支持向量机&#34;&gt;线性支持向量机
&lt;/h2&gt;&lt;p&gt;要求 $D$ 线性可分有点苛刻. 容忍一些误差, 引入松弛变量 $\xi_i \geq 0$, 使得约束条件变为&lt;/p&gt;
$$
y_i(w \cdot x_i + b) \geq 1 - \xi_i
$$&lt;p&gt;对于被错误分类的点, $\xi_i$ 可以大于 $1$. 把 $\xi_i \ne 0$ 的点视为特异点, 那么希望特异点尽可能少, 于是优化目标变为&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n I(\xi_i \ne 0) \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;直接用 $\xi_i$ 代替 $I(\xi_i \ne 0)$, 问题变为&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;既然要 $\xi_i$ 尽可能小, 不妨取 $\xi_i = 1 - y_i(w \cdot x_i + b)$,  引入合页损失函数 $h(z) = \max(0, 1-z)$, 即&lt;/p&gt;
$$\xi_i = h(y_i(w \cdot x_i + b))$$&lt;p&gt;则提出一个 $C$ 后, 优化目标变为&lt;/p&gt;
$$
\min_{w, b} \frac{1}{2C} \|w\|^2 + \sum_{i=1}^n h(y_i(w \cdot x_i + b))
$$&lt;p&gt;做了这么多, 只是相当于把 0-1 损失函数换成了合页损失函数.&lt;/p&gt;
&lt;p&gt;回到原问题, 引入 Lagrange 乘子 $\alpha_i, \beta_i \geq 0$, 得到&lt;/p&gt;
$$
L(w, b, \xi, \alpha, \beta) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b) - 1 + \xi_i) - \sum_{i=1}^n \beta_i \xi_i
$$&lt;p&gt;对 $w, b, \xi$ 偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
&amp; w = \sum_{i=1}^n \alpha_i y_i x_i \\
&amp; 0 = \sum_{i=1}^n \alpha_i y_i \\
&amp; \beta_i = C - \alpha_i
\end{aligned}
$$&lt;p&gt;代入 $L(w, b, \xi, \alpha, \beta)$, 得到对偶问题&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title&#34;&gt; 线性支持向量机对偶问题 &lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i \cdot x_j \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;与线性可分支持向量机类似, 只是多了一个 $\alpha_i \leq C$ 的约束. 现在考虑 KKT 条件, 有&lt;/p&gt;
$$
\begin{aligned}
\alpha_i(y_i(w \cdot x_i + b) - 1 + \xi_i) &amp;= 0 \\
y_i(w \cdot x_i + b) - 1 + \xi_i &amp;\geq 0 \\
\beta_i \xi_i &amp;= 0 \\
\alpha_i &amp;\geq 0 \\
\beta_i &amp;\geq 0 \\
\alpha_i + \beta_i&amp;=C
\end{aligned}
$$&lt;p&gt;则 $\alpha_i &gt; 0$ 的点 $x_i$ 为支持向量, 满足 $y_i(w \cdot x_i + b) = 1 - \xi_i$. 这点与线性可分支持向量机的支持向量不同. 但进一步如果 $\alpha_i \lt C$ , 则 $\beta_i \gt 0$, 则 $\xi_i=0$, 从而 $y_i(w \cdot x_i + b) = 1$, 这样就一致了.&lt;/p&gt;
&lt;p&gt;进一步, 把 $y_i(w \cdot x_i + b) = 1$ 两边乘 $y_i$, 类似有&lt;/p&gt;
$$
b = y_j - \sum_{i=1}^n \alpha_i y_i x_i \cdot x_j
$$&lt;p&gt;因而最优分类超平面为&lt;/p&gt;
$$
\sum_{i=1}^n \alpha_i y_i x_i \cdot x + b = 0
$$&lt;p&gt;和决策函数&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i x_i \cdot x + b\right)
$$&lt;p&gt;超平面法向量可以被唯一确定, 但是偏置不唯一.&lt;/p&gt;
&lt;h2 id=&#34;smo-算法&#34;&gt;SMO 算法
&lt;/h2&gt;&lt;p&gt;SMO 算法是一种启发式算法, 用于求解支持向量机的对偶问题. SMO 算法的基本思想是: 每次选择两个变量, 固定其他变量, 优化这两个变量. 这样不断迭代, 直到收敛.&lt;/p&gt;
&lt;p&gt;设当前迭代的两个变量为 $\alpha_i, \alpha_j$, 则&lt;/p&gt;
$$
\alpha_1 y_1 + \alpha_2 y_2 = -\sum_{i=3}^n \alpha_i y_i
$$&lt;p&gt;同乘 $y_1$, 有&lt;/p&gt;
$$
\alpha_1 + \alpha_2 y_1y_2= -\sum_{i=3}^n \alpha_i y_1y_i
$$&lt;p&gt;记右边为 $\gamma$, $s=y_1y_2 \in \{-1, 1\}$, 则&lt;/p&gt;
$$
\alpha_1 + s\alpha_2 = \gamma
$$&lt;p&gt;记$K_{ij} = x_i \cdot x_j$, $v_i = \sum_{j=3}^{N} \alpha_j y_j K_{ij}$, 则对偶问题转化为&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha_1, \alpha_2} \alpha_1 + \alpha_2 - \frac{1}{2} K_{11}\alpha_1^2 - \frac{1}{2} K_{22}\alpha_2^2 - sK_{12}\alpha_1\alpha_2 - y_1v_1\alpha_1 - y_2v_2\alpha_2 \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \alpha_1 + s\alpha_2 = \gamma
\end{aligned}
$$&lt;p&gt;再由 $\alpha_1 = \gamma - s\alpha_2$, 代入目标函数, 并对 $\alpha_2$ 求导为 $0$, 得到&lt;/p&gt;
$$
\alpha_2 = \frac{s(K_{11}-K_{12})\gamma + y_2(v_1 - v_2) - s + 1}{K_{11} + K_{22} - 2K_{12}}
$$&lt;p&gt;代入 $v$ 的定义, 随后化简得&lt;/p&gt;
$$
\alpha_2 = \alpha_2^* + y_2 \frac{(y_2 - f(x_2))- (y_1-f(x_1))}{K_{11} + K_{22} - 2K_{12}}
$$&lt;p&gt;别忘了约束 $0 \le \alpha_1, \alpha_2 \le C$, 以及 $\alpha_1 + s\alpha_2 = \gamma$, 对 $\alpha_2$ 进行裁剪为 $\alpha_2^{\text{clip}}$. 相应地,&lt;/p&gt;
$$
\alpha_1 = \alpha_1^* + s(\alpha_2^* - \alpha_2^{\text{clip}})
$$&lt;p&gt;最后, 更新 $b$. 假设在 $\alpha_1, \alpha_2$ 中, $0 \lt \alpha_i \lt C$, 则&lt;/p&gt;
$$
b = y_i - \sum_{j=1}^n \alpha_j y_j K_{ij}
$$&lt;p&gt;关于选取 $\alpha_1, \alpha_2$, 一般有两个原则:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择违反 KKT 条件最严重的两个变量.&lt;/li&gt;
&lt;li&gt;选择两个变量使得目标函数有最大变化.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;核方法和非线性支持向量机&#34;&gt;核方法和非线性支持向量机
&lt;/h2&gt;&lt;p&gt;对于非线性问题, 可以通过核方法将数据映射到高维空间, 从而在高维空间中找到一个线性超平面.&lt;/p&gt;
&lt;p&gt;假设有一个映射 $\phi: \mathcal{X} \mapsto \mathcal{Z}$, 则在 $\mathcal{Z}$ 的线性支持向量机变为:&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&amp; \text{s.t.} \quad y_i(w \cdot \phi(x_i) + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;对应的对偶问题为&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \phi(x_i) \cdot \phi(x_j) \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;p&gt;相应的分类决策函数为&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i \phi(x_i) \cdot \phi(x) + b\right)
$$&lt;p&gt;然而, 直接计算 $\phi(x_i) \cdot \phi(x_j)$ 的复杂度很高. 为此, 引入核函数&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}$ 是输入空间, $\mathcal{Z}$ 是特征空间, 如果存在一个从 $\mathcal{X}$ 到 $\mathcal{Z}$ 的映射 $\phi$, 使得对任意 $x, x&#39; \in \mathcal{X}$, 都有&lt;/p&gt;
$$
K(x, x&#39;) = \phi(x) \cdot \phi(x&#39;)
$$&lt;p&gt;则称 $K$ 为 &lt;strong&gt;核函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;注意, 这里我们不再需要显式地计算 $\phi(x_i)$, 因为结果只与 $K(x_i, x_j)$ 有关.&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title&#34;&gt; 非线性支持向量机对偶问题 &lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j) \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;此时, 分类决策函数为&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i K(x_i, x) + b\right)
$$&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;$\mathcal{X}$ 上的函数 $K: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ 称为 &lt;strong&gt;正定对称核函数&lt;/strong&gt;, 如果对任意 $x_1, x_2, \cdots, x_n \in \mathcal{X}$, 核矩阵 (Gram 矩阵) $[K_{ij}]_{m \times m}$ 是半正定的.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;常见的核函数有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性核函数: $K(x, x&#39;) = x \cdot x&#39;$, 对应线性支持向量机.&lt;/li&gt;
&lt;li&gt;多项式核函数: $K(x, x&#39;) = (x \cdot x&#39; + 1)^d, c \gt 0$&lt;/li&gt;
&lt;li&gt;高斯核函数: $K(x, x&#39;) = \exp\left(-\frac{\|x-x&#39;\|^2}{2\sigma^2}\right), \sigma \gt 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-2.pdf&#34; &gt;本节作业链接&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(1) —— 概述</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/intro/</link>
        <pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/intro/</guid>
        <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://LeoDreamer2004.github.io/styles/notes.css&#34; &gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-1.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;基础数学工具&#34;&gt;基础数学工具
&lt;/h2&gt;&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机变量 $X$ 的 &lt;strong&gt;期望&lt;/strong&gt; $E[X]$ 定义为&lt;/p&gt;
$$
E[X] = \sum_{x} x \cdot P(X=x)
$$&lt;p&gt;随机变量 $X$ 的 &lt;strong&gt;方差&lt;/strong&gt; $\text{Var}(X)$ 定义为&lt;/p&gt;
$$
\text{Var}(X) = E[(X - E[X])^2]
$$&lt;p&gt;&lt;strong&gt;标准差&lt;/strong&gt; $\sigma(X)$ 定义为&lt;/p&gt;
$$
\sigma(X) = \sqrt{\text{Var}(X)}
$$&lt;/div&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title thm&#34;&gt;定理&lt;span class=&#34;subtitle&#34;&gt;Markov 不等式&lt;/p&gt;
&lt;p&gt;设 $X$ 是一个非负随机变量, 期望存在, 那么对于任意 $t &gt; 0$ 有&lt;/p&gt;
$$
P(X \geq t) \leq \frac{E[X]}{t}
$$&lt;/div&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title thm&#34;&gt;定理&lt;span class=&#34;subtitle&#34;&gt;Chebyshev 不等式&lt;/p&gt;
&lt;p&gt;设 $X$ 是一个随机变量, 期望和方差都存在, 那么对于任意 $t &gt; 0$ 有&lt;/p&gt;
$$
P(|X - E[X]| \geq t) \leq \frac{\text{Var}(X)}{t^2}
$$&lt;/div&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机变量 $X$ 和 $Y$ 的 &lt;strong&gt;协方差&lt;/strong&gt; $\text{Cov}(X, Y)$ 定义为&lt;/p&gt;
$$
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]
$$&lt;p&gt;如果 $\text{Cov}(X, Y) = 0$, 则称 $X$ 和 $Y$ &lt;strong&gt;不相关&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;协方差具有对称性, 双线性.&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机向量 $X=(X_1, X_2, \ldots, X_n)$ 的 &lt;strong&gt;协方差矩阵&lt;/strong&gt; $C(X)$ 定义为&lt;/p&gt;
$$
C(X) = E[(X - E[X])(X - E[X])^T] = (\text{Cov}(X_i, X_j))_{ij}
$$&lt;/div&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gauss 分布&lt;/strong&gt; (正态分布) 的概率密度函数为&lt;/p&gt;
$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$&lt;p&gt;&lt;strong&gt;Laplace 分布&lt;/strong&gt; 的概率密度函数为&lt;/p&gt;
$$
f(x) = \frac{1}{2b} \exp(-\frac{|x-\mu|}{b})
$$&lt;/div&gt;
&lt;p&gt;最优化问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \min f(x) \\
\text{s.t. } &amp; c_i(x) \leq 0, i = 1, 2, \dots, k \\
&amp; h_j(x) = 0, j = 1, 2, \dots, l
\end{aligned}
$$&lt;p&gt;构造 Lagrange 函数&lt;/p&gt;
$$
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{k} \alpha_i c_i(x) + \sum_{j=1}^{l} \beta_j h_j(x)
$$&lt;p&gt;引入 Karush-Kuhn-Tucker (KKT) 条件&lt;/p&gt;
$$
\begin{aligned}
&amp; \nabla_x L(x, \alpha, \beta) = 0 \\
&amp; c_i(x) \leq 0, i = 1, 2, \dots, k \\
&amp; h_j(x) = 0, j = 1, 2, \dots, l \\
&amp; \alpha_i c_i(x) = 0, i = 1, 2, \dots, k \\
&amp; \alpha_i \geq 0, i = 1, 2, \dots, k
\end{aligned}
$$&lt;h2 id=&#34;基本概念和术语&#34;&gt;基本概念和术语
&lt;/h2&gt;&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;监督学习&lt;/strong&gt;: 基于标记数据 $T=\{ (x_i,y_i) \}_{i=1}^N$, 学习一个从输入空间到输出空间的映射 $f: \mathcal{X} \mapsto \mathcal{Y}$. 利用此对未见数据进行预测. 通常分为 &lt;strong&gt;回归&lt;/strong&gt; 和 &lt;strong&gt;分类&lt;/strong&gt; 两类.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt;: 基于未标记数据 $T=\{ x_i \}_{i=1}^N$, 发现其中隐含的知识模式. &lt;strong&gt;聚类&lt;/strong&gt; 是典型的无监督学习任务.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;半监督学习&lt;/strong&gt;: 既有标记数据又有未标记数据 (通常占比较大).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;: 通过观察环境的反馈, 学习如何选择动作以获得最大的奖励.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;模型评估与选择&#34;&gt;模型评估与选择
&lt;/h2&gt;&lt;h3 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h3&gt;&lt;p&gt;模型基于算法按照一定策略给出假设 $h \in \mathcal{H}$, 通过 &lt;strong&gt;损失函数&lt;/strong&gt; $L(h(x), y)$ 衡量假设的好坏.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0-1 损失函数:&lt;/li&gt;
&lt;/ul&gt;
$$L(h(x), y) = \mathbb{I}(h(x) \neq y) = \begin{cases} 0, &amp; h(x) = y \\ 1, &amp; h(x) \neq y \end{cases}$$&lt;ul&gt;
&lt;li&gt;平方损失函数:&lt;/li&gt;
&lt;/ul&gt;
$$L(h(x), y) = (h(x) - y)^2$$&lt;p&gt;平均损失 $R(h) = E_{x \sim D} [L(h(x), y)]$ 称为 &lt;strong&gt;泛化误差&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;容易验证, 对于 0-1 损失函数, 准确率 $a = 1-R(h)$.&lt;/p&gt;
&lt;h3 id=&#34;二分类&#34;&gt;二分类
&lt;/h3&gt;&lt;p&gt;对于二分类问题, 样本预测结果有四种情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;真正例&lt;/strong&gt; (True Positive, TP): 预测为正例, 实际为正例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;假正例&lt;/strong&gt; (False Positive, FP): 预测为正例, 实际为负例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真负例&lt;/strong&gt; (True Negative, TN): 预测为负例, 实际为负例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;假负例&lt;/strong&gt; (False Negative, FN): 预测为负例, 实际为正例.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由此引入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准确率(查准率):&lt;/strong&gt; $P = \frac{TP}{TP+FP}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;召回率(查全率):&lt;/strong&gt; $R = \frac{TP}{TP+FN}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$F_1$ 度量:&lt;/strong&gt; 考虑到二者抵触, 引入调和均值 $F_1 = \frac{2PR}{P+R}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;过拟合和正则化&#34;&gt;过拟合和正则化
&lt;/h3&gt;&lt;p&gt;为了防止由于模型过于复杂而导致的过拟合, 可以通过 &lt;strong&gt;正则化&lt;/strong&gt; 方法来限制模型的复杂度.&lt;/p&gt;
$$
\min \sum_{i=1}^{N} L(h(x_i), y_i) + \lambda J(h)
$$&lt;p&gt;其中 $J(h)$ 是随着模型复杂度增加而增加的函数. $\lambda$ 是正则化参数.&lt;/p&gt;
&lt;p&gt;怎么选取合适的 $\lambda$ ? 一般是先给出若干候选, 在验证集上进行评估, 选取泛化误差最小的.&lt;/p&gt;
&lt;h3 id=&#34;数据集划分&#34;&gt;数据集划分
&lt;/h3&gt;&lt;p&gt;一般将数据集划分为 &lt;strong&gt;训练集&lt;/strong&gt; $T$ 和 &lt;strong&gt;测试(验证)集&lt;/strong&gt; $T^\prime$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;留出法 (hold-out)&lt;/strong&gt;: 分层无放回地随机采样. 也叫简单交叉验证.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$k$ 折交叉验证 ($k$-fold cross validation)&lt;/strong&gt;: 将数据集分为 $k$ 个大小相等的子集, 每次取其中一个作为验证集, 其余作为训练集, 最后以这 $k$ 次的平均误差作为泛化误差的估计. 当 $k=|D|$ 时称为留一 (leave-one-out) 验证法.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自助法 (bootstrapping)&lt;/strong&gt;: 从数据集中&lt;em&gt;有放回地&lt;/em&gt;采样 $|D|$ 个数据作为训练集, 没抽中的作为验证集. 因而训练集 $T$ 和原始数据集 $D$ 的分布未必一致, 对数据分布敏感的模型不适用.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;偏差-方差分解&#34;&gt;偏差-方差分解
&lt;/h2&gt;&lt;p&gt;为什么泛化误差会随着模型复杂度的增加而先减小后增大?&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;偏差&lt;/strong&gt; (bias): 模型预测值的期望与真实值之间的差异. 体现了模型的拟合能力.&lt;/p&gt;
$$\text{Bias}(x) = E_T[h_T(x)-c(x)] = \bar{h}(x) - c(x)$$&lt;p&gt;&lt;strong&gt;方差&lt;/strong&gt; (variance): 模型预测值的方差. 体现了模型的对数据扰动的稳定性.&lt;/p&gt;
$$\text{Var}(x) = E[(h(x) - \bar{h}(x))^2]$$&lt;/div&gt;
&lt;p&gt;现在对泛化误差进行分解:&lt;/p&gt;
$$
\begin{aligned}
R(h) &amp;= E_T[(h_T(x) - c(x))^2] \\
&amp;= E_T[h_T^2(x) - 2h_T(x)c(x) + c^2(x)] \\
&amp;= E_T[h_T^2(x)] - 2c(x)E_T[h_T(x)] + c^2(x) \\
&amp;= E_T[h_T^2(x)] - \bar{h}^2(x) + \bar{h}^2(x) - 2\bar{h}(x)c(x) + c^2(x) \\
&amp;= E_T[(h_T(x) - \bar{h}(x))^2] + (\bar{h}(x) - c(x))^2 \\
&amp;= \text{Var}(x) + \text{Bias}^2(x)
\end{aligned}
$$&lt;p&gt;当然, 由于噪声存在, $y$ 未必一定等于 $c(x)$, 不妨设 $y=c(x)+\varepsilon$, 其中 $\varepsilon \sim \Epsilon$ 期望为 $0$. 可以证明&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title thm&#34;&gt;定理&lt;span class=&#34;subtitle&#34;&gt;偏差-方差分解&lt;/p&gt;
$$
E_{T \sim D^{|T|}, \varepsilon \sim \Epsilon} [(h_T(x)-y)^2] = \text{Bias}^2(x) + \text{Var}(x) + E[\varepsilon^2]
$$&lt;p&gt;即泛化误差可以分解为偏差、方差和噪声三部分.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;起初, 模型较为简单, 偏差在泛化误差起主导作用. 随着模型复杂度的增加, 拟合能力增强, 偏差减小, 但带来过拟合风险, 算法对数据扰动敏感, 方差增大. 方差占比逐渐增大, 最终导致泛化误差增大.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-1.pdf&#34; &gt;本节作业链接&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/</link>
        <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/</guid>
        <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://LeoDreamer2004.github.io/styles/notes.css&#34; &gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title&#34;&gt;导航&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;博客页面&lt;/th&gt;
          &lt;th&gt;课件&lt;/th&gt;
          &lt;th&gt;作业&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;intro&#34; &gt;机器学习基础(1) —— 概述&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-1.pdf&#34; &gt;课件链接&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-1.pdf&#34; &gt;作业链接&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;vector-machine&#34; &gt;机器学习基础(2) —— 支持向量机&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-2.pdf&#34; &gt;课件链接&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-2.pdf&#34; &gt;作业链接&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;bayers&#34; &gt;机器学习基础(3) —— 基于后验概率最大化准则的分类模型&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-3.pdf&#34; &gt;课件链接&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-3.pdf&#34; &gt;作业链接&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
        </item>
        
    </channel>
</rss>
