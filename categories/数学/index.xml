<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>数学 on LeoDreamer</title>
        <link>https://LeoDreamer2004.github.io/categories/%E6%95%B0%E5%AD%A6/</link>
        <description>Recent content in 数学 on LeoDreamer</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>LeoDreamer</copyright>
        <lastBuildDate>Tue, 18 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://LeoDreamer2004.github.io/categories/%E6%95%B0%E5%AD%A6/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习基础(5) —— 决策树模型</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/</link>
        <pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-5.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;特征的分类能力评估&#34;&gt;特征的分类能力评估
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 其中 $x_i=\left(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(m)}\right) \in \mathcal{X}$ 是第 $i$ 个样本的特征向量, $y_i \in \mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$ 是第 $i$ 个样本的标签. 假设数据集 $D$ 根据特征分成了 $K$ 个子集 $D_1,D_2,\cdots,D_K$, 定义 &lt;strong&gt;经验熵&lt;/strong&gt; 为&lt;/p&gt;
$$
H(D) = -\sum_{k=1}^K \frac{|D_k|}{|D|} \log_2 \frac{|D_k|}{|D|}
$$&lt;p&gt;现在给定某维特征 $A$ 和其取值集合 $\{a_1,a_2,\cdots,a_m\}$, 根据 $A$ 的取值将数据集 $D$ 分成了 $m$ 个子集 $D_1^A,D_2^A,\cdots,D_m^A$, 并进一步考虑 $D_i^A$ 中的标签分布, 定义 &lt;strong&gt;条件经验熵&lt;/strong&gt; 为&lt;/p&gt;
$$
H(D|A) = \sum_{i=1}^m \frac{|D_i^A|}{|D|} H(D_i^A)
$$&lt;/div&gt;
&lt;p&gt;如果条件经验熵和经验熵之差越大, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;属性 $A$ 对数据集 $D$ 的 &lt;strong&gt;信息增益&lt;/strong&gt; $g(D,A)$ 定义为&lt;/p&gt;
$$
g(D,A) = H(D) - H(D|A)
$$&lt;/div&gt;
&lt;p&gt;考虑到信息增益的计算会偏向于选择取值较多的特征, 为了避免这种情况, 引入信息增益率来评估特征的分类能力.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;特征 $A$ 的 &lt;strong&gt;分裂信息&lt;/strong&gt; $IV(A)$ 定义为&lt;/p&gt;
$$
IV(A) = -\sum_{i=1}^m \frac{|D_i^A|}{|D|} \log_2 \frac{|D_i^A|}{|D|}
$$&lt;p&gt;特征 $A$ 的 &lt;strong&gt;信息增益率&lt;/strong&gt; $g_R(D,A)$ 定义为&lt;/p&gt;
$$
g_R(D,A) = \frac{g(D,A)}{IV(A)}
$$&lt;/div&gt;
&lt;p&gt;分裂信息其实就是按照 $A$ 取值作划分的经验熵.&lt;/p&gt;
&lt;p&gt;除了信息增益和信息增益率, 还有 Gini 指数可以用来评估特征的分类能力.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;数据集 $D$ 的 &lt;strong&gt;Gini 指数&lt;/strong&gt; $\text{Gini}(D)$ 定义为&lt;/p&gt;
$$
\text{Gini}(D) = 1 - \sum_{k=1}^K \left(\frac{|D_k|}{|D|}\right)^2
$$&lt;p&gt;特征 $A$ 的 &lt;strong&gt;Gini 指数&lt;/strong&gt; $\text{Gini}(D,A)$ 定义为&lt;/p&gt;
$$
\text{Gini}(D,A) = \sum_{i=1}^m \frac{|D_i^A|}{|D|} \text{Gini}(D_i^A)
$$&lt;p&gt;如果按照特征 $A$ 是否取值为 $a_i$ 对数据集 $D$ 进行划分 $D=D_i^A \cup (D-D_i^A)$, 则 $A=a_i$ 的 &lt;strong&gt;Gini 指数&lt;/strong&gt; $\text{Gini}_d(D,A=a_i)$ 定义为&lt;/p&gt;
$$
\text{Gini}_d(D,A=a_i) = \frac{|D_i^A|}{|D|} \text{Gini}(D_i^A) + \frac{|D-D_i^A|}{|D|} \text{Gini}(D-D_i^A)
$$&lt;/div&gt;
&lt;p&gt;Gini 指数可以看作任取两个样本, 它们的标签不一致的概率. 如果 Gini 指数越小, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.&lt;/p&gt;
&lt;h2 id=&#34;决策树模型&#34;&gt;决策树模型
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;生成决策树算法&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 特征集 $\mathcal{A}=\{A_1,A_2,\cdots,A_m\}$, 最优特征选择函数 $F$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 决策树 $T$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若数据集 $D$ 中所有样本的标签都是 $c_k$, 则生成一个类标记为 $c_k$ 的叶结点, 返回 $T$;&lt;/li&gt;
&lt;li&gt;若 $A=\emptyset$, 且 $D$ 非空, 则生成一个单节点树, 并以 $D$ 中样本数最多的类标记作为该节点的类标记, 返回 $T$;&lt;/li&gt;
&lt;li&gt;计算 $A^\ast=F(D,\mathcal{A})$;&lt;/li&gt;
&lt;li&gt;对 $A^\ast$ 的每一个取值 $a_i$, 构造一个对应于 $D_i$ 的子节点;&lt;/li&gt;
&lt;li&gt;若 $D_i=\emptyset$, 则将子节点标记为叶结点, 类标记为 $D$ 中样本数最多的类标记;&lt;/li&gt;
&lt;li&gt;否则, 将 $D_i$ 中样本数最多的类标记作为该节点的类标记&lt;/li&gt;
&lt;li&gt;对每个 $D_i$ 对应的非叶子节点, 以 $D_i$ 为训练集, 以 $\mathcal{A}-\{A^\ast\}$ 为特征集, 递归调用 1-6 步, 构建决策树 $T$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;如果以信息增益为特征选择函数, 即 $A^\ast = \arg\max_{A \in \mathcal{A}} g(D,A)$, 则算法对应于 ID3 算法; 如果以信息增益率为特征选择函数, 即 $A^\ast = \arg\max_{A \in \mathcal{A}} g_R(D,A)$, 则算法对应于 C4.5 算法.&lt;/p&gt;
&lt;p&gt;二路划分会采用以特征的可能取值为切分点的二分法划分当前数据集, 例如与选择 Gini 指数最小的特征和切分点对应的特征值, 即 $(A^\ast,a^\ast) = \arg\min_{A \in \mathcal{A},a \in V(A)} \text{Gini}_d(D,A=a)$, 则算法对应于 CART 算法.&lt;/p&gt;
&lt;p&gt;为了降低过拟合风险, 可以对决策树进行剪枝. 常用的是后剪枝, 即先生成一棵完全生长的决策树, 然后根据泛化性能决定是否剪枝. 也可以采用正则化方法, 例如, 定义决策树 $T$ 的损失或代价函数:&lt;/p&gt;
$$
C_\alpha(T) = C(T) + \alpha |T|
$$&lt;p&gt;其中 $C(T)$ 用于衡量 $T$ 对 $D$ 的拟合程度, $|T|$ 表示 $T$ 的叶结点个数, $\alpha \geq 0$ 用于权衡拟合程度和模型复杂度.&lt;/p&gt;
&lt;p&gt;CART 算法有特别的剪枝处理: 从 CART 算法生成得到完整决策树 $T_0$ 开始, 产生一个递增的权衡系数序列 $0=\alpha_0 &lt; \alpha_1 &lt; \cdots &lt; \alpha_n &lt; +\infty$ 和一个嵌套的子树序列 $\{T_0, T_1, \cdots, T_n\}$, $T_i$ 为 $\alpha \in [\alpha_i, \alpha_{i+1})$ 时的最优子树, $T_n$ 是根节点单独构成的树.&lt;/p&gt;
&lt;p&gt;如果是连续特征, 则可以考虑将其离散化, 例如, 通过二分法将其划分为两个区间, 选择最优划分点.&lt;/p&gt;
&lt;p&gt;现在继续从经验风险的角度来看决策树模型.采用 $0-1$ 损失函数, 设节点 $t$ 设置的标记是 $c_k$, 则在 $t$ 对应的数据集上的经验风险为&lt;/p&gt;
$$
\frac{1}{|D_t|} \sum_{i=1}^{|D_t|} I(y_i \neq c_k)
$$&lt;p&gt;显见, 等价于&lt;/p&gt;
$$
\max_{c_k \in \mathcal{Y}} \frac{1}{|D_t|} \sum_{i=1}^{|D_t|} I(y_i = c_k)
$$&lt;p&gt;从现在来看, 决策树构造过程中划分的单元都是矩形的, 即分类边界是若干与特征坐标轴平行的边界组成. 多变量决策树模型允许用若干特征的线性组合来划分数据集, 对每个非叶结点学习一个线性分类器.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(4) —— 基于近邻的分类方法</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/knn/</link>
        <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/knn/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-4.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;k-近邻算法&#34;&gt;k-近邻算法
&lt;/h2&gt;&lt;p&gt;k-近邻算法的主要思想是, 对于一个给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点, 然后根据这 $k$ 个样本点的类别进行多数占优的投票方式来预测 $x$ 的类别.&lt;/p&gt;
&lt;p&gt;在 $n$ 维实数空间 $\mathbb{R}$ 中, 通常用 Minkowski 距离来度量两个点 $x_i, x_j$ 的相似性:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $x_i, x_j \in \mathbb{R}^n$, 则 $x_i, x_j$ 之间的 &lt;strong&gt;Minkowski 距离&lt;/strong&gt; $\text{dist}_p(x_i,x_j)$ 定义为&lt;/p&gt;
$$
\text{dist}_p(x_i,x_j) = \left( \sum_{l=1}^n |x_i^l - x_j^l|^p \right)^{1/p}
$$&lt;/div&gt;
&lt;p&gt;$p=1$ 时, 就是 Manhattan 距离; $p=2$ 时, 就是 Euclidean 距离; $p=\infty$ 时, 就是 Chebyshev 距离. 在必要时, 还可以给每个维度的特征值加权.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定训练样本集 $D = \{(x_i, y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_k\}$, 以及距离度量 $\text{dist}$, &lt;strong&gt;k-近邻算法&lt;/strong&gt; 的基本步骤如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;基于度量 $\text{dist}$, 对于给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点所构成的邻域 $N_k^{\text{dist}}(x)$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;采用如下的多数投票规则来预测 $x$ 的类别:&lt;/p&gt;
$$
    y = \arg\max_{c_i} \sum_{x_j \in N_k^{\text{dist}}(x)} I(y_j = c_i)
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;如果把 0-1 作为损失函数, 那么 k-近邻算法实际上就是让经验风险最小化.&lt;/p&gt;
&lt;h2 id=&#34;最近邻算法&#34;&gt;最近邻算法
&lt;/h2&gt;&lt;p&gt;在 k-近邻算法中, 当 $k=1$ 时, 称为最近邻算法. 因此, 特点是偏差小, 方差大. 这其实是特征空间的一个划分 $\mathcal{X}=\bigcup_{i=1}^n \{R_i\}$. 对每个划分单元 $R_i$, 该单元的数据点到其他样本的距离都不会小于到 $x_i$ 的距离.&lt;/p&gt;
&lt;h2 id=&#34;最近邻算法的扩展&#34;&gt;最近邻算法的扩展
&lt;/h2&gt;&lt;p&gt;给定样本集 $D = \{(x_i,y_i)\}_{i=1}^n$, 以 $D_i$ 表示属于类 $c_i$ 的样本集, 希望找一个方式把每个 $D_i$ 分成 $k$ 个簇 $(D_{i1}, D_{i2}, \cdots, D_{ik})$, 使得数据分布的方差最小, 即&lt;/p&gt;
$$
(D^\ast_{i1}, D^\ast_{i2}, \cdots, D^\ast_{il}) = \arg\min_{D_{i1}, D_{i2}, \cdots, D_{ik}} \sum_{j=1}^k \sum_{(x_t,y_t) \in D_{ij}} \Vert x_t-c_{ij} \Vert_2^2
$$&lt;p&gt;然而很难找到最优解, 因此采用迭代的方式来近似求解:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;K-means 算法&lt;/strong&gt; 的基本步骤如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化 $k$ 个簇的中心 $c_{ij}$;&lt;/li&gt;
&lt;li&gt;对每个 $(x_t),(y_t) \in D_i$ (即 $y_t=c_i$), 令
$$I_{x_t}= \arg\min_{j} \Vert x_t-c_{ij} \Vert_2^2$$
即将 $x_t$ 分配到最近的簇;&lt;/li&gt;
&lt;li&gt;对每个 $D_{ij}$, 更新均值
$$c_{ij} = \frac{1}{|D_{ij}|} \sum_{(x_t,y_t) \in D_{ij}} x_t$$&lt;/li&gt;
&lt;li&gt;重复 2, 3 直到收敛.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;有可能会使得某些离分类边界很近的点被错误分类. 引入学习向量量化方法 (LVQ 算法). 让同类和异类的点在构建过程中都能起作用.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LVQ 算法&lt;/strong&gt; 的基本步骤如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对每个类 $c_m$ 随机选择 $k$ 个点 $I_{mi}$ 作为代表;&lt;/li&gt;
&lt;li&gt;对每个样本点 $x_t$, 找到最近的代表元 $I_{m^\ast i^\ast}$, 即
$$I_{m^\ast i^\ast} = \arg\min_{m,i} \Vert x_t - I_{mi} \Vert_2^2$$&lt;/li&gt;
&lt;li&gt;如果 $y_t=c_{m^\ast}$, 则
$$I_{m^\ast i^\ast} \gets I_{m^\ast i^\ast} + \eta(x_t - I_{m^\ast i^\ast})$$
否则
$$I_{m^\ast i^\ast} \gets I_{m^\ast i^\ast} - \eta(x_t - I_{m^\ast i^\ast})$$&lt;/li&gt;
&lt;li&gt;重复 2, 3 直到收敛.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里 $\eta$ 是学习率.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;在 $\eta=1$ 时, LVQ 算法相当于逐步地进行 k-means 算法.&lt;/p&gt;
&lt;p&gt;在最近邻算法和其扩展方法中, 每个簇的代表点也称为相应单元的原型. 这种方法也常被称作原型方法或免模型方法.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(3) —— 基于后验概率最大化准则的分类模型</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/bayers/</link>
        <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/bayers/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-3.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;后验概率最大化准则&#34;&gt;后验概率最大化准则
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对训练样本集 $D=\{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathcal{X}$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_K\}$, 将 $x$ 的类别预测为 $c_i$ 的 &lt;strong&gt;风险&lt;/strong&gt; 为&lt;/p&gt;
$$
R(Y=c_i | x) = \sum_{j=1}^K \lambda_{ij} P(Y=c_j | x)
$$&lt;p&gt;其中 $\lambda_{ij}$ 是将属于 $c_j$ 的样本预测为 $c_i$ 的损失. &lt;strong&gt;最优预测&lt;/strong&gt; $\hat{y}$ 是使得风险最小的类别, 即&lt;/p&gt;
$$
\hat{y} = \arg\min_{c_i} R(Y=c_i | x)
$$&lt;/div&gt;
&lt;p&gt;假设采用 $0-1$ 损失函数, 易知&lt;/p&gt;
$$
R(Y=c_i | x) = 1 - P(Y=c_i | x)
$$&lt;p&gt;即输入 $x$ 的最优预测 $\hat{y}$ 为使得后验概率 $P(y | x)$ 最大的类别.&lt;/p&gt;
&lt;h2 id=&#34;逻辑斯蒂回归模型&#34;&gt;逻辑斯蒂回归模型
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}=\mathbb{R}^n, \mathcal{Y}=\{c_1,c_2\}$, 逻辑斯蒂回归模型是如下的后验概率分布:&lt;/p&gt;
$$
\begin{aligned}
P(Y=c_1 | x) &amp;= \frac{\exp(w \cdot x + b)}{1+\exp(w \cdot x + b)} \\
P(Y=c_2 | x) &amp;= \frac{1}{1+\exp(w \cdot x + b)
}
\end{aligned}
$$&lt;p&gt;其中 $w,b$ 是模型参数.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;按照后验概率最大化准则, 显然当 $w \cdot x + b &gt; 0$ 时, 预测为 $c_1$, 否则预测为 $c_2$.&lt;/p&gt;
&lt;p&gt;对于多类分类任务, 仍然可以使用逻辑斯蒂回归模型:&lt;/p&gt;
$$
\begin{aligned}
p(y=c_i | x) &amp;= \frac{\exp(w_i \cdot x + b_i)}{\sum_{j=1}^{K-1} \exp(w_j \cdot x + b_j)}, \quad i=1,2,\cdots,K-1 \\
p(y=c_K | x) &amp;= \frac{1}{\sum_{j=1}^{K-1} \exp(w_j \cdot x + b_j)}
\end{aligned}
$$&lt;p&gt;给定 $D=\{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{0,1\}$, 用 $\theta=(w,b)$ 表示二项逻辑斯蒂回归模型的参数, 令&lt;/p&gt;
$$
p(x;\theta) = p(Y=1 | x;\theta)
$$&lt;p&gt;则考虑似然函数为&lt;/p&gt;
$$
\begin{aligned}
L(\theta) &amp;= \prod_{i=1}^n p(x_i;\theta)^{y_i} (1-p(x_i;\theta))^{1-y_i} \\
\log L(\theta) &amp;= \sum_{i=1}^n y_i \log p(x_i;\theta) + (1-y_i) \log (1-p(x_i;\theta)) \\
&amp;= \sum_{i=1}^N y_i(w \cdot x_i + b) - \log(1+\exp(w \cdot x_i + b))
\end{aligned}
$$&lt;p&gt;对 $w,b$ 求偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial \log L(\theta)}{\partial w} &amp;= \sum_{i=1}^n x_i(y_i - p(x_i;\theta)) = 0\\
\frac{\partial \log L(\theta)}{\partial b} &amp;= \sum_{i=1}^n (y_i - p(x_i;\theta)) = 0
\end{aligned}
$$&lt;h2 id=&#34;朴素-bayers-分类器&#34;&gt;朴素 Bayers 分类器
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Bayers 公式&lt;/span&gt;&lt;/p&gt;
$$
\begin{aligned}
P(Y=c_i | x) &amp;= \frac{P(x | Y=c_i) P(Y=c_i)}{P(x)} \\
&amp;= \frac{P(x | Y=c_i) P(Y=c_i)}{\sum_{j=1}^K P(x | Y=c_j) P(Y=c_j)}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;朴素 Bayers 假定特征之间相互独立, 即&lt;/p&gt;
$$
p(X^1=x^1, X^2=x^2, \cdots, X^n=x^n | Y=c_k) = \prod_{j=1}^n p(X^j=x^j | Y=c_k)
$$&lt;p&gt;对于输入实例 $x=(x^1,x^2,\cdots,x^n)$, 则后验概率&lt;/p&gt;
$$
p(Y=c_k|x)=\frac{\left( \prod_{i=1}^n p(X^i=x^i | Y=c_k) \right) P(Y=c_k)}{\sum_{j=1}^K \left( \prod_{i=1}^n p(X^i=x^i | Y=c_j) \right) P(Y=c_j)}
$$&lt;p&gt;分母是固定的, 只需比较分子的大小即可. 但是一旦某个特征取值和分类没有同时出现, 后验概率直接为 $0$, 为了避免这种情况, 通常引入一些平滑技术:&lt;/p&gt;
$$
p_{\lambda}(Y=c_k) = \frac{\sum_{j=1}^NI(y_j=c_k)+\lambda}{N+K\lambda}
$$&lt;p&gt;$\lambda=1$ 时称为 Laplace 平滑.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(2) —— 支持向量机</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/</link>
        <pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-2.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;线性可分支持向量机&#34;&gt;线性可分支持向量机
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于一个数据集 $D$, 如果能找到一个超平面 $H: w^Tx + b = 0$, 将数据分为两类. 即对任意 $(x_i, y_i) \in D$, 若 $y_i = 1$, 则 $w^Tx_i + b \geq 0$; 若 $y_i = -1$, 则 $w^Tx_i + b &lt; 0$. 则称 $D$ 是 &lt;strong&gt;线性可分的&lt;/strong&gt; , 超平面 $H$ 是 $D$ 的一个 &lt;strong&gt;分离超平面&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;最优超平面不仅要能够将数据分开, 还要使得两类数据点到超平面的距离尽可能远.&lt;/p&gt;
&lt;p&gt;考虑到 $w,b$ 任意缩放都不影响超平面的位置, 我们可以规定 $w^Tx + b = 1$ 为最近的正类数据点满足的方程. 此时距离为 $1/{\|w\|}$, 要最大化这个量, 即化归成凸二次规划问题:&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b} \frac{1}{2} \|w\|^2 \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1, \quad i = 1, 2, \cdots, n
\end{aligned}
$$&lt;p&gt;只要 $D$ 是线性可分的, 上述问题一定有解且唯一. 对应的分类决策函数&lt;/p&gt;
$$
f(x) = \text{sign}(w^Tx + b)
$$&lt;p&gt;称为 &lt;strong&gt;线性可分支持向量机&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;引入 Lagrange 乘子 $\alpha_i \geq 0$:&lt;/p&gt;
$$
L(w, b, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b) - 1)
$$&lt;p&gt;对 $w, b$ 求偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
&amp; w = \sum_{i=1}^n \alpha_i y_i x_i \\
&amp; 0 = \sum_{i=1}^n \alpha_i y_i
\end{aligned}
$$&lt;p&gt;代入 $L(w, b, \alpha)$, 得到对偶问题:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;线性可分对偶问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i \cdot x_j \\
&amp; \text{s.t.} \quad \alpha_i \geq 0, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;由 KKT 条件, 最优解一定满足&lt;/p&gt;
$$
\begin{aligned}
\alpha_i(y_i(w \cdot x_i + b) - 1) &amp;= 0 \\
y_i(w \cdot x_i + b) - 1 &amp;\geq 0 \\
\alpha_i &amp;\geq 0 \\
\end{aligned}
$$&lt;p&gt;由于 $\alpha_i$ 不全为 $0$, 存在 $j$ 使得 $y_j(w \cdot x_j + b) = 1$, 由此&lt;/p&gt;
$$
b = y_j - w \cdot x_j = y_j - \sum_{i=1}^n \alpha_i y_i x_i \cdot x_j
$$&lt;p&gt;乘上 $\alpha_jy_j$ 做累和, 有&lt;/p&gt;
$$
0=\sum_{j=1}^n \alpha_jy_jb = \sum_{j=1}^n \alpha_j - \| w \|^2
$$&lt;p&gt;上式中 $\alpha_i=0$ 的 $i$ 也成立, 因为都是 $0$ 不影响结果. 注意到 $w = \sum_{i=1}^n \alpha_i y_i x_i$ 也只收到 $\alpha_i &gt; 0$ 的影响, 而这些项的点都落在间隔边界&lt;/p&gt;
$$
H_1: w \cdot x + b = 1, \quad H_2: w \cdot x + b = -1
$$&lt;p&gt;上, 称这些点 $x_i$ 为 &lt;strong&gt;支持向量&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;支持向量机的留一误差&lt;/p&gt;
$$
\hat{R}_{\text{loo}} = \frac{1}{n} \sum_{i=1}^n I(f_{D-\{x_i\}}(x_i) \neq y_i)
$$&lt;p&gt;则 $\hat{R}_{\text{loo}} \le N_{SV}/n$, 其中 $N_{SV}$ 为支持向量的个数.&lt;/p&gt;
&lt;h2 id=&#34;线性支持向量机&#34;&gt;线性支持向量机
&lt;/h2&gt;&lt;p&gt;要求 $D$ 线性可分有点苛刻. 容忍一些误差, 引入松弛变量 $\xi_i \geq 0$, 使得约束条件变为&lt;/p&gt;
$$
y_i(w \cdot x_i + b) \geq 1 - \xi_i
$$&lt;p&gt;对于被错误分类的点, $\xi_i$ 可以大于 $1$. 把 $\xi_i \ne 0$ 的点视为特异点, 那么希望特异点尽可能少, 于是优化目标变为&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n I(\xi_i \ne 0) \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;直接用 $\xi_i$ 代替 $I(\xi_i \ne 0)$, 问题变为&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;既然要 $\xi_i$ 尽可能小, 不妨取 $\xi_i = 1 - y_i(w \cdot x_i + b)$,  引入合页损失函数 $h(z) = \max(0, 1-z)$, 即&lt;/p&gt;
$$\xi_i = h(y_i(w \cdot x_i + b))$$&lt;p&gt;则提出一个 $C$ 后, 优化目标变为&lt;/p&gt;
$$
\min_{w, b} \frac{1}{2C} \|w\|^2 + \sum_{i=1}^n h(y_i(w \cdot x_i + b))
$$&lt;p&gt;做了这么多, 只是相当于把 0-1 损失函数换成了合页损失函数.&lt;/p&gt;
&lt;p&gt;回到原问题, 引入 Lagrange 乘子 $\alpha_i, \beta_i \geq 0$, 得到&lt;/p&gt;
$$
L(w, b, \xi, \alpha, \beta) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b) - 1 + \xi_i) - \sum_{i=1}^n \beta_i \xi_i
$$&lt;p&gt;对 $w, b, \xi$ 偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
&amp; w = \sum_{i=1}^n \alpha_i y_i x_i \\
&amp; 0 = \sum_{i=1}^n \alpha_i y_i \\
&amp; \beta_i = C - \alpha_i
\end{aligned}
$$&lt;p&gt;代入 $L(w, b, \xi, \alpha, \beta)$, 得到对偶问题&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;线性支持向量机对偶问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i \cdot x_j \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;与线性可分支持向量机类似, 只是多了一个 $\alpha_i \leq C$ 的约束. 现在考虑 KKT 条件, 有&lt;/p&gt;
$$
\begin{aligned}
\alpha_i(y_i(w \cdot x_i + b) - 1 + \xi_i) &amp;= 0 \\
y_i(w \cdot x_i + b) - 1 + \xi_i &amp;\geq 0 \\
\beta_i \xi_i &amp;= 0 \\
\alpha_i &amp;\geq 0 \\
\beta_i &amp;\geq 0 \\
\alpha_i + \beta_i&amp;=C
\end{aligned}
$$&lt;p&gt;则 $\alpha_i &gt; 0$ 的点 $x_i$ 为支持向量, 满足 $y_i(w \cdot x_i + b) = 1 - \xi_i$. 这点与线性可分支持向量机的支持向量不同. 但进一步如果 $\alpha_i \lt C$ , 则 $\beta_i \gt 0$, 则 $\xi_i=0$, 从而 $y_i(w \cdot x_i + b) = 1$, 这样就一致了.&lt;/p&gt;
&lt;p&gt;进一步, 把 $y_i(w \cdot x_i + b) = 1$ 两边乘 $y_i$, 类似有&lt;/p&gt;
$$
b = y_j - \sum_{i=1}^n \alpha_i y_i x_i \cdot x_j
$$&lt;p&gt;因而最优分类超平面为&lt;/p&gt;
$$
\sum_{i=1}^n \alpha_i y_i x_i \cdot x + b = 0
$$&lt;p&gt;和决策函数&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i x_i \cdot x + b\right)
$$&lt;p&gt;超平面法向量可以被唯一确定, 但是偏置不唯一.&lt;/p&gt;
&lt;h2 id=&#34;smo-算法&#34;&gt;SMO 算法
&lt;/h2&gt;&lt;p&gt;SMO 算法是一种启发式算法, 用于求解支持向量机的对偶问题. SMO 算法的基本思想是: 每次选择两个变量, 固定其他变量, 优化这两个变量. 这样不断迭代, 直到收敛.&lt;/p&gt;
&lt;p&gt;设当前迭代的两个变量为 $\alpha_i, \alpha_j$, 则&lt;/p&gt;
$$
\alpha_1 y_1 + \alpha_2 y_2 = -\sum_{i=3}^n \alpha_i y_i
$$&lt;p&gt;同乘 $y_1$, 有&lt;/p&gt;
$$
\alpha_1 + \alpha_2 y_1y_2= -\sum_{i=3}^n \alpha_i y_1y_i
$$&lt;p&gt;记右边为 $\gamma$, $s=y_1y_2 \in \{-1, 1\}$, 则&lt;/p&gt;
$$
\alpha_1 + s\alpha_2 = \gamma
$$&lt;p&gt;记$K_{ij} = x_i \cdot x_j$, $v_i = \sum_{j=3}^{N} \alpha_j y_j K_{ij}$, 则对偶问题转化为&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha_1, \alpha_2} \alpha_1 + \alpha_2 - \frac{1}{2} K_{11}\alpha_1^2 - \frac{1}{2} K_{22}\alpha_2^2 - sK_{12}\alpha_1\alpha_2 - y_1v_1\alpha_1 - y_2v_2\alpha_2 \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \alpha_1 + s\alpha_2 = \gamma
\end{aligned}
$$&lt;p&gt;再由 $\alpha_1 = \gamma - s\alpha_2$, 代入目标函数, 并对 $\alpha_2$ 求导为 $0$, 得到&lt;/p&gt;
$$
\alpha_2 = \frac{s(K_{11}-K_{12})\gamma + y_2(v_1 - v_2) - s + 1}{K_{11} + K_{22} - 2K_{12}}
$$&lt;p&gt;代入 $v$ 的定义, 随后化简得&lt;/p&gt;
$$
\alpha_2 = \alpha_2^* + y_2 \frac{(y_2 - f(x_2))- (y_1-f(x_1))}{K_{11} + K_{22} - 2K_{12}}
$$&lt;p&gt;别忘了约束 $0 \le \alpha_1, \alpha_2 \le C$, 以及 $\alpha_1 + s\alpha_2 = \gamma$, 对 $\alpha_2$ 进行裁剪为 $\alpha_2^{\text{clip}}$. 相应地,&lt;/p&gt;
$$
\alpha_1 = \alpha_1^* + s(\alpha_2^* - \alpha_2^{\text{clip}})
$$&lt;p&gt;最后, 更新 $b$. 假设在 $\alpha_1, \alpha_2$ 中, $0 \lt \alpha_i \lt C$, 则&lt;/p&gt;
$$
b = y_i - \sum_{j=1}^n \alpha_j y_j K_{ij}
$$&lt;p&gt;关于选取 $\alpha_1, \alpha_2$, 一般有两个原则:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择违反 KKT 条件最严重的两个变量.&lt;/li&gt;
&lt;li&gt;选择两个变量使得目标函数有最大变化.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;核方法和非线性支持向量机&#34;&gt;核方法和非线性支持向量机
&lt;/h2&gt;&lt;p&gt;对于非线性问题, 可以通过核方法将数据映射到高维空间, 从而在高维空间中找到一个线性超平面.&lt;/p&gt;
&lt;p&gt;假设有一个映射 $\phi: \mathcal{X} \mapsto \mathcal{Z}$, 则在 $\mathcal{Z}$ 的线性支持向量机变为:&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&amp; \text{s.t.} \quad y_i(w \cdot \phi(x_i) + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;对应的对偶问题为&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \phi(x_i) \cdot \phi(x_j) \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;p&gt;相应的分类决策函数为&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i \phi(x_i) \cdot \phi(x) + b\right)
$$&lt;p&gt;然而, 直接计算 $\phi(x_i) \cdot \phi(x_j)$ 的复杂度很高. 为此, 引入核函数&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}$ 是输入空间, $\mathcal{Z}$ 是特征空间, 如果存在一个从 $\mathcal{X}$ 到 $\mathcal{Z}$ 的映射 $\phi$, 使得对任意 $x, x&#39; \in \mathcal{X}$, 都有&lt;/p&gt;
$$
K(x, x&#39;) = \phi(x) \cdot \phi(x&#39;)
$$&lt;p&gt;则称 $K$ 为 &lt;strong&gt;核函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;注意, 这里我们不再需要显式地计算 $\phi(x_i)$, 因为结果只与 $K(x_i, x_j)$ 有关.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;非线性支持向量机对偶问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j) \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;此时, 分类决策函数为&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i K(x_i, x) + b\right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;$\mathcal{X}$ 上的函数 $K: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ 称为 &lt;strong&gt;正定对称核函数&lt;/strong&gt;, 如果对任意 $x_1, x_2, \cdots, x_n \in \mathcal{X}$, 核矩阵 (Gram 矩阵) $[K_{ij}]_{m \times m}$ 是半正定的.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;常见的核函数有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性核函数: $K(x, x&#39;) = x \cdot x&#39;$, 对应线性支持向量机.&lt;/li&gt;
&lt;li&gt;多项式核函数: $K(x, x&#39;) = (x \cdot x&#39; + 1)^d, c \gt 0$&lt;/li&gt;
&lt;li&gt;高斯核函数: $K(x, x&#39;) = \exp\left(-\frac{\|x-x&#39;\|^2}{2\sigma^2}\right), \sigma \gt 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-2.pdf&#34; &gt;本节作业链接&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(1) —— 概述</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/intro/</link>
        <pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/intro/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/MachineLearning-1.pdf&#34; &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;基础数学工具&#34;&gt;基础数学工具
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机变量 $X$ 的 &lt;strong&gt;期望&lt;/strong&gt; $E[X]$ 定义为&lt;/p&gt;
$$
E[X] = \sum_{x} x \cdot P(X=x)
$$&lt;p&gt;随机变量 $X$ 的 &lt;strong&gt;方差&lt;/strong&gt; $\text{Var}(X)$ 定义为&lt;/p&gt;
$$
\text{Var}(X) = E[(X - E[X])^2]
$$&lt;p&gt;&lt;strong&gt;标准差&lt;/strong&gt; $\sigma(X)$ 定义为&lt;/p&gt;
$$
\sigma(X) = \sqrt{\text{Var}(X)}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Markov 不等式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $X$ 是一个非负随机变量, 期望存在, 那么对于任意 $t &gt; 0$ 有&lt;/p&gt;
$$
P(X \geq t) \leq \frac{E[X]}{t}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Chebyshev 不等式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $X$ 是一个随机变量, 期望和方差都存在, 那么对于任意 $t &gt; 0$ 有&lt;/p&gt;
$$
P(|X - E[X]| \geq t) \leq \frac{\text{Var}(X)}{t^2}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机变量 $X$ 和 $Y$ 的 &lt;strong&gt;协方差&lt;/strong&gt; $\text{Cov}(X, Y)$ 定义为&lt;/p&gt;
$$
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]
$$&lt;p&gt;如果 $\text{Cov}(X, Y) = 0$, 则称 $X$ 和 $Y$ &lt;strong&gt;不相关&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;协方差具有对称性, 双线性.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机向量 $X=(X_1, X_2, \ldots, X_n)$ 的 &lt;strong&gt;协方差矩阵&lt;/strong&gt; $C(X)$ 定义为&lt;/p&gt;
$$
C(X) = E[(X - E[X])(X - E[X])^T] = (\text{Cov}(X_i, X_j))_{ij}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gauss 分布&lt;/strong&gt; (正态分布) 的概率密度函数为&lt;/p&gt;
$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$&lt;p&gt;&lt;strong&gt;Laplace 分布&lt;/strong&gt; 的概率密度函数为&lt;/p&gt;
$$
f(x) = \frac{1}{2b} \exp(-\frac{|x-\mu|}{b})
$$&lt;/div&gt;
&lt;p&gt;最优化问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \min f(x) \\
\text{s.t. } &amp; c_i(x) \leq 0, i = 1, 2, \dots, k \\
&amp; h_j(x) = 0, j = 1, 2, \dots, l
\end{aligned}
$$&lt;p&gt;构造 Lagrange 函数&lt;/p&gt;
$$
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{k} \alpha_i c_i(x) + \sum_{j=1}^{l} \beta_j h_j(x)
$$&lt;p&gt;引入 Karush-Kuhn-Tucker (KKT) 条件&lt;/p&gt;
$$
\begin{aligned}
&amp; \nabla_x L(x, \alpha, \beta) = 0 \\
&amp; c_i(x) \leq 0, i = 1, 2, \dots, k \\
&amp; h_j(x) = 0, j = 1, 2, \dots, l \\
&amp; \alpha_i c_i(x) = 0, i = 1, 2, \dots, k \\
&amp; \alpha_i \geq 0, i = 1, 2, \dots, k
\end{aligned}
$$&lt;h2 id=&#34;基本概念和术语&#34;&gt;基本概念和术语
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;监督学习&lt;/strong&gt;: 基于标记数据 $T=\{ (x_i,y_i) \}_{i=1}^N$, 学习一个从输入空间到输出空间的映射 $f: \mathcal{X} \mapsto \mathcal{Y}$. 利用此对未见数据进行预测. 通常分为 &lt;strong&gt;回归&lt;/strong&gt; 和 &lt;strong&gt;分类&lt;/strong&gt; 两类.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt;: 基于未标记数据 $T=\{ x_i \}_{i=1}^N$, 发现其中隐含的知识模式. &lt;strong&gt;聚类&lt;/strong&gt; 是典型的无监督学习任务.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;半监督学习&lt;/strong&gt;: 既有标记数据又有未标记数据 (通常占比较大).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;: 通过观察环境的反馈, 学习如何选择动作以获得最大的奖励.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;模型评估与选择&#34;&gt;模型评估与选择
&lt;/h2&gt;&lt;h3 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h3&gt;&lt;p&gt;模型基于算法按照一定策略给出假设 $h \in \mathcal{H}$, 通过 &lt;strong&gt;损失函数&lt;/strong&gt; $L(h(x), y)$ 衡量假设的好坏.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0-1 损失函数:&lt;/li&gt;
&lt;/ul&gt;
$$L(h(x), y) = \mathbb{I}(h(x) \neq y) = \begin{cases} 0, &amp; h(x) = y \\ 1, &amp; h(x) \neq y \end{cases}$$&lt;ul&gt;
&lt;li&gt;平方损失函数:&lt;/li&gt;
&lt;/ul&gt;
$$L(h(x), y) = (h(x) - y)^2$$&lt;p&gt;平均损失 $R(h) = E_{x \sim D} [L(h(x), y)]$ 称为 &lt;strong&gt;泛化误差&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;容易验证, 对于 0-1 损失函数, 准确率 $a = 1-R(h)$.&lt;/p&gt;
&lt;h3 id=&#34;二分类&#34;&gt;二分类
&lt;/h3&gt;&lt;p&gt;对于二分类问题, 样本预测结果有四种情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;真正例&lt;/strong&gt; (True Positive, TP): 预测为正例, 实际为正例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;假正例&lt;/strong&gt; (False Positive, FP): 预测为正例, 实际为负例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真负例&lt;/strong&gt; (True Negative, TN): 预测为负例, 实际为负例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;假负例&lt;/strong&gt; (False Negative, FN): 预测为负例, 实际为正例.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由此引入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准确率(查准率):&lt;/strong&gt; $P = \frac{TP}{TP+FP}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;召回率(查全率):&lt;/strong&gt; $R = \frac{TP}{TP+FN}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$F_1$ 度量:&lt;/strong&gt; 考虑到二者抵触, 引入调和均值 $F_1 = \frac{2PR}{P+R}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;过拟合和正则化&#34;&gt;过拟合和正则化
&lt;/h3&gt;&lt;p&gt;为了防止由于模型过于复杂而导致的过拟合, 可以通过 &lt;strong&gt;正则化&lt;/strong&gt; 方法来限制模型的复杂度.&lt;/p&gt;
$$
\min \sum_{i=1}^{N} L(h(x_i), y_i) + \lambda J(h)
$$&lt;p&gt;其中 $J(h)$ 是随着模型复杂度增加而增加的函数. $\lambda$ 是正则化参数.&lt;/p&gt;
&lt;p&gt;怎么选取合适的 $\lambda$ ? 一般是先给出若干候选, 在验证集上进行评估, 选取泛化误差最小的.&lt;/p&gt;
&lt;h3 id=&#34;数据集划分&#34;&gt;数据集划分
&lt;/h3&gt;&lt;p&gt;一般将数据集划分为 &lt;strong&gt;训练集&lt;/strong&gt; $T$ 和 &lt;strong&gt;测试(验证)集&lt;/strong&gt; $T^\prime$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;留出法 (hold-out)&lt;/strong&gt;: 分层无放回地随机采样. 也叫简单交叉验证.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$k$ 折交叉验证 ($k$-fold cross validation)&lt;/strong&gt;: 将数据集分为 $k$ 个大小相等的子集, 每次取其中一个作为验证集, 其余作为训练集, 最后以这 $k$ 次的平均误差作为泛化误差的估计. 当 $k=|D|$ 时称为留一 (leave-one-out) 验证法.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自助法 (bootstrapping)&lt;/strong&gt;: 从数据集中&lt;em&gt;有放回地&lt;/em&gt;采样 $|D|$ 个数据作为训练集, 没抽中的作为验证集. 因而训练集 $T$ 和原始数据集 $D$ 的分布未必一致, 对数据分布敏感的模型不适用.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;偏差-方差分解&#34;&gt;偏差-方差分解
&lt;/h2&gt;&lt;p&gt;为什么泛化误差会随着模型复杂度的增加而先减小后增大?&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;偏差&lt;/strong&gt; (bias): 模型预测值的期望与真实值之间的差异. 体现了模型的拟合能力.&lt;/p&gt;
$$\text{Bias}(x) = E_T[h_T(x)-c(x)] = \bar{h}(x) - c(x)$$&lt;p&gt;&lt;strong&gt;方差&lt;/strong&gt; (variance): 模型预测值的方差. 体现了模型的对数据扰动的稳定性.&lt;/p&gt;
$$\text{Var}(x) = E[(h(x) - \bar{h}(x))^2]$$&lt;/div&gt;
&lt;p&gt;现在对泛化误差进行分解:&lt;/p&gt;
$$
\begin{aligned}
R(h) &amp;= E_T[(h_T(x) - c(x))^2] \\
&amp;= E_T[h_T^2(x) - 2h_T(x)c(x) + c^2(x)] \\
&amp;= E_T[h_T^2(x)] - 2c(x)E_T[h_T(x)] + c^2(x) \\
&amp;= E_T[h_T^2(x)] - \bar{h}^2(x) + \bar{h}^2(x) - 2\bar{h}(x)c(x) + c^2(x) \\
&amp;= E_T[(h_T(x) - \bar{h}(x))^2] + (\bar{h}(x) - c(x))^2 \\
&amp;= \text{Var}(x) + \text{Bias}^2(x)
\end{aligned}
$$&lt;p&gt;当然, 由于噪声存在, $y$ 未必一定等于 $c(x)$, 不妨设 $y=c(x)+\varepsilon$, 其中 $\varepsilon \sim \Epsilon$ 期望为 $0$. 可以证明&lt;/p&gt;
&lt;div class=&#34;block&#34;&gt;
&lt;p class=&#34;block-title thm&#34;&gt;定理&lt;span class=&#34;subtitle&#34;&gt;偏差-方差分解&lt;/p&gt;
$$
E_{T \sim D^{|T|}, \varepsilon \sim \Epsilon} [(h_T(x)-y)^2] = \text{Bias}^2(x) + \text{Var}(x) + E[\varepsilon^2]
$$&lt;p&gt;即泛化误差可以分解为偏差、方差和噪声三部分.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;起初, 模型较为简单, 偏差在泛化误差起主导作用. 随着模型复杂度的增加, 拟合能力增强, 偏差减小, 但带来过拟合风险, 算法对数据扰动敏感, 方差增大. 方差占比逐渐增大, 最终导致泛化误差增大.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://LeoDreamer2004.github.io/materials/machine-learning/homework-1.pdf&#34; &gt;本节作业链接&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>最优化方法(4) —— 优化问题</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/opt-problem/</link>
        <pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/opt-problem/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/lect/06-opt-dzw-pku.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;凸优化&#34;&gt;凸优化
&lt;/h2&gt;&lt;p&gt;凸问题的可行集都是凸集.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;凸优化问题的任意局部极小点都是全局最优点.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;假设 $x$ 是局部极小, $y$ 全局最优且 $f(y) &lt; f(x)$.&lt;/p&gt;
&lt;p&gt;考虑 $z = \theta x + (1-\theta) y$, 则由于 $z$ 是可行点的凸组合, 也是可行点. 由于 $f$ 是凸函数, 有&lt;/p&gt;
$$
f(z) \leq \theta f(x) + (1-\theta) f(y) &lt; f(x)
$$&lt;p&gt;取 $\theta \to 1$, 则 $f(z) \to f(x)$, 与局部最小性矛盾.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;线性规划&#34;&gt;线性规划
&lt;/h2&gt;&lt;p&gt;所谓 &lt;strong&gt;线性规划(LP)&lt;/strong&gt; 问题是指目标函数和约束条件都是线性的优化问题. 一般形式如下:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; c^T x \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; Gx \le e
\end{aligned}
$$&lt;p&gt;$\ell_1$ 和 $\ell_\infty$ 范数实际上也是线性的.&lt;/p&gt;
&lt;h3 id=&#34;示例-最大球问题&#34;&gt;示例: 最大球问题
&lt;/h3&gt;&lt;p&gt;凸多边形&lt;/p&gt;
$$P = \{ x \mid a_i^Tx \le b_i \}$$&lt;p&gt;的 Chebyshev 中心是最大半径内切球的中心. 代入得&lt;/p&gt;
$$
\sup \{a_i^T (x_c + u) \mid \Vert u \Vert_2 \le r \} = a_i^Tx_c + r \Vert a_i \Vert_2 \le b_i
$$&lt;p&gt;这也变成了一个线性规划问题.&lt;/p&gt;
&lt;h2 id=&#34;二次规划&#34;&gt;二次规划
&lt;/h2&gt;&lt;p&gt;二次规划问题是指目标函数是二次的的优化问题.&lt;/p&gt;
&lt;p&gt;例如, 对于线性约束条件的问题, 一般形式如下:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \frac{1}{2} x^T P x + q^T x + r \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; Gx \le e
\end{aligned}
$$&lt;p&gt;也有 &lt;strong&gt;带二次约束的二次规划 (QCQP)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;我们归结为 &lt;strong&gt;二次锥规划 (SOCP)&lt;/strong&gt;:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f^T x \\
\text{s.t.} \quad &amp; \Vert A_i x + b_i \Vert_2 \le c_i^T x + d_i, \quad i = 1, \ldots, m \\
&amp; Fx = g
\end{aligned}
$$&lt;h3 id=&#34;示例-最小范数问题&#34;&gt;示例: 最小范数问题
&lt;/h3&gt;&lt;p&gt;令 $\bar{v}_i = A_ix+b_i \in \mathbb{R}^{n_i}$, 则 $\min_x \sum_i \Vert \bar{v}_i \Vert_2$ 等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \sum_i v_{i0} \\
\text{s.t.} \quad &amp;\bar{v}_i = A_i x + b_i \\
&amp;(v_{i0}, \bar{v}_i) \succeq_\mathcal{Q} 0
\end{aligned}
$$&lt;p&gt;其中 $\mathcal{Q}$ 是二次锥.&lt;/p&gt;
&lt;h3 id=&#34;示例-最小化最大函数和问题&#34;&gt;示例: 最小化最大函数和问题
&lt;/h3&gt;&lt;p&gt;设 $\theta(x)=(\theta_1(x), \theta_2(x), \cdots, \theta_m(x))^T$. $\theta_{[i]}$ 是 $\theta_i$ 的非递增方式的排序. 则 $\min_{x\in Q} \sum_{i=1}^m \theta_{[i]}(x)$ 等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \sum_{i=1}^m u_i + kt \\
\text{s.t.} \quad &amp; x \in Q \\
&amp; \theta_i(x)\le u_i + t \\
&amp; u_i \ge 0
\end{aligned}
$$&lt;h2 id=&#34;半定优化&#34;&gt;半定优化
&lt;/h2&gt;&lt;p&gt;半定优化 (SDP) 一般形式如下:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; c^Tx \\
\text{s.t.} \quad &amp; x_1A_1 + \cdots + x_nA_n + B \succeq 0 \\
&amp; Gx=h
\end{aligned}
$$&lt;p&gt;其实是线性规划在矩阵空间的推广. 仍然考虑标准形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \left&lt; C, X \right&gt; \\
\text{s.t.} \quad &amp; \left&lt; A_i, X \right&gt; = b_i, \quad i = 1, \ldots, m \\
&amp; X \succeq 0
\end{aligned}
$$&lt;p&gt;和对偶形式:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \sum_{i=1}^m b_i y_i \\
\text{s.t.} \quad &amp; C - \sum_{i=1}^m y_i A_i \succeq 0
\end{aligned}
$$&lt;h3 id=&#34;示例-二次约束二次规划问题&#34;&gt;示例: 二次约束二次规划问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; x^T A_0 x + 2b_0^T x + c_0 \\
\text{s.t.} \quad &amp; x^T A_i x + 2b_i^T x + c_i \le 0, \quad i = 1, \ldots, m
\end{aligned}
$$&lt;p&gt;其中 $A_i$ 是 $n \times n$ 的对称矩阵, 这个问题在 $A_i$ 不定时实际上是 NP-hard 的. 考虑它的半定松弛, 记 $X=x^Tx$ 注意到有&lt;/p&gt;
$$
x^T A_i x + 2b_i^T x + c_i = \left&lt; A_i, X \right&gt; + 2\left&lt; b_i, x \right&gt; + c_i = \left&lt;
\begin{bmatrix}
A_i &amp; b_i \\
b_i^T &amp; c_i
\end{bmatrix},
\begin{bmatrix}
X &amp; x \\
x^T &amp; 1
\end{bmatrix}
\right&gt;
$$&lt;p&gt;我们记作 $\left&lt; \bar{A}_i, \bar{X} \right&gt;$. 注意到, 现在唯一的非线性部分是约束 $X=xx^T$, 我们将其松弛成半正定约束 $X \succeq xx^T$. 可以证明, $\bar{X} \succeq 0$ 等价于 $X \succeq xx^T$. 这样我们就得到了一个半定优化问题:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \left&lt; A_0, X \right&gt; \\
\text{s.t.} \quad &amp; \left&lt; \bar{A}_i, \bar{X} \right&gt; \le 0, \quad i = 1, \ldots, m \\
&amp; \bar{X} \succeq 0 \\
&amp; \bar{X}_{n+1,n+1} = 1
\end{aligned}
$$&lt;h3 id=&#34;示例-最大割问题&#34;&gt;示例: 最大割问题
&lt;/h3&gt;&lt;p&gt;令 $G$ 为一个无向图, 节点集为 $V = \{1, 2, \cdots, n\}$, 边集为 $E$. 设 $w_{ij} = w_{ji} \ge 0$ 为边 $(i, j) \in E$ 上的权重, 要找 $S \subseteq V$ 使得 $S$ 与 $\bar{S}$ 之间相连边的权重之和最大化.&lt;/p&gt;
&lt;p&gt;我们定义 $x_j = 1, j \in S$ 和 $x_j = -1, j \in \bar{S}$, 则&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \sum_{(i, j) \in E} \frac{1}{2} (1-x_i x_j) w_{ij} \\
\text{s.t.} \quad &amp; x_i = \pm 1, \quad i = 1, \ldots, n
\end{aligned}
$$&lt;p&gt;然而这是一个离散优化问题, 考虑对它做松弛. 令 $W=(w_{ij}) \in \mathbb{S}^n$ 为权重矩阵, $C=-\frac{1}{4}(\text{Diag}(W\mathbf{1})-W)$ 是 Laplacian 矩阵的 $-1/4$ 倍. 则&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; x^T C x \\
\text{s.t.} \quad &amp; x_i^2 = 1, \quad i = 1, \ldots, n
\end{aligned}
$$&lt;p&gt;仍令 $X=x^Tx$, 则容易看出与下问题等价:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \left&lt; C, X \right&gt; \\
\text{s.t.} \quad &amp; X_{ii} = 1, \quad i = 1, \ldots, n \\
&amp; X \succeq 0 \\
&amp; \text{rank}(X) = 1
\end{aligned}
$$&lt;h3 id=&#34;示例-极小化最大特征值问题&#34;&gt;示例: 极小化最大特征值问题
&lt;/h3&gt;$$
\min \quad \lambda_{\max}(A_0 + \sum_{i=1}^m x_i A_i)
$$&lt;p&gt;注意到:&lt;/p&gt;
$$\lambda_{\max}(A) \le t \Leftrightarrow A \preceq tI$$&lt;p&gt;于是我们有 SDP 形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; z \\
\text{s.t.} \quad &amp; A_0 + \sum_{i=1}^m x_i A_i \preceq zI
\end{aligned}
$$&lt;h3 id=&#34;示例-极小化二范数问题&#34;&gt;示例: 极小化二范数问题
&lt;/h3&gt;$$
\min \quad \Vert A_0 + \sum_{i=1}^m x_i A_i \Vert_2
$$&lt;p&gt;记 $A = A_0 + \sum_{i=1}^m x_i A_i$. 注意到:&lt;/p&gt;
$$
\Vert A \Vert_2 \le t \Leftrightarrow A^TA \preceq t^2I \Leftrightarrow
\begin{bmatrix}
tI &amp; A \\
A^T &amp; tI
\end{bmatrix} \succeq 0
$$&lt;p&gt;于是我们有 SDP 形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; t \\
\text{s.t.} \quad &amp; \begin{bmatrix}
tI &amp; A \\
A &amp; tI
\end{bmatrix} \succeq 0
\end{aligned}
$$&lt;h3 id=&#34;示例-特征值优化问题&#34;&gt;示例: 特征值优化问题
&lt;/h3&gt;$$
\min \quad \sum_{i=1}^n \lambda_{[i]}(A_0 + \sum_{j=1}^m x_j A_j)
$$&lt;p&gt;其中 $\lambda_{[i]}(A)$ 表示 $A$ 的第 $i$ 大特征值. 前面的极小最大函数和提到它等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \sum_{i=1}^n u_i + kt \\
\text{s.t.} \quad &amp; u_i+t \ge \lambda_i(A_0 + \sum_{j=1}^m x_j A_j), \quad i = 1, \ldots, n \\
&amp; u_i \ge 0
\end{aligned}
$$&lt;p&gt;设 $u_i = \lambda_i(X)$, 则写成 SDP 形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; kt + \text{Tr}(X) \\
\text{s.t.} \quad &amp; tI + X \succeq A_0 + \sum_{j=1}^m z_j A_j \\
&amp; X \succeq 0
\end{aligned}
$$</description>
        </item>
        <item>
        <title>最优化方法(3) —— 凸函数</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/convex-function/</link>
        <pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/convex-function/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/lect/03_functions_newhyx.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;基本线性代数知识&#34;&gt;基本线性代数知识
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定函数 $f: \mathbb{R}^n \mapsto \mathbb{R}$, 且 $f$ 在 $x$ 一个邻域内有定义, 若存在 $g \in \mathbb{R}^n$, 使得&lt;/p&gt;
$$
\lim_{p \to 0} \frac{f(x+p)-f(x)-g^Tp}{\Vert p \Vert} = 0
$$&lt;p&gt;其中 $\Vert \cdot \Vert$ 是向量范数, 则称 $f$ 在 $x$ 处 &lt;strong&gt;可微&lt;/strong&gt;. 此时, $g$ 称为 $f$ 在 $x$ 处的 &lt;strong&gt;梯度&lt;/strong&gt;, 记为 $\nabla f(x)$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;显然, 如果梯度存在, 令 $p = \varepsilon e_i$, 易得&lt;/p&gt;
$$
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果函数 $f(x): \mathbb{R}^n \mapsto \mathbb{R}$ 在点 $x$ 处的二阶偏导数 $\dfrac{\partial^2 f}{\partial x_i \partial x_j}$ 存在, 则称 $f$ 在 $x$ 处 &lt;strong&gt;二次可微&lt;/strong&gt;. 此时, $n \times n$ 矩阵&lt;/p&gt;
$$
\nabla^2 f(x) = \begin{pmatrix}
\dfrac{\partial^2 f}{\partial x_1^2} &amp; \dfrac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial x_1 \partial x_n} \\
\dfrac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \dfrac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dfrac{\partial^2 f}{\partial x_n \partial x_1} &amp; \dfrac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$&lt;p&gt;称为 $f$ 在 $x$ 处的 &lt;strong&gt;Hessian 矩阵&lt;/strong&gt;. 若 $\nabla^2 f(x)$ 在 $D$ 上连续, 则称 $f$ 在 $D$ 上 &lt;strong&gt;二次连续可微&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;可以证明, 若 $f$ 在 $D$ 上二次连续可微, 则 $\nabla^2 f(x)$ 为对称矩阵.&lt;/p&gt;
&lt;p&gt;多元函数的梯度可以推广到变量是矩阵的情形.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定函数 $f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$, 且 $f$ 在 $X$ 一个邻域内有定义, 若存在 $G \in \mathbb{R}^{m \times n}$, 使得&lt;/p&gt;
$$
\lim_{V \to 0} \frac{f(X+V)-f(X)-\left&lt; G, V \right&gt;}{\Vert V \Vert} = 0
$$&lt;p&gt;其中 $\Vert \cdot \Vert$ 是矩阵范数, 则称 $f$ 在 $X$ 处 &lt;strong&gt;(Fréchet)可微&lt;/strong&gt;. 此时, $G$ 称为 $f$ 在 $X$ 处的 &lt;strong&gt;梯度&lt;/strong&gt;, 记为 $\nabla f(X)$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;矩阵的可微有另一种较为简单常用的定义.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定函数 $f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$, 若存在矩阵 $G \in \mathbb{R}^{m \times n}$, 使得&lt;/p&gt;
$$
\lim_{t \to 0} \frac{f(X+tV)-f(X)}{t} = \left&lt; G, V \right&gt;
$$&lt;p&gt;则称 $f$ 在 $X$ 处 &lt;strong&gt;(Gâteaux)可微&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$f(X) = \text{tr}(AX^TB)$, 此时 $\nabla f(X) = BA$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f(X, Y)=\frac{1}{2} \Vert XY-A \Vert_F^2$. 此时&lt;/p&gt;
$$
    \begin{aligned}
    &amp;f(X,Y+tV)-f(X,Y) \\
    &amp;= \frac{1}{2} \Vert X(Y+tV)-A \Vert_F^2 - \frac{1}{2} \Vert XY-A \Vert_F^2 \\
    &amp;= \frac{1}{2} \Vert XY - A + tVX \Vert_F^2 - \frac{1}{2} \Vert XY - A \Vert_F^2 \\
    &amp;= \frac{1}{2} \Vert tVX \Vert_F^2 + \left&lt; XY-A, tVX \right&gt; \\
    &amp;= t \left&lt; X^T(XY-A), V \right&gt; + o(t)
    \end{aligned}
    $$&lt;p&gt;所以 $\frac{\partial f}{\partial Y} = X^T(XY-A)$, 类似地, $\frac{\partial f}{\partial X} = (XY-A)Y^T$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f(X)=\ln\text{det}(X)$, $X$ 为正定矩阵. 此时&lt;/p&gt;
$$
    \begin{aligned}
    &amp;f(X+tV)-f(X) \\
    &amp;= \ln\text{det}(X+tV) - \ln\text{det}(X) \\
    &amp;= \ln\text{det}(I+tX^{-1/2}VX^{-1/2})
    \end{aligned}
    $$&lt;p&gt;考虑 $X^{-1/2}VX^{-1/2}$ 的特征值 $\lambda_i$, 则由特征值之和为迹, 有&lt;/p&gt;
$$
    \begin{aligned}
    &amp;= \ln\text{det}\prod_{i=1}^n (1+t\lambda_i) \\
    &amp;= \sum_{i=1}^n \ln(1+t\lambda_i) \\
    &amp;= \sum_{i=1}^n t\lambda_i + o(t) \\
    &amp;= t\text{tr}(X^{-1/2}VX^{-1/2}) + o(t) \\
    &amp;= t\text{tr}(X^{-1}V) + o(t) \\
    &amp;= t\left&lt; X^{-T}, V \right&gt; + o(t)
    \end{aligned}
    $$&lt;p&gt;所以 $\nabla f(X) = X^{-T}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;广义实数&lt;/strong&gt; 是一种扩充实数域的数, 记为 $\bar{\mathbb{R}} = \mathbb{R} \cup \{ \pm \infty \}$. 映射 $f: \mathbb{R}^n \mapsto \bar{\mathbb{R}}$ 称为 &lt;strong&gt;广义实值函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定广义实值函数 $f$ 和非空集合 $X$. 如果存在 $x \in X$ 使得 $f(x) &lt; +\infty$, 并且对任意的 $x \in X$, 都有 $f(x) &gt; -\infty$, 那么称函数 $f$ 关于集合 $X$ 是 &lt;strong&gt;适当的&lt;/strong&gt;．&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于广义实值函数 $f: \mathbb{R}^n \mapsto \bar{\mathbb{R}}$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$C_\alpha = \{x \mid f(x) \le \alpha \}$ 称为 $f$ 的 &lt;strong&gt;$\alpha$-下水平集&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$\text{epi} f = \{ (x, t) \mid f(x) \le t \}$ 称为 $f$ 的 &lt;strong&gt;上方图&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;若 $\text{epi} f$ 为闭集, 则称 $f$ 为&lt;strong&gt;闭函数&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;若对任意的 $x \in \mathbb{R}^n$, 有 $\liminf_{y \to x} f(y) \ge f(x)$, 则称 $f$ 为 &lt;strong&gt;下半连续函数&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;对于广义实值函数 $f$, 以下命题等价:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f(x)$ 的任意 $\alpha$-下水平集都是闭集;&lt;/li&gt;
&lt;li&gt;$f(x)$ 是下半连续的;&lt;/li&gt;
&lt;li&gt;$f(x)$ 是闭函数.&lt;/li&gt;
&lt;/ol&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;(1) $\Rightarrow$ (2): 反证, 假设 $x_k \to \bar{x}$ 但 $\liminf_{k \to \infty} f(x_k) &lt; f(\bar{x})$. 取 $t$ 介于二者之间.&lt;/p&gt;
&lt;p&gt;考虑到 $\liminf_{k \to \infty} f(x_k) &lt; t$, 则有无穷多 $x_k$ 使得 $f(x_k) \le t$, 即这些 $x_k$ 在 $C_t$ 中. 由于 $C_t$ 是闭集, 则 $\bar{x} \in C_t$, 即 $f(\bar{x}) \le t$, 矛盾.&lt;/p&gt;
&lt;p&gt;(2) $\Rightarrow$ (3): 考虑 $(x_k,y_k) \in \text{epi} f \to (\bar{x},\bar{y})$, 由于 $f$ 下半连续, 则&lt;/p&gt;
$$ f(\bar{x}) \le \liminf_{k \to \infty} f(x_k) = \liminf_{k \to \infty} y_k = \bar{y} $$&lt;p&gt;即 $(\bar{x}, \bar{y}) \in \text{epi} f$.&lt;/p&gt;
&lt;p&gt;(3) $\Rightarrow$ (1): 考虑 $x_k \in C_\alpha \to \bar{x}$, 则 $(x_k, \alpha) \in \text{epi} f \to (\bar{x}, \alpha)$, 所以 $(\bar{x}, \alpha) \in \text{epi} f$, 即 $f(\bar{x}) \le \alpha$, 所以 $\bar{x} \in C_\alpha$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;适当闭函数的和, 复合, 逐点上确界仍然是闭函数.&lt;/p&gt;
&lt;h2 id=&#34;凸函数&#34;&gt;凸函数
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;适当函数 $f: \mathbb{R}^n \mapsto \mathbb{R}$ 称为 &lt;strong&gt;凸函数&lt;/strong&gt;, 如果 $\text{dom} f$ 是凸集, 且对任意的 $x, y \in \text{dom} f$ 和 $\theta \in [0,1]$, 有&lt;/p&gt;
$$
f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)
$$&lt;/div&gt;
&lt;p&gt;易知仿射函数既是凸函数又是凹函数. 所有的范数都是凸函数.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;若存在常数 $m &gt; 0$, 使得 $g(x) = f(x) - \frac{m}{2} \Vert x \Vert^2$ 是凸函数, 则称 $f$ 是 &lt;strong&gt;强凸函数&lt;/strong&gt;, $m$ 称为 &lt;strong&gt;强凸参数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;凸函数判定定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;适当函数 $f: \mathbb{R}^n \mapsto \mathbb{R}$ 是凸函数的充要条件是, 对任意的 $x \in \text{dom} f$, 函数 $g: \mathbb{R} \mapsto \mathbb{R}$ 是凸函数, 其中&lt;/p&gt;
$$g(t) = f(x+tv), \quad \text{dom}g = \{ t \mid x + tv \in \text{dom} f \}$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;一阶条件&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于定义在凸集上的可微函数 $f$, $f$ 是凸函数当且仅当&lt;/p&gt;
$$
f(y) \ge f(x) + \nabla f(x)^T(y-x), \quad \forall x, y \in \text{dom} f
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必要性&lt;/strong&gt;: 设 $f$ 凸, 则 $\forall x, y \in \text{dom} f, t \in [0,1]$, 有&lt;/p&gt;
$$tf(y)+(1-t)f(x) \ge f(x+t(y-x))$$&lt;p&gt;令 $t \to 0$, 即&lt;/p&gt;
$$f(y)-f(x) \ge \frac{f(x+t(y-x))-f(x)}{t} \to \nabla f(x)^T(y-x)$$&lt;p&gt;&lt;strong&gt;充分性&lt;/strong&gt;: $\forall x, y \in \text{dom}f, t\in (0,1)$, 取 $z = tx+(1-t)y$, 则&lt;/p&gt;
$$
\begin{aligned}
f(x) &amp;\ge f(z) + \nabla f(z)^T(x-z)  \\
f(y) &amp;\ge f(z) + \nabla f(z)^T(y-z)
\end{aligned}
$$&lt;p&gt;一式乘以 $t$, 二式乘以 $1-t$, 相加即得.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;梯度单调性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $f$ 为可微函数, 则 $f$ 为凸函数当且仅当 $\text{dom} f$ 为凸集且 $\nabla f$ 为单调映射.&lt;/p&gt;
$$(\nabla f(x) - \nabla f(y))^T(x-y) \ge 0$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必要性&lt;/strong&gt;: 根据一阶条件, 有&lt;/p&gt;
$$
\begin{aligned}
f(y) &amp;\ge f(x) + \nabla f(x)^T(y-x) \\
f(x) &amp;\ge f(y) + \nabla f(y)^T(x-y)
\end{aligned}
$$&lt;p&gt;相加即可.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;充分性&lt;/strong&gt;: 考虑 $g(t)=f(x+t(y-x))$, 则 $g^\prime(t)=\nabla f(x+t(y-x))^T (y-x)$, 从而 $g^\prime (t) \ge g^\prime (0)$.&lt;/p&gt;
$$
\begin{aligned}
f(y) &amp;= g(1) = g(0) + \int_{0}^1 g^\prime(t) dt \\
&amp;\ge g(0) + \int_{0}^1 g^\prime(0) dt = g(0) + g^\prime(0) \\ &amp;= f(x) + \nabla f(x)^T(y-x)
\end{aligned}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;函数 $f(x)$ 是凸函数当且仅当 $\text{epi}f$ 是凸集.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;二阶条件&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $f$ 为定义在凸集上的二阶连续可微函数, $f$ 是凸函数当且仅当 $\nabla^2 f(x) \succeq 0, \forall x \in \text{dom} f$. 若不取等, 则为严格凸函数.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必要性&lt;/strong&gt;: 反设 $f(x)$ 在 $x$ 处 $\nabla^2 f(x) \prec 0$, 则存在 $v \in \mathbb{R}^n$, 使得 $v^T \nabla^2 f(x) v &lt; 0$, 考虑 Peano 余项&lt;/p&gt;
$$
f(x+tv)=f(x)+t\nabla f(x)^Tv+\frac{t^2}{2}v^T\nabla^2 f(x+tv)v + o(t^2)
$$&lt;p&gt;取 $t$ 充分小,&lt;/p&gt;
$$
\frac{f(x+tv)-f(x)-t\nabla f(x)^T v}{t^2}=\frac{1}{2}v^T\nabla^2 f(x+tv)v + o(1) &lt; 0
$$&lt;p&gt;这和一阶条件矛盾.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;充分性&lt;/strong&gt;: 对于任意的 $x, y \in \text{dom} f$, 有&lt;/p&gt;
$$
\begin{aligned}
f(y) &amp;= f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^T\nabla^2 f(z)(y-x) \\
    &amp;\ge f(x)+\nabla f(x)^T(y-x)
\end{aligned}
$$&lt;p&gt;由一阶条件, $f$ 为凸函数.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;保凸运算&#34;&gt;保凸运算
&lt;/h2&gt;&lt;p&gt;下面举一些重要的例子.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;逐点取上界: 若对每个 $y \in A$, $f(x,y)$ 都是关于 $x$ 的凸函数, 则&lt;/p&gt;
$$g(x)=\sup_{y \in A} f(x,y)$$&lt;p&gt;也是凸函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$C$ 的支撑函数 $f(x)=\sup_{y \in C} y^Tx$ 是凸函数.&lt;/li&gt;
&lt;li&gt;$C$ 到 $x$ 的最远距离 $f(x)=\sup_{y \in C} \Vert x-y \Vert$ 是凸函数.&lt;/li&gt;
&lt;li&gt;对称阵 $X \in \mathbb{S}^n$ 的最大特征值 $\lambda_{\max}(X)=\sup_{\Vert x \Vert=1} x^TXx$ 是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标量函数的复合: 若 $g: \mathbb{R}^n \mapsto \mathbb{R}$ 是凸函数, $h: \mathbb{R} \mapsto \mathbb{R}$ 是单调不减的凸函数, 则&lt;/p&gt;
$$f(x) = h(g(x))$$&lt;p&gt;也是凸函数. 凹同理.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $g$ 凸, 则 $f(x) = \exp(g(x))$ 也是凸函数.&lt;/li&gt;
&lt;li&gt;如果 $g$ 凹, 则 $f(x) = 1/g(x)$ 也是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;取下确界: 若 $f(x, y)$ 关于 $(x, y)$ 整体是凸函数, $C$ 是凸集, 则&lt;/p&gt;
$$g(x) = \inf_{y \in C} f(x, y)$$&lt;p&gt;也是凸函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;凸集 $C$ 到 $x$ 的距离 $f(x)=\inf_{y \in C} \Vert x-y \Vert$ 是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;透视函数: 若 $f: \mathbb{R}^{n} \mapsto \mathbb{R}$ 是凸函数, 则&lt;/p&gt;
$$g(x, t) = tf(x/t), \quad \text{dom} g = \{ (x, t) \mid x / t \in \text{dom} f, t &gt; 0 \}$$&lt;p&gt;也是凸函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相对熵函数 $g(x,t)=t\log t-t\log x$ 是凸函数.&lt;/li&gt;
&lt;li&gt;若 $f$ 凸, 则 $g(x)=(c^T+d)f((Ax+b)/(c^T+d))$ 也是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;共轭函数: 任意适当函数 $f$ 的共轭函数&lt;/p&gt;
$$f^\ast(y)=\sup_{x \in \text{dom} f} (\left&lt; x, y \right&gt; - f(x))$$&lt;p&gt;是凸函数.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;凸函数的推广&#34;&gt;凸函数的推广
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;$f: \mathbb{R}^n \mapsto \mathbb{R}$ 称为 &lt;strong&gt;拟凸的&lt;/strong&gt;, 如果 $\text{dom} f$ 是凸集, 且对任意 $\alpha$, 下水平集 $C_\alpha$ 是凸集.&lt;/p&gt;
&lt;p&gt;若 $f$ 是拟凸的, 则称 $-f$ 是 &lt;strong&gt;拟凹的&lt;/strong&gt;. 若 $f$ 既是拟凸又是拟凹的, 则称 $f$ 是 &lt;strong&gt;拟线性的&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;注意: 拟凸函数的和不一定是拟凸函数.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拟凸函数满足类 Jenson 不等式: 对拟凸函数 $f$ 和 $\forall x, y \in \text{dom} f, \theta \in [0,1]$, 有&lt;/li&gt;
&lt;/ul&gt;
$$f(\theta x + (1-\theta)y) \le \max\left\{f(x),f(y)\right\}$$&lt;ul&gt;
&lt;li&gt;拟凸函数满足一阶条件: 定义在凸集上的可微函数 $f$ 拟凸当且仅当&lt;/li&gt;
&lt;/ul&gt;
$$f(y) \le f(x) \Rightarrow \nabla f(x)^T(y-x) \le 0$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果正值函数 $f$ 满足 $\log f$ 是凸函数, 则 $f$ 称为 &lt;strong&gt;对数凸函数&lt;/strong&gt;; 若为凹函数, 则 $f$ 称为 &lt;strong&gt;对数凹函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;例如, 正态分布&lt;/p&gt;
$$f(x) = \frac{1}{\sqrt{(2\pi)^n \text{det} \Sigma}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$&lt;p&gt;是对数凹函数.&lt;/p&gt;
&lt;p&gt;对数凹函数的乘积, 积分都是对数凹的, 但加和不一定是对数凹的.&lt;/p&gt;
&lt;p&gt;在广义不等式下, 也可以定义凸凹性.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
$$f(\theta x+(1-\theta)y \preceq_K \theta f(x)+(1-\theta)f(y))$$&lt;p&gt;
对任意 $x,y \in \text{dom} f, 0 \le \theta \le 1$ 成立.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;例如, $f: \mathbb{S}^m \mapsto \mathbb{S}^m$, $f(X)=X^2$ 是 $\mathbb{S}^m_+$-凸函数. 这点利用 $z^TX^2z=\Vert Xz \Vert^2$ 是关于 $X$ 的凸函数即可得知.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>最优化方法(2) —— 凸集</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/convex-set/</link>
        <pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/convex-set/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/lect/02-convex-set.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;范数&#34;&gt;范数
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;记号 $\Vert \cdot \Vert: \mathbb{R}^n \mapsto \mathbb{R}$ 称为 &lt;strong&gt;向量范数&lt;/strong&gt;, 若满足:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正定性: $\Vert x \Vert \geq 0$, 且 $\Vert x \Vert = 0 \Leftrightarrow x = 0$;&lt;/li&gt;
&lt;li&gt;齐次性: $\Vert \alpha x \Vert = \vert \alpha \vert \Vert x \Vert$;&lt;/li&gt;
&lt;li&gt;三角不等式: $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;$\ell_p$ 范数是最常见的向量范数&lt;/p&gt;
$$
\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right) ^{\frac{1}{p}}
$$&lt;p&gt;特别地, 当 $p = \infty$ 时, $\Vert x \Vert_\infty = \max_i \vert x_i \vert$.&lt;/p&gt;
&lt;p&gt;向量范数可以自然地推广到矩阵范数. 常见的矩阵范数有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;和范数&lt;/strong&gt;: $\Vert A \Vert_1 = \sum_{i,j} \vert A_{ij} \vert$;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frobenius 范数&lt;/strong&gt;: $\Vert A \Vert_F = \sqrt{\sum_{i,j} A_{ij} ^2} = \sqrt{\text{tr}(A^T A)}$;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算子范数&lt;/strong&gt;: $\Vert A \Vert_{(m,n)}=\max_{\Vert x \Vert_n = 1} \Vert Ax \Vert_m$. 特别地, 当 $m = n = p$ 时:
&lt;ul&gt;
&lt;li&gt;$p=1$ 时, $\Vert A \Vert_{p=1} = \max_j \sum_i \vert A_{ij} \vert$;&lt;/li&gt;
&lt;li&gt;$p=2$ 时, $\Vert A \Vert_{p=2} = \sqrt{\lambda_{\max}(A^T A)}$, 亦称为 &lt;strong&gt;谱范数&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$p=\infty$ 时, $\Vert A \Vert_{p=\infty} = \max_i \sum_j \vert A_{ij} \vert$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核范数&lt;/strong&gt;: $\Vert A \Vert_\ast = \sum_i \sigma_i$, 其中 $\sigma_i$ 为 $A$ 的奇异值.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Cauchy 不等式&lt;/span&gt;&lt;/p&gt;
$$\vert \left&lt; X, Y \right&gt; \vert \leq \Vert X \Vert \Vert Y \Vert$$&lt;p&gt;等号成立当且仅当 $X$ 与 $Y$ 线性相关.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;凸集&#34;&gt;凸集
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果对于任意 $x, y \in C$ 和 $\theta \in \mathbb{R}$, 都有 $\theta x + (1-\theta) y \in C$, 则称 $C$ 为 &lt;strong&gt;仿射集&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;如果对于任意 $x, y \in C$ 和 $\theta \in [0, 1]$, 都有 $\theta x + (1-\theta) y \in C$, 则称 $C$ 为 &lt;strong&gt;凸集&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;换言之, 仿射集要求过任意两点的直线都在集合内, 而凸集要求过任意两点的线段都在集合内. 显然, 仿射集都是凸集. 线性方程组的解集是一个仿射集, 而线性规划问题的可行域是一个凸集. 可以证明, 仿射集均可表示为某个线性方程组的解集.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 $S$ 是凸集, 则 $kS = \left\{ ks \mid k \in \mathbb{R}, s \in S \right\}$ 也是凸集;&lt;/li&gt;
&lt;li&gt;若 $S, T$ 是凸集, 则 $S + T = \left\{ s + t \mid s \in S, t \in T \right\}$ 也是凸集;&lt;/li&gt;
&lt;li&gt;若 $S, T$ 是凸集, 则 $S \cap T$ 也是凸集.&lt;/li&gt;
&lt;li&gt;凸集的内部和闭包均为凸集.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;可以证明, 任意多个凸集的交集仍为凸集.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;形如 $x=\theta_1x_1+\theta_2x_2+\cdots+\theta_kx_k$, 其中 $\theta_i \geq 0$ 且 $\sum_i \theta_i = 1$, 的表达式称为 $x$ 的 &lt;strong&gt;凸组合&lt;/strong&gt;. 集合 $S$ 的所有点的凸组合构成的集合称为 $S$ 的 &lt;strong&gt;凸包&lt;/strong&gt;, 记为 $\text{conv}(S)$.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;若 $\text{conv} S \subseteq S$, 则 $S$ 为凸集, 反之亦然.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;先证明正方向. 对任意 $x,y \in S, \theta \in [0,1]$, 有 $\theta x + (1-\theta) y \in \text{conv} S \subseteq S$, 故 $S$ 为凸集.&lt;/p&gt;
&lt;p&gt;再证明反方向, 对凸组合的维数 $k$ 采用数学归纳法证明之.&lt;/p&gt;
&lt;p&gt;若 $k=1$, 显然成立. 假设对于 $k-1$ 成立, 则对于 $k$, 考虑&lt;/p&gt;
$$
\begin{aligned}
x &amp;= \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_k x_k \\
  &amp;= (1-\theta_k)\left(\frac{\theta_1}{1-\theta_k} x_1 + \frac{\theta_2}{1-\theta_k} x_2 + \cdots + \frac{\theta_{k-1}}{1-\theta_k} x_{k-1}\right) + \theta_k x_k
\end{aligned}
$$&lt;p&gt;前面大括号内的表达式为 $k-1$ 个凸组合, 故在 $S$ 中. 于是 $x$ 又成为两个点的凸组合, 由于 $S$ 为凸集, 故 $x \in S$. 则 $\text{conv} S \subseteq S$.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{conv}S$ 是包含 $S$ 的最小凸集;&lt;/li&gt;
&lt;li&gt;$\text{conv}S$ 是所有包含 $S$ 的凸集的交集.&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;显然第一个是第二个的推论, 只证明第二个.&lt;/p&gt;
&lt;p&gt;已知凸集的交是凸集, 从而所有包含 $S$ 的凸集的交集 $X$ 是凸集. 且 $\text{conv} S$ 是包含 $S$ 的凸集, 则 $X \subseteq \text{conv} S$.&lt;/p&gt;
&lt;p&gt;另一方面, $S \subseteq X$, 则 $\text{conv} S \subseteq \text{conv}X$, 而 $X$ 是凸集, 则 $\text{conv}X = X$, 即 $\text{conv}S \subseteq X$. 综上, $\text{conv}S = X$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;仿照凸组合和凸包, 也可以定义仿射组合和仿射包 $\text{affine} S$, 不再赘述.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;形如 $x=\theta_1x_1+\theta_2x_2+\cdots+\theta_kx_k$, 其中 $\theta_i \geq 0$ 的表达式称为 $x$ 的 &lt;strong&gt;锥组合&lt;/strong&gt;. 若集合 $S$ 中任意点的锥组合都在 $S$ 中, 则称 $S$ 为凸锥.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;常见凸集&#34;&gt;常见凸集
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;任取非零向量 $a\in \mathbb{R}^n$, 形如&lt;/p&gt;
$$ \left\{ x \mid a^Tx =b \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;超平面&lt;/strong&gt;, 形如&lt;/p&gt;
$$ \left\{ x \mid a^Tx \le b \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;半空间&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;满足线性等式和不等式组的点的集合称为 &lt;strong&gt;多面体&lt;/strong&gt;, 即&lt;/p&gt;
$$ \left\{x \mid Ax \le b, Cx = d\right\} $$&lt;p&gt;其中 $A \in \mathbb{R}^{m \times n}, C \in \mathbb{R}^{p \times n}$.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对中心 $x_c$ 和半径 $r$, 形如&lt;/p&gt;
$$ B(x_c, r) = \left\{ x \mid \Vert x - x_c \Vert \le r \right\} = \left\{ x_c + ru \mid \Vert u \Vert \le 1 \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;球&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对中心 $x_c$ 和对称正定矩阵 $P$, 非奇异矩阵 $A$, 形如&lt;/p&gt;
$$ \left\{ x \mid (x-x_c)^TP(x-x_c) \le 1 \right\} = \left\{ x_c + Au \mid \Vert u \Vert \le 1 \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;椭球&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;形如&lt;/p&gt;
$$ \left\{(x,t) \mid \Vert x \Vert \le t \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;(范数)锥&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;保凸运算&#34;&gt;保凸运算
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;仿射运算保凸, 即对 $f(x)=Ax+b$, 则凸集在 $f$ 下的像是凸集, 凸集在 $f$ 下的原像是凸集.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;考虑双曲锥&lt;/p&gt;
$$
\left\{ x \mid x^TPx \le \left( c^Tx \right)^2, c^Tx \ge 0, P \in \mathbb{S}_+^n \right\}
$$&lt;p&gt;$\mathbb{S}_+^n$ 表示半正定矩阵. 双曲锥可以表示为二阶锥&lt;/p&gt;
$$
\left\{ x \mid \Vert Ax \Vert_2 \le c^Tx, c^Tx \ge 0, A^TA = P \right\}
$$&lt;p&gt;这个可以由二次范数锥得到.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;透视变换 $P: \mathbb{R}^{n+1} \mapsto \mathbb{R}^n$:&lt;/p&gt;
$$
  P(x,t) = \frac{x}{t}, \quad \text{dom} P = \left\{ (x,t) \mid t &gt; 0 \right\}
  $$&lt;p&gt;保凸.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分式线性变换 $f: \mathbb{R}^n \mapsto \mathbb{R}^m$:&lt;/p&gt;
$$
  f(x) = \frac{Ax+b}{c^Tx+d}, \quad \text{dom} f = \left\{ x \mid c^Tx+d &gt; 0 \right\}
  $$&lt;p&gt;保凸.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;广义不等式和对偶锥&#34;&gt;广义不等式和对偶锥
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;我们称一个凸锥 $K \subseteq \mathbb{R}^n$ 为 &lt;strong&gt;适当锥&lt;/strong&gt;, 当其还满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$K$ 是闭集;&lt;/li&gt;
&lt;li&gt;$K$ 是实心的, 即 $\text{int} K \neq \emptyset$;&lt;/li&gt;
&lt;li&gt;$K$ 是尖的, 即内部不包含直线: 若 $x \in \text{int} K, -x \in \text{int} K$. 则一定有 $x = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非负卦限 $K=\mathbb{R}_+^n=\left\{ x \in \mathbb{R}^n \mid x_i \ge 0 \right\}$ 是适当锥.&lt;/li&gt;
&lt;li&gt;半正定锥 $K=\mathbb{S}_+^n$ 是适当锥.&lt;/li&gt;
&lt;li&gt;$[0,1]$ 上的有限非负多项式 $K=\left\{ x \in \mathbb{R}^n \mid x_1 + x_2t + \cdots + x_nt^{n-1} \ge 0, t \in [0,1] \right\}$ 是适当锥.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以在 &lt;strong&gt;适当锥&lt;/strong&gt; 上定义广义不等式.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于适当锥 $K$ , 定义偏序 &lt;strong&gt;广义不等式&lt;/strong&gt; 为&lt;/p&gt;
$$x \preceq_K y \Leftrightarrow y - x \in K$$&lt;p&gt;严格版本:&lt;/p&gt;
$$x \prec_K y \Leftrightarrow y - x \in \text{int} K$$&lt;/div&gt;
&lt;p&gt;广义不等式是一个偏序关系, 具有自反性, 反对称性, 传递性, 可加性, 非负缩放性, 不再赘述.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;令锥 $K$ 为全空间 $\Omega$ 的子集, 则 $K$ 的 &lt;strong&gt;对偶锥&lt;/strong&gt; 为&lt;/p&gt;
$$
K^\ast = \left\{ y \mid \left&lt; x, y \right&gt; \ge 0, \forall x \in K \right\}
$$&lt;/div&gt;
&lt;p&gt;例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非负卦限是自对偶锥.&lt;/li&gt;
&lt;li&gt;半正定锥是自对偶锥.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $K$ 是一锥, $K^\ast$ 是其对偶锥, 则满足:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$K^\ast$ 是锥 (即使 $K$ 不是锥);&lt;/li&gt;
&lt;li&gt;$K^\ast$ 是凸且闭的;&lt;/li&gt;
&lt;li&gt;若 $\text{int} \neq \emptyset$, 则 $K^\ast$ 是尖的.&lt;/li&gt;
&lt;li&gt;若 $K$ 是尖的, 则 $\text{int} K^\ast \neq \emptyset$.&lt;/li&gt;
&lt;li&gt;若 $K$ 是适当锥, 则 $K^\ast$ 是适当锥.&lt;/li&gt;
&lt;li&gt;$K^{\ast\ast}$ 是 $K$ 的凸包. 特别地, 若 $K$ 是凸且闭的, 则 $K^\ast=K$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;适当锥的对偶锥仍是适当锥, 则适当锥 $K$ 的对偶锥 $K^\ast$ 也可以诱导广义不等式.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于适当锥 $K$, 定义其对偶锥 $K^\ast$ 上的 &lt;strong&gt;对偶广义不等式&lt;/strong&gt; 为:&lt;/p&gt;
$$x \preceq_{K^\ast} y \Leftrightarrow y - x \in K^\ast$$&lt;p&gt;其满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x \preceq_{K} y \Leftrightarrow \lambda^Tx \le \lambda^Ty, \forall \lambda \succeq_{K^\ast} K^\ast$.&lt;/li&gt;
&lt;li&gt;$y \succeq_{K^\ast} 0 \Leftrightarrow y^Tx \ge 0, \forall x \succeq_K 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id=&#34;分离超平面定理&#34;&gt;分离超平面定理
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;分离超平面定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 $C$ 和 $D$ 是不相交的凸集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 分开, 即存在 $a \neq 0, b$ 使得&lt;/p&gt;
$$
\begin{aligned}
a^Tx &amp;\le b, \quad \forall x \in C \\
a^Tx &amp;\ge b, \quad \forall x \in D
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;简要想法是找距离最近的一对点, 以这两点的中点为中心, 以两点的连线为法向量构造超平面.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;严格分离定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 $C$ 和 $D$ 是不相交的凸集, 且 $C$ 是闭集, $D$ 是紧集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 严格分开, 即存在 $a \neq 0, b$ 使得&lt;/p&gt;
$$
\begin{aligned}
a^Tx &amp;\lt b, \quad \forall x \in C \\
a^Tx &amp;\gt b, \quad \forall x \in D
\end{aligned}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定集合 $C$ 和边界点 $x_0$, 如果 $a\ne 0$ 满足 $a^Tx \le a^T x_0, \forall x \in C$, 则称&lt;/p&gt;
$$
\left\{ x \mid a^Tx = a^T x_0 \right\}
$$&lt;p&gt;为 $C$ 的 &lt;strong&gt;支撑超平面&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;由分离超平面的特殊情况 ($D$ 为单点集) 可以得到支撑超平面的存在性.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;支撑超平面定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若 $C$ 是凸集, 则 $C$ 的任意边界点处存在支撑超平面.&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>最优化方法(1) —— 简介</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/intro/</link>
        <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/intro/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/lect/01-opt-pku.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本节课件链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要
&lt;/h2&gt;&lt;p&gt;最优化问题的一般形式:&lt;/p&gt;
$$
\begin{aligned}
\min_{x} \quad &amp; f(x) \\
\text{s.t.} \quad &amp; g_i(x) \leq 0, \quad i = 1, 2, \ldots, m \\
&amp; h_j(x) = 0, \quad j = 1, 2, \ldots, p
\end{aligned}
$$&lt;h2 id=&#34;稀疏优化&#34;&gt;稀疏优化
&lt;/h2&gt;&lt;p&gt;考虑线性方程组 $Ax = b$, 优化函数 $\min_{x \in R^n} {\Vert x \Vert}_0, {\Vert x \Vert}_1, {\Vert x \Vert}_2$, 分别指代 $x$ 的非零元个数, $l_1, l_2$ 范数.
LASSO(least absolute shrinkage and selection
operator) 问题:&lt;/p&gt;
$$
\min_{x \in \mathbb{R}^n} \mu {\Vert x \Vert}_1 + \frac{1}{2} {\Vert Ax - b \Vert}_2^2
$$&lt;h2 id=&#34;低秩矩阵优化&#34;&gt;低秩矩阵优化
&lt;/h2&gt;&lt;p&gt;考虑矩阵 $M$, 希望 $X$ 在描述 $M$ 有效特征元素的同时, 尽可能保证 $X$ 的低秩性质. 低秩矩阵问题:&lt;/p&gt;
$$
\min_{X \in \mathbb{R}^{m \times n}} \text{rank}(X) \quad \text{s.t.} \quad X_{ij} = M_{ij}, \quad (i, j) \in \Omega
$$&lt;p&gt;核范数 ${\Vert X \Vert}_*$ 为所有奇异值的和. 也有二次罚函数的形式:&lt;/p&gt;
$$
\min_{X \in \mathbb{R}^{m \times n}} \mu {\Vert X \Vert}_* + \frac{1}{2} \sum_{(i,j)\in \Omega} (X_{ij} - M_{ij})^2
$$&lt;p&gt;对于低秩情形, $X=LR^T$, 其中 $L \in \mathbb{R}^{m \times r}, R \in \mathbb{R}^{n \times r}$, $r \ll m,n$ 为秩. 优化问题可写为:&lt;/p&gt;
$$
\min_{L,R} \alpha {\Vert L \Vert}^2_F + \beta {\Vert R \Vert}^2_F + \frac{1}{2} \sum_{(i,j)\in \Omega} ([LR^T]_{ij} - M_{ij})^2
$$&lt;p&gt;引入正则化系数 $\alpha, \beta$ 来消除 $L,R$ 在常数缩放下的不确定性.&lt;/p&gt;
&lt;h2 id=&#34;深度学习&#34;&gt;深度学习
&lt;/h2&gt;&lt;p&gt;机器学习的问题通常形如&lt;/p&gt;
$$
\min_{x \in W} \frac{1}{N} \sum_{i=1}^N \ell(f(a_i, x), b_i) + \lambda R(x)
$$&lt;hr&gt;
&lt;h2 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $f: \mathbb{R}^n \mapsto \mathbb{R}$, $x \in \mathbb{R}^n$ 的可行区域为 $S$. 若存在一个邻域 $N(x)$, 使得 $\forall x \in N(x) \cap S$, 有 $f(x^\ast) \leq f(x)$, 则称 $x^\ast$ 为 $f$ 的&lt;strong&gt;局部极小点&lt;/strong&gt;. 若 $\forall x \in S$, 有 $f(x^\ast) \leq f(x)$, 则称 $x^\ast$ 为 $f$ 的&lt;strong&gt;全局极小点&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;大多数的问题是不能显式求解的, 通常要使用迭代算法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;称算法是 &lt;strong&gt;Q-线性收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert}} \le a, \quad a \in (0, 1)
$$&lt;p&gt;称算法是 &lt;strong&gt;Q-超线性收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\lim_{k \to \infty} \frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert}} = 0
$$&lt;p&gt;称算法是 &lt;strong&gt;Q-次线性收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\lim_{k \to \infty} \frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert}} = 1
$$&lt;p&gt;称算法是 &lt;strong&gt;Q-二次收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert^2}} \le a, \quad a &gt; 0
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $x_k$ 是迭代算法产生的序列且收敛到 $x^\ast$, 如果存在 Q-线性收敛于 $0$ 的非负序列 $t_k$, 且&lt;/p&gt;
$$
\Vert x_k - x^\ast \Vert \le t_k
$$&lt;p&gt;则称 $x_k$ 是 &lt;strong&gt;R-线性收敛&lt;/strong&gt; 的.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;一般来说, 收敛准则可以是&lt;/p&gt;
$$
\frac{f(x_k) - f^\ast}{\max\left\{\left|f^\ast \right|, 1\right\}} \le \varepsilon
$$&lt;p&gt;也可以是&lt;/p&gt;
$$
\nabla f(x_k) \le \varepsilon
$$&lt;p&gt;如果有约束要求, 还要同时考虑到约束违反度. 对于实际的计算机算法, 会设计适当的停机准则, 例如&lt;/p&gt;
$$
\frac{{\Vert x_{k+1} - x_k \Vert}}{\max\left\{\Vert x_k \Vert, 1\right\}} \le \varepsilon
$$</description>
        </item>
        
    </channel>
</rss>
