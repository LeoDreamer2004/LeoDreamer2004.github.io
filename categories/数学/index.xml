<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>数学 on LeoDreamer</title>
        <link>https://LeoDreamer2004.github.io/categories/%E6%95%B0%E5%AD%A6/</link>
        <description>Recent content in 数学 on LeoDreamer</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>LeoDreamer</copyright>
        <lastBuildDate>Tue, 13 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://LeoDreamer2004.github.io/categories/%E6%95%B0%E5%AD%A6/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习基础(11) —— 奇异值分解与主成分分析简介</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/pca/</link>
        <pubDate>Tue, 13 May 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/pca/</guid>
        <description>&lt;h2 id=&#34;奇异值分解&#34;&gt;奇异值分解
&lt;/h2&gt;&lt;p&gt;用 $R(A)$ 表达 $\text{Im}(A)$, $N(A)$ 表达 $\text{Ker}(A)$. 则 $\text{dim} R(A) = \text{rank}(A)$, $\text{dim} R(A) + \text{dim} N(A) = n$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;奇异值分解&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对任意矩阵 $A$, 存在正交矩阵 $U$ 和 $V$, 以及对角矩阵 $\Sigma$ 使得:&lt;/p&gt;
$$
A = U \Sigma V^T
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;$A^TA$ 是对称的, $\text{rank}(A^TA) = r$, 则特征值 $\lambda_1 \ge \lambda_2 \ge \cdots \lambda_r &gt; 0 = \lambda_{r+1} = \lambda_{r+2} = \cdots \lambda_n$, 可正交对角化:&lt;/p&gt;
$$A^TA = V \Lambda V^T$$&lt;p&gt;把 $V$ 分成两部分 $V=[V_1, V_2]$, 其中 $V_1 = [v_1, \cdots, v_r]$, $V_2 = [v_{r+1}, \cdots, v_n]$. 显见 $v_{r+1}, \cdots, v_n$ 恰好构成 $N(A^TA)$ 的标准正交基.&lt;/p&gt;
&lt;p&gt;从 $V_1 = [v_1, \cdots, v_r]$ 出发考虑 $U_1 = [u_1, \cdots, u_r]$:&lt;/p&gt;
$$
u_i = \frac{1}{\sqrt{\lambda_i}} A v_i
$$&lt;p&gt;则容易验证 $u_i$ 是 $R(A)$ 的标准正交基. $R(A)$ 的正交补是 $N(A^T)$, 考虑其一组正交基 $U_2 = [u_{r+1}, \cdots, u_n]$, 则 $U = [U_1, U_2]$ 是正交矩阵. 记:&lt;/p&gt;
$$
\Sigma_1 = \text{diag}(\sqrt{\lambda_1}, \cdots, \sqrt{\lambda_r}) \\
\Sigma = \begin{pmatrix}
\Sigma_1 &amp; 0 \\
0 &amp; 0
\end{pmatrix}
$$&lt;p&gt;则可以得出 $U_1\Sigma_1 = AV_1$, 随后命题即可得证.&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(10) —— PAC 和 UC 可学习性</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/pac-uc/</link>
        <pubDate>Tue, 29 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/pac-uc/</guid>
        <description>&lt;h2 id=&#34;概率近似正确-pac&#34;&gt;概率近似正确 (PAC)
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;我们定义 &lt;strong&gt;泛化误差&lt;/strong&gt; 为:&lt;/p&gt;
$$
L_{\mathcal{D},f}(h) = P_{X \sim \mathcal{\mathcal{D}}}(h(X) \neq f(X))
$$&lt;p&gt;&lt;strong&gt;训练误差&lt;/strong&gt; 为:&lt;/p&gt;
$$
L_S(h) = \frac{1}{m} \sum_{i=1}^m \mathbb{I}(h(X_i) \neq f(X_i))
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果存在 $h^*$ 使得对任意 $L_{\mathcal{D},f}(h^*) = 0$, 则称为 $f,D$ 满足 &lt;strong&gt;可实现假设&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;可实现假设意味着对 $1$ 的概率, 满足 $L_S(h^*) = 0$, 且对每个经验风险最小化的假设 $h_S$ 有 $L_S(h_S) =0$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $\mathcal{H}$ 是有限的假设空间, $\delta \in (0,1), \epsilon&gt;0$, 设正整数 $m$ 满足:&lt;/p&gt;
$$
m \ge \frac{\log(|\mathcal{H}|/\delta)}{\epsilon}
$$&lt;p&gt;对任意标签函数 $f$ 和任意分布 $\mathcal{D}$, 如果可实现性假设相对于 $\mathcal{H}, \mathcal{\mathcal{D}}, f$ 成立, 则在大小为 $m$ 的独立同分布样本 $S$ 的选择上有最低 $1-\delta$ 的概率满足: 对每个经验风险最小化的假设 $h_S$ 有:&lt;/p&gt;
$$
L_{\mathcal{D},f}(h_S) \leq \epsilon
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;令 $\mathcal{H}_B$ 表示“坏”假设的集合, 即&lt;/p&gt;
$$
\mathcal{H}_B = \{ h \in \mathcal{H} : L_{(\mathcal{D}, f)}(h) &gt; \epsilon \}
$$&lt;p&gt;令 $S|_x = \{ x_1, \cdots, x_m \}$ 表示训练集的实例, $M = \{S|_x : \exists h \in \mathcal{H}_B, L_S(h) = 0\}$. 注意由可实现假设:&lt;/p&gt;
$$
\{S|_x : L_{(\mathcal{D}, f)}(h_S) &gt; \epsilon\} \subseteq M = \bigcup_{h \in \mathcal{H}_B} \{S|_x : L_S(h) = 0\}.
$$&lt;p&gt;因此:&lt;/p&gt;
$$
\begin{aligned}
D^m(\{S|_x : L_{(\mathcal{D}, f)}(h_S) &gt; \epsilon\}) &amp;\leq \mathcal{\mathcal{D}}^m(M) = \mathcal{\mathcal{D}}^m\left(\bigcup_{h \in \mathcal{H}_B} \{S|_x : L_S(h) = 0\}\right) \\
&amp;\leq \sum_{h \in \mathcal{H}_B} \mathcal{\mathcal{D}}^m(\{S|_x : L_S(h) = 0\}) \\
&amp;= \sum_{h \in \mathcal{H}_B} \prod_{i=1}^m \mathcal{\mathcal{D}}(\{x_i : h(x_i) = f(x_i)\})
\end{aligned}
$$&lt;p&gt;注意到对于每个 $h \in \mathcal{H}_B$,&lt;/p&gt;
$$
D(\{x_i : h(x_i) = f(x_i)\}) = 1 - L_{(\mathcal{D}, f)}(h) \leq 1 - \epsilon
$$&lt;p&gt;代入上式, 再利用 $m$ 的定义可得:&lt;/p&gt;
$$
D^m(\{S|_x : L_{(\mathcal{D}, f)}(h_S) &gt; \epsilon\}) \leq |\mathcal{H}_B| (1 - \epsilon)^m \le |\mathcal{H}| e^{-\epsilon m} \le \delta
$$&lt;p&gt;由此, 即 $1-D^m(\{S|_x : L_{(\mathcal{D}, f)}(h_S) &gt; \epsilon\}) &gt; 1-\delta$, 得证.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;我们现在可以引入 PAC 可学习性的概念.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;称假设空间 $\mathcal{H}$ 是 &lt;strong&gt;PAC 可学习的&lt;/strong&gt;, 如果存在一个函数 $m_{\mathcal{H}} : (0, 1)^2 \to \mathbb{N}$ 和一个学习算法, 满足以下性质: 对于任意 $\delta, \epsilon \in (0, 1)$, 对于任意定义在 $\mathcal{X}$ 上的分布 $\mathcal{\mathcal{D}}$, 以及对于任意标记函数 $f : \mathcal{X} \to \{0, 1\}$, 如果可实现性假设相对于 $\mathcal{H}, \mathcal{\mathcal{D}}, f$ 成立, 那么当使用由 $\mathcal{\mathcal{D}}$ 生成的 $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ 个独立同分布样本, 并用 $f$ 标记这些样本运行该算法时, 算法将返回一个假设 $h$, 使得在样本选择上以至少 $1 - \delta$ 的概率满足&lt;/p&gt;
$$
L_{(\mathcal{\mathcal{D}}, f)}(h) \leq \epsilon
$$&lt;p&gt;这里, $m$ 的大小称为 &lt;strong&gt;样本复杂度&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;由刚才的定理, 显然:&lt;/p&gt;
$$
m_{\mathcal{H}}(\epsilon, \delta) \le \left\lceil\frac{\log(|\mathcal{H}|/\delta)}{\epsilon}\right\rceil
$$&lt;h2 id=&#34;不可知-pac-可学习性&#34;&gt;不可知 PAC 可学习性
&lt;/h2&gt;&lt;p&gt;实际中 PAC 可学习性的假设很强. 我们放宽可实现性假设.&lt;/p&gt;
&lt;p&gt;Bayers 最优预测: 对于任意 $\mathcal{X} \times {0,1}$ 上的分布 $\mathcal{\mathcal{D}}$, 则最优预测是:&lt;/p&gt;
$$
f_{\mathcal{\mathcal{D}}}(x) = \begin{cases}
1 &amp; \text{if } P(y=1|x) \ge \frac{1}{2} \\
0 &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;但由于 $\mathcal{\mathcal{D}}$ 是未知的, 我们不能直接使用 $f_{\mathcal{\mathcal{D}}}$ 进行预测. 我们希望找一个预测函数使得损失不比 $f_{\mathcal{\mathcal{D}}}$ 大很多.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;称假设空间 $\mathcal{H}$ 是 &lt;strong&gt;不可知 PAC 可学习的&lt;/strong&gt;, 如果存在一个函数 $m_{\mathcal{H}} : (0, 1)^2 \to \mathbb{N}$ 和一个学习算法, 满足以下性质: 对于任意 $\delta, \epsilon \in (0, 1)$, 对于任意定义在 $\mathcal{X} \times \mathcal{Y}$ 上的分布 $\mathcal{\mathcal{D}}$, 当使用由 $\mathcal{\mathcal{D}}$ 生成的 $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ 个独立同分布样本训练时, 算法将返回一个假设 $h$, 使得在样本选择上以至少 $1 - \delta$ 的概率满足:&lt;/p&gt;
$$
L_{\mathcal{\mathcal{D}}}(h) \le \min_{h&#39; \in \mathcal{H}} L_{\mathcal{\mathcal{D}}}(h&#39;) + \epsilon
$$&lt;p&gt;特别地, 我们称假设空间 $\mathcal{H}$ 是关于集合 $Z$ 和损失函数 $\ell: \mathcal{H} \times Z \to \mathbb{R}_+$ &lt;strong&gt;不可知 PAC 可学习的&lt;/strong&gt;, 如果在上述定义中 $\mathcal{\mathcal{D}}$ 是 $Z$ 上的分布, 且不等式中 $L_{\mathcal{\mathcal{D}}}(h) = \mathbb{E}_{z \sim \mathcal{\mathcal{D}}}[\ell(h, z)]$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;显然, 如果可实现性假设成立, 则不可知 PAC 可学习性转化为 PAC 可学习性.&lt;/p&gt;
&lt;h2 id=&#34;一致收敛-uc&#34;&gt;一致收敛 (UC)
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;训练集 $S$ 被称为关于域 $Z$, 假设空间 $\mathcal{H}$, 损失函数 $\ell$ 和分布 $\mathcal{\mathcal{D}}$ 是 &lt;strong&gt;$\epsilon$-典型的&lt;/strong&gt;, 如果&lt;/p&gt;
$$
\forall h \in \mathcal{H}, |L_S(h) - L_{\mathcal{\mathcal{D}}}(h)| \leq \epsilon
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;假设训练集 $S$ 是 $\epsilon/2$-典型的, 则对于任意 $ERM_\mathcal{\mathcal{H}}(S)$ 算法的输出, 即任意 $h_S \in \argmin_{h \in \mathcal{H}} L_S(h)$, 有:&lt;/p&gt;
$$
L_{\mathcal{\mathcal{D}}}(h_S) \leq \min{h \in \mathcal{H}} L_D(h) + \epsilon
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;利用定义可知&lt;/p&gt;
$$
L_{\mathcal{\mathcal{D}}}(h_S) \le L_S(h_S) + \epsilon/2 \le L_S(h) + \epsilon/2 \le L_{\mathcal{\mathcal{D}}}(h) + \epsilon
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;称假设空间 $\mathcal{H}$ 关于域 $Z$, 损失函数 $\ell$ 具有 &lt;strong&gt;一致收敛性&lt;/strong&gt;, 如果存在一个函数 $m_{\mathcal{H}}^{UC}: (0, 1)^2 \to \mathbb{N}$, 使得对于任意 $\epsilon, \delta \in (0, 1)$ 和任意 $Z$ 上的分布 $\mathcal{\mathcal{D}}$,  如果 $S$ 是从 $\mathcal{\mathcal{D}}$ 中独立同分布抽取的大小为 $m \geq m_{\mathcal{H}}^{UC}(\epsilon, \delta)$ 的样本, 则以至少 $1 - \delta$ 的概率, $S$ 是 $\epsilon$-典型的.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;如果假设空间 $\mathcal{H}$ 对于 $m_{\mathcal{H}}^{UC}(\epsilon, \delta)$ 具有一致收敛性, 则 $\mathcal{H}$ 是不可知 PAC 可学习的, 且样本复杂度满足:&lt;/p&gt;
$$
m_{\mathcal{H}}(\epsilon, \delta) \leq m_{\mathcal{H}}^{UC}(\epsilon/2, \delta)
$$&lt;p&gt;在这种情况下, $ERM_\mathcal{H}(S)$ 算法是 $\mathcal{H}$ 的不可知 PAC 学习算法.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Hoeffding 不等式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $\theta_1, \cdots, \theta_m$ 是独立同分布随机变量, 且 $\mathbb{E}[\theta_i] = \mu$, $P(\theta_i \in [a, b]) = 1$. 则对于任意 $\epsilon &gt; 0$, 有:&lt;/p&gt;
$$
P\left(\left|\frac{1}{m} \sum_{i=1}^m \theta_i - \mu\right| &gt; \epsilon\right) \leq 2 \exp\left(-\frac{2m\epsilon^2}{(b-a)^2}\right)
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $\mathcal{H}$ 是有限的假设空间, $Z$ 是一个域, $\ell : \mathcal{H} \times Z \to [0, 1]$ 是一个损失函数. 则 $\mathcal{H}$ 具有一致收敛性, 且样本复杂度满足:&lt;/p&gt;
$$
m_{\mathcal{H}}^{UC}(\epsilon, \delta) \leq \left\lceil \frac{\log(2|\mathcal{H}|/\delta)}{2\epsilon^2} \right\rceil
$$&lt;p&gt;且此时 $\mathcal{H}$ 是不可知 PAC 可学习的, 且样本复杂度满足:&lt;/p&gt;
$$
m_{\mathcal{H}}(\epsilon, \delta) \leq m_{\mathcal{H}}^{UC}(\epsilon/2, \delta) \leq \left\lceil \frac{2\log(2|\mathcal{H}|/\delta)}{\epsilon^2} \right\rceil
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;固定 $\epsilon, \delta$, 我们要找 $m$ 使得对任意 $\mathcal{\mathcal{D}}$, 至少 $1 - \delta$ 的概率, $S$ 是 $\epsilon$-典型的. 即:&lt;/p&gt;
$$
\mathcal{\mathcal{D}}^m(\{ S: \forall h \in \mathcal{H}, |L_S(h)-L_{\mathcal{\mathcal{D}}}(h)| \le \epsilon \}) \ge 1 - \delta
$$&lt;p&gt;注意由 Hoeffding 不等式:&lt;/p&gt;
$$
\begin{aligned}
&amp;\mathcal{\mathcal{D}}^m(\{ S: \forall h \in \mathcal{H}, |L_S(h)-L_{\mathcal{\mathcal{D}}}(h)| &gt; \epsilon \}) \\
&amp; \le \sum_{h \in \mathcal{H}} \mathcal{\mathcal{D}}^m(\{ S: |L_S(h)-L_{\mathcal{\mathcal{D}}}(h)| &gt; \epsilon \}) \\
&amp; \le \sum_{h \in \mathcal{H}} 2 e^{-2m\epsilon^2} = 2|\mathcal{H}| e^{-2m\epsilon^2}
\end{aligned}
$$&lt;p&gt;我们只要取:&lt;/p&gt;
$$
m \ge \frac{\log(2|\mathcal{H}|/\delta)}{2\epsilon^2}
$$&lt;p&gt;即得:&lt;/p&gt;
$$
\mathcal{\mathcal{D}}^m(\{ S: \forall h \in \mathcal{H}, |L_S(h)-L_{\mathcal{\mathcal{D}}}(h)| &gt; \epsilon \}) \le \delta
$$&lt;p&gt;从而得证.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;偏差复杂性分解&#34;&gt;偏差复杂性分解
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;无免费午餐&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $A$ 是在域 $X$ 上的 $0-1$ 误差函数二分类学习算法, 训练集大小 $m$ 是小于 $|X|/2$ 的任意数. 则存在一个在 $X \times \{0, 1\}$ 上的分布 $\mathcal{\mathcal{D}}$ 使得:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;存在一个函数 $f : X \to \{0, 1\}$ 使得 $L_{\mathcal{\mathcal{D}}}(f) = 0$.&lt;/li&gt;
&lt;li&gt;选择 $S \sim \mathcal{\mathcal{D}}^m$ 时, 有至少 $1/7$ 的概率满足 $L_{\mathcal{\mathcal{D}}}(A(S)) \geq 1/8$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;以此我们可以得到如下推论:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}$ 是一个无限域, $\mathcal{H}$ 是从 $\mathcal{X}$ 到 $\{0, 1\}$ 的所有函数的集合. 则 $\mathcal{H}$ 不是 PAC 可学习的.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;假设 $\mathcal{H}$ 是 PAC 可学习的, 选 $\epsilon &lt; 1/8, \delta &lt; 1/7$, 则存在一个算法 $A$ 和一个整数 $m=m_{\mathcal{H}}(\epsilon, \delta)$, 对任意 $\mathcal{X} \times \{0, 1\}$ 上的分布 $\mathcal{D}$, 如果对于某个函数 $f: \mathcal{X} \to \{0, 1\}$, $L_{\mathcal{\mathcal{D}}}(f) = 0$, 则当 $A$ 在 $S \sim \mathcal{\mathcal{D}}^m$ 上运行时, 有至少 $1 - \delta$ 的概率满足: $L_{\mathcal{\mathcal{D}}}(A(S)) \leq \epsilon$.&lt;/p&gt;
&lt;p&gt;但根据无免费午餐定理, 由于 $|X|&gt;2m$, 对于算法 $A$, 存在一个分布 $\mathcal{\mathcal{D}}$ 使得有至少 $1/7&gt;\delta$ 的概率满足 $L_{\mathcal{\mathcal{D}}}(A(S)) \geq 1/8&gt;\epsilon$, 矛盾.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;误差分解:&lt;/p&gt;
$$
L_{\mathcal{\mathcal{D}}}(h_S) = \min_{h \in \mathcal{H}} L_{\mathcal{\mathcal{D}}}(h) + (L_{\mathcal{\mathcal{D}}}(h_S) - \min_{h \in \mathcal{H}} L_{\mathcal{\mathcal{D}}}(h))
$$&lt;p&gt;第一项称为 &lt;strong&gt;近似误差&lt;/strong&gt;; 第二项称为 &lt;strong&gt;估计误差&lt;/strong&gt; $\epsilon_{\text{est}}$: 最小化风险和经验风险之间的差距.&lt;/p&gt;
&lt;p&gt;在有限假设情形下, $\epsilon_{\text{est}}$ 通常随 $|H|$ 增加, 随 $m$ 减小. 当 $\mathcal{H}$ 很小时, 估计误差很小, 但近似误差可能很大, 是欠拟合; 当 $\mathcal{H}$ 很大时, 近似误差很小, 但估计误差可能很大, 是过拟合.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(9) —— 隐 Markov 模型</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/markov/</link>
        <pubDate>Tue, 22 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/markov/</guid>
        <description>&lt;h2 id=&#34;markov-链&#34;&gt;Markov 链
&lt;/h2&gt;&lt;p&gt;Markov 链是刻画随机变量序列的概率分布的模型.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\{X_t\mid t=1,2,\cdots\}$ 是随机序列, 若 $X_t$ 都在 $S$ 中取值, 则称 $S$ 是 $\{X_t\}$ 的状态空间, $S$中的元素称为 &lt;strong&gt;状态&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;如果对任何正整数 $t\geq 2$ 和 $S$ 中的状态 $s_i,s_j,s_{i_1},s_{i_2},\cdots,s_{i_{t-1}}$, 随机序列 $\{X_t\}$ 满足&lt;/p&gt;
$$
P(X_{t+1}=s_j\mid X_t=s_i,X_{t-1}=s_{i_{t-1}},\cdots,X_1=s_{i_1}) \\
= P(X_{t+1}=s_j\mid X_t=s_i) = P(X_2=s_j\mid X_1=s_i)
$$&lt;p&gt;则称$\{X_t\}$为时齐的 &lt;strong&gt;Markov 链&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;我们称&lt;/p&gt;
$$a_{ij} = P(X_2 = s_j | X_1 = s_i), s_i, s_j \in S$$&lt;p&gt;为 Markov 链 $\{X_t\}$ 的 &lt;strong&gt;转移概率&lt;/strong&gt;. 称矩阵 $A = [a_{ij}]$ 为 Markov 链 $\{X_t\}$ 的 &lt;strong&gt;一步转移概率矩阵&lt;/strong&gt;, 简称为 &lt;strong&gt;转移矩阵&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Markov 链的初始状态 $X_1$ 的分布称为 &lt;strong&gt;初始分布&lt;/strong&gt;, 记为 $\pi = (\pi_1,\pi_2,\cdots,\pi_N)$, 其中 $\pi_i = P(X_1 = s_i)$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;设 $|S| = N$, 则转移矩阵为 $N \times N$ 矩阵, 且 $\sum_{j=1}^N a_{ij} = 1$.&lt;/p&gt;
&lt;p&gt;Markov 链的性质直观上可以理解为, 在时刻 $t$ 的状态只与时刻 $t-1$ 的状态有关, 与之前的状态无关. 也就是说, Markov 链具有 无记忆性.&lt;/p&gt;
&lt;h2 id=&#34;隐-markov-模型&#34;&gt;隐 Markov 模型
&lt;/h2&gt;&lt;p&gt;实际中, 我们往往无法直接观察到 Markov 链的状态, 而只能观察到与状态相关的观测值.&lt;/p&gt;
&lt;p&gt;隐 Markov 模型 (HMM) 刻画了首先由一个马尔可夫链随机生成不可观测的状态随机序列 $\{X_t\}$, 再由每个状态 $X_t$ 生成一个观测 $O_t$ 而生成观测随机序列 $\{O_t\}$ 的过程.&lt;/p&gt;
&lt;p&gt;设 &lt;strong&gt;观测概率&lt;/strong&gt; 矩阵 $B = [b_{ij}]$, 其中 $b_{ij} = P(O_t = o_j | X_t = s_i)$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;HMM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 隐 Markov 模型 $M = (A, B, \pi)$, 其中 $A$ 是转移概率矩阵, $B$ 是观测概率矩阵, $\pi$ 是初始分布.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 长度为 $T$ 的观测序列.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;令 $t=1$, 随机选择初始状态 $X_1$ 使得 $P(X_1 = s_i) = \pi_i$.&lt;/li&gt;
&lt;li&gt;根据状态 $X_t$ 和观测概率矩阵 $B$, 随机生成观测 $O_t$ 使得 $P(O_t = o_j | X_t = s_i) = b_{ij}$.&lt;/li&gt;
&lt;li&gt;根据状态 $X_t$ 和转移概率矩阵 $A$, 随机选择下一个状态 $X_{t+1}$ 使得 $P(X_{t+1} = s_j | X_t = s_i) = a_{ij}$.&lt;/li&gt;
&lt;li&gt;令 $t = t + 1$, 如果 $t \leq T$, 则返回第 2 步, 否则停止.&lt;/li&gt;
&lt;li&gt;返回观测序列 $\mathbf{O} = (O_1, O_2, \cdots, O_T)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h2 id=&#34;概率计算方法&#34;&gt;概率计算方法
&lt;/h2&gt;&lt;p&gt;Markov 的第一个核心问题是概率计算问题: 给定 Markov 模型 $\lambda = (A,B,\pi)$, 计算 $p(\mathbf{O} | \lambda)$, 其中 $O=(O_1,O_2, \cdots, O_T)$, 即计算给定模型时得到观测序列的概率.&lt;/p&gt;
&lt;h3 id=&#34;前向算法&#34;&gt;前向算法
&lt;/h3&gt;&lt;p&gt;我们定义前向概率:&lt;/p&gt;
$$\alpha_t(i) = p(O_1, O_2, \cdots, O_t, X_t = s_i | \lambda)$$&lt;p&gt;显见 $\alpha_T(i) = p(\mathbf{O}, X_T = s_i | \lambda)$, 因此 $p(\mathbf{O} | \lambda) = \sum_{i=1}^N \alpha_T(i)$. 对于首项:&lt;/p&gt;
$$
\begin{aligned}
\alpha_1(i) &amp;= p(O_1, X_1 = s_i | \lambda) \\
&amp;= p(X_1 = s_i | \lambda) p(O_1 | X_1 = s_i, \lambda) \\
&amp;= \pi_i b_i(O_1)
\end{aligned}
$$&lt;p&gt;推导递推式:&lt;/p&gt;
$$
\begin{aligned}
\alpha_{t+1}(i) &amp;= p(O_1, O_2, \cdots, O_t, O_{t+1}, X_{t+1} = s_i | \lambda) \\
&amp;= \sum_{j=1}^N p(O_1, O_2, \cdots, O_t, X_t = s_j, X_{t+1} = s_i | \lambda) \\
&amp;= \sum_{j=1}^N \alpha_t(j) p(O_{t+1}|X_{t+1}=s_i, \lambda) p(X_{t+1}=s_i|X_t=s_j, \lambda) \\
&amp;= \sum_{j=1}^N \alpha_t(j)b_i(O_{t+1}) a_{ji} = \left(\sum_{j=1}^N a_{ji}\alpha_t(j)\right)b_i(O_{t+1}) \\
\end{aligned}
$$&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;前向算法&lt;/p&gt;
$$\alpha_1(i) = \pi_i b_i(O_1)$$$$\alpha_{t+1}(i) = \left(\sum_{j=1}^N a_{ji}\alpha_t(j)\right)b_i(O_{t+1})$$$$p(\mathbf{O} | \lambda) = \sum_{i=1}^N \alpha_T(i)$$&lt;/div&gt;
&lt;h3 id=&#34;后向算法&#34;&gt;后向算法
&lt;/h3&gt;&lt;p&gt;我们定义后向概率:&lt;/p&gt;
$$\beta_t(i) = p(O_{t+1}, O_{t+2}, \cdots, O_T | X_t = s_i, \lambda)$$&lt;p&gt;约定 $\beta_T(i) = 1$. 仿照前向算法的思路推导即可.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;后向算法&lt;/p&gt;
$$\beta_T(i) = 1$$$$\beta_t(i) = \sum_{j=1}^N a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)$$$$p(\mathbf{O} | \lambda) = \sum_{i=1}^N \pi_i b_i(O_1) \beta_1(i)$$&lt;/div&gt;
&lt;h2 id=&#34;viterbi-算法&#34;&gt;Viterbi 算法
&lt;/h2&gt;&lt;p&gt;Markov 的第二个核心问题是解码问题: 给定 Markov 模型 $\lambda = (A,B,\pi)$ 和观测序列 $O$, 计算最可能的状态序列 $X = \{X_1, X_2, \cdots, X_T\}$. 即找:&lt;/p&gt;
$$X^* = \argmax_X p(X | \mathbf{O}, \lambda)$$&lt;p&gt;也可以定义为:&lt;/p&gt;
$$X^* = \argmax_X p(X, O| \lambda)$$&lt;p&gt;Viterbi 算法是求解该问题的动态规划算法. 考虑时刻 $T$ 状态为 $s_i$ 的所有单个路径 $(X_1,X_2,\cdots X_{T-1},X_T = s_i)$ 的概率最大值为&lt;/p&gt;
$$
\delta_{T}(i) = \max_{X_{1},X_{2},\cdots,X_{T-1}} P(X_{1},X_{2},\cdots,X_{T-1},O_{1},O_{2},\cdots,O_{T},X_{T}=s_{i}|\lambda)
$$&lt;p&gt;对于最优路径 $X^*$, 即有:&lt;/p&gt;
$$
P(X^*|\mathbf{O},\lambda) = \max_{1 \le i \le N} \delta_{T}(i), X_T^* = \argmax_{1 \le i \le N} \delta_{T}(i)
$$&lt;p&gt;特别地, $\delta_1(i)=\pi_i b_i(O_1)$. 既然要动态规划, 递推公式如下:&lt;/p&gt;
$$
\delta_t(i) = \max_{1 \le j \le N} \left( \delta_{t-1}(j) a_{ji} \right) b_i(O_t)
$$&lt;p&gt;动态规划还要记住路径, 用 $\Psi_t(s_i)$ 记录时刻 $t$ 状态为 $s_i$ 的概率最大的路径的前一个状态, 即:&lt;/p&gt;
$$
\Psi_t(s_i) = \argmax_{1 \le j \le N} \left( \delta_{t-1}(j) a_{ji} \right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Viterbi&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: $\lambda = (A,B,\pi)$, 观测序列 $\mathbf{O} = (O_1,O_2,\cdots,O_T)$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 最优状态序列 $X^* = (X_1^*, X_2^*, \cdots, X_T^*)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化 $\delta_1(i) = \pi_i b_i(O_1)$, $\Psi_1(s_i) = 0$.&lt;/li&gt;
&lt;li&gt;对于 $t=2,3,\cdots,T$:
$$
    \begin{aligned}
    \delta_t(i) &amp;= \max_{1 \le j \le N} \left( \delta_{t-1}(j) a_{ji} \right) b_i(O_t) \\
    \Psi_t(s_i) &amp;= \argmax_{1 \le j \le N} \left( \delta_{t-1}(j) a_{ji} \right)
    \end{aligned}
    $$&lt;/li&gt;
&lt;li&gt;选择最优路径:
$$
    \begin{aligned}
    P^* &amp;= \max_{1 \le i \le N} \delta_T(i) \\
    X_T^* &amp;= \argmax_{1 \le i \le N} \delta_T(i)
    \end{aligned}
    $$&lt;/li&gt;
&lt;li&gt;从时间 $T$ 追溯历史:
$$X_{t-1}^* = \Psi_t(X_t^*)$$&lt;/li&gt;
&lt;li&gt;返回最优路径 $X^* = (X_1^*, X_2^*, \cdots, X_T^*)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h2 id=&#34;baum-welch-算法&#34;&gt;Baum-Welch 算法
&lt;/h2&gt;&lt;p&gt;Markov 的第三个核心问题是学习问题: 给定观测序列 $O$ 和隐 Markov 模型 $\lambda = (A,B,\pi)$, 计算最优的模型参数使得似然 $p(O|\lambda)$ 最大.&lt;/p&gt;
&lt;p&gt;如果 Markov 链是可观测的, 则可以直接用极大似然估计来估计参数. 如果隐藏, 可以用 EM 算法来估计参数. 我们依然沿用 EM 算法的思路:&lt;/p&gt;
$$
Q(\theta|\theta^{(t)}) = \sum_{Z} LL(\theta|D,Z) p(Z|D,\theta^{(t)})
$$&lt;p&gt;用 $\bar{\lambda}$ 表示当前的参数, 则在 M 步中的 $Q$ 函数为:&lt;/p&gt;
$$
\begin{aligned}
Q(\lambda|\bar{\lambda}) &amp;= \sum_{X} p(X|\mathbf{O},\bar{\lambda}) \log p(\mathbf{O},X|\lambda) \\
&amp;= \frac{1}{p(O|\bar{\lambda})} \sum_{X} p(\mathbf{O},X|\bar{\lambda})\log p(\mathbf{O},X|\lambda)
\end{aligned}
$$&lt;p&gt;忽略前面的常数项, Baum-Welch 直接定义 $Q$ 函数为:&lt;/p&gt;
$$
Q(\lambda|\bar{\lambda}) = \sum_{X} p(X|\mathbf{O},\bar{\lambda})\log p(\mathbf{O},X|\lambda)
$$&lt;p&gt;如果我们记相应的隐状态序列为 $X = (X_1=s_{i_1}, X_2=s_{i_2}, \cdots, X_T=s_{i_T})$, 则有:&lt;/p&gt;
$$
P(\mathbf{O},X|\lambda) = \pi_{i_1} b_{i_1}(O_1) \prod_{t=1}^{T-1} a_{i_t i_{t+1}} b_{i_{t+1}}(O_{t+1})
$$&lt;p&gt;代入有&lt;/p&gt;
$$
\begin{aligned}
Q(\lambda,\bar{\lambda}) &amp; =\sum_{X}p(\mathbf{O},X|\bar{\lambda})\log\left[\pi_{i_{1}}b_{i_{1}}(O_{1})\prod_{t=1}^{T-1}a_{i_{t}i_{t+1}}b_{i_{t+1}}(O_{t+1})\right] \\
&amp;=\sum_{X}p(\mathbf{O},X|\bar{\lambda})\log\pi_{i_{1}}+\sum_{X}p(\mathbf{O},X|\bar{\lambda})\left[\sum_{t=1}^{T-1}\log a_{i_{t}i_{t+1}}\right] +\sum_X p(\mathbf{O},X|\bar{\lambda})\left[\sum_{t=1}^T\log b_{i_t}(O_t)\right].
\end{aligned}
$$&lt;p&gt;三个部分分别设为 $Q_1, Q_2, Q_3$.&lt;/p&gt;
&lt;h3 id=&#34;计算-q1&#34;&gt;计算 Q1
&lt;/h3&gt;$$
\begin{aligned}
Q_1 &amp;= \sum_{i=1}^N \sum_{X_1=s_i, X_2, \cdots, X_T} p(\mathbf{O},X|\bar{\lambda}) \log \pi_i \\
&amp;= \sum_{i=1}^N p(\mathbf{O},X_1=s_i|\bar{\lambda}) \log \pi_i \\
\end{aligned}
$$&lt;p&gt;$\pi_i$ 要满足 $\sum_{i=1}^N \pi_i = 1$, 因此可以用 Lagrange 乘子法来求解. 得到:&lt;/p&gt;
$$
\pi_i = \frac{p(\mathbf{O},X_1=s_i|\bar{\lambda})}{p(O|\bar{\lambda})} = p(X_1=s_i|\mathbf{O},\bar{\lambda})
$$&lt;h3 id=&#34;计算-q2&#34;&gt;计算 Q2
&lt;/h3&gt;&lt;p&gt;类似 $Q_1$ 的处理手法:&lt;/p&gt;
$$
Q_2 = \sum_{i,j=1}^{N} \sum_{t=1}^{T-1} p(\mathbf{O}, X_t=s_i, X_{t+1}=s_j|\bar{\lambda}) \log a_{ij}
$$&lt;p&gt;附加条件 $\sum_{j=1}^N a_{ij} = 1$, Lagrange 乘子法求解, 得到:&lt;/p&gt;
$$
a_{ij} = \frac{\sum_{t=1}^{T-1}P(X_t=s_i,X_{t+1}=s_j|\mathbf{O},\bar{\lambda})}{\sum_{t=1}^{T-1}P(X_t=s_i|\mathbf{O},\bar{\lambda})}
$$&lt;p&gt;这里分子分母也同时除了 $p(O|\bar{\lambda})$.&lt;/p&gt;
&lt;h3 id=&#34;计算-q3&#34;&gt;计算 Q3
&lt;/h3&gt;&lt;p&gt;仍然类似处理:&lt;/p&gt;
$$
Q_{3}=\sum_{j=1}^{N}\sum_{t=1}^{T}P(\mathbf{O},X_{t}=s_{j}|\bar{\lambda})\log b_{j}(O_{t})
$$&lt;p&gt;附加条件 $\sum_{k=1}^M b_j(k) = 1$. 注意 $b_{j}(O_{t})$ 和  $b_{j}(t)$ 并不见得相同, 我们需要简单改写一下:&lt;/p&gt;
$$
\log b_j(O_t) = \sum_{k=1}^M I(O_t=\nu_k) \log b_j(k)
$$&lt;p&gt;此时再用 Lagrange 乘子法求解, 得到:&lt;/p&gt;
$$
b_{j}(k) = \frac{\sum_{t=1}^{T}P(X_{t}=s_{j}|\mathbf{O},\bar{\lambda})I(O_{t}=\nu_{k})}{\sum_{t=1}^{T}P(X_{t}=s_{j}|\mathbf{O},\bar{\lambda})}
$$&lt;p&gt;这里分子分母也同时除了 $p(O|\bar{\lambda})$.&lt;/p&gt;
&lt;p&gt;于是核心转化为了计算:&lt;/p&gt;
$$
\begin{aligned}
\gamma_t(i|\lambda) &amp;= p(X_t=s_i|\mathbf{O},\bar{\lambda})\\
\xi(i,j|\lambda)&amp;=p(X_t=s_i,X_{t+1}=s_j|\mathbf{O},\bar{\lambda})
\end{aligned}
$$&lt;p&gt;利用 Bayes 公式, 结合前向后向算法可得结果.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Baum-Welch&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 观测序列 $\mathbf{O} = (O_1,O_2,\cdots,O_T)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 隐 Markov 模型 $\lambda = (A,B,\pi)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化 $\lambda = (A,B,\pi)$.&lt;/li&gt;
&lt;li&gt;按概率计算方法计算前向概率 $\alpha_t(i)$ 和后向概率 $\beta_t(i)$.&lt;/li&gt;
&lt;li&gt;计算以下参数:
$$
    \begin{aligned}
    \gamma_t(i|\lambda) &amp;= \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)} \\
    \xi(i,j|\lambda) &amp;= \frac{\alpha_t(i)a_{ij}b_j(O_{t+1})\beta_{t+1}(j)}{\sum_{j=1}^N \sum_{k=1}^N \alpha_t(k)a_{kj}b_j(O_{t+1})\beta_{t+1}(k)} \\
    \pi_i &amp;= \gamma_1(i|\lambda) \\
    a_{ij} &amp;= \frac{\sum_{t=1}^{T-1} \xi(i,j|\lambda)}{\sum_{t=1}^{T-1} \gamma_t(i|\lambda)} \\
    b_{j}(k) &amp;= \frac{\sum_{t=1}^{T} \gamma_t(j|\lambda)I(O_t=\nu_k)}{\sum_{t=1}^{T} \gamma_t(j|\lambda)}
    \end{aligned}
    $$&lt;/li&gt;
&lt;li&gt;得到新的参数 $\lambda = (A,B,\pi)$.&lt;/li&gt;
&lt;li&gt;重复步骤 2-4 直到收敛.&lt;/li&gt;
&lt;li&gt;返回隐 Markov 模型 $\lambda = (A,B,\pi)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(8) —— 聚类简介</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/clustering-intro/</link>
        <pubDate>Tue, 08 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/clustering-intro/</guid>
        <description>&lt;h2 id=&#34;基于原型的聚类方法&#34;&gt;基于原型的聚类方法
&lt;/h2&gt;&lt;p&gt;与监督学习不同, 无监督学习基于数据集 $D=\{x_i\}_{i=1}^N$, 没有标签 $y_i$. 基于原型的方法通常假设数据内在的分布结构可以通过一组原型刻画, 先对原型初始化, 然后按照相应策略和准则进行迭代更新.&lt;/p&gt;
&lt;h3 id=&#34;k-means-聚类&#34;&gt;K-means 聚类
&lt;/h3&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;K-means 聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 数据集 $D=\{x_i\}_{i=1}^N$, 聚类簇个数 $K$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 簇划分 $\mathcal{C}=\{C_l\}_{l=1}^K$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择 $K$ 个样本点作为初始簇心 $\mu_l$. 初始化 $C_l = \emptyset$.&lt;/li&gt;
&lt;li&gt;对每个 $x_i$, 求 $x_i$ 的簇标记 $\lambda_i = \argmin_j \|x_i - \mu_j\|^2$, 即找到距离最近的簇心, 并将 $x_i$ 加入到 $C_{\lambda_i}$.&lt;/li&gt;
&lt;li&gt;对每个簇 $C_l$, 更新簇心 $\mu_l = \frac{1}{|C_l|} \sum_{x_i \in C_l} x_i$.&lt;/li&gt;
&lt;li&gt;如果簇心不再变化, 则停止迭代, 否则返回第 2 步.&lt;/li&gt;
&lt;li&gt;返回 $\mathcal{C} = \{C_l\}_{l=1}^K$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;一般会基于不同的核心多次运行 K-means. 均值运算对于噪声和离群点非常敏感.&lt;/p&gt;
&lt;p&gt;还有一些变体, K-中心点方法通过挑选簇内相对处于最中心位置的一个实际样本点而非样本均值向量来作为簇心.&lt;/p&gt;
&lt;p&gt;用 $O_l$ 表示簇 $C_l$ 的簇心样本点, 用 $\text{dist}(x_i, O_l)$ 表示样本点 $x_i$ 和 $O_l$ 的相异程度度量, 则 K-中心点方法相当于通过最小化绝对误差&lt;/p&gt;
$$E = \sum_{l=1}^{K} \sum_{x \in C_l} \text{dist}(x, O_l)$$&lt;p&gt;围绕中心点的划分算法 (PAM) 是一种典型的 K-中心点方法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;PAM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 数据集 $D=\{x_i\}_{i=1}^N$, 聚类簇个数 $K$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 簇划分 $\mathcal{C}=\{C_l\}_{l=1}^K$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先对每个簇的中心点进行随机初始化，并将非中心点的样本划分到簇心与其最相似的簇中，形成样本集的初始划分.&lt;/li&gt;
&lt;li&gt;然后采用贪心策略，迭代更新划分，直到没有变化为止.&lt;/li&gt;
&lt;li&gt;对当前的一个中心点 $o_l$, 随机选择一个非中心点样本 $x_i$, 评估以 $x_i$ 替代 $o_l$ 作为簇心能否得到更好的划分.&lt;/li&gt;
&lt;li&gt;如果这种替代能得到更好的划分，则以 $x_i$ 作为簇 $C_l$ 的新中心点, 然后对当前的非中心点样本进行重新划分;&lt;/li&gt;
&lt;li&gt;尝试这样所有可能的替换, 直到簇划分不再发生变化为止.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;PAM 算法使用中心点作为簇的原型表示，可以避免均值向量作为原型时易受离群点影响的问题.&lt;/p&gt;
&lt;h3 id=&#34;gauss-混合模型&#34;&gt;Gauss 混合模型
&lt;/h3&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gauss 混合模型&lt;/strong&gt; 是指具有如下概率分布密度函数的模型:&lt;/p&gt;
$$p(x|\theta) = \sum_{k=1}^K \alpha_i p(x | \mu_i, \Sigma_i)$$&lt;p&gt;其中:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha_i$ 是混合系数, 满足 $\sum_{i=1}^K \alpha_i = 1$;&lt;/li&gt;
&lt;li&gt;$p(x | \mu_i, \Sigma_i)$ 是 Gauss 分布, 其均值为 $\mu_i$, 协方差矩阵为 $\Sigma_i$, 即
$$p(x|\mu_i, \Sigma_i) = \frac{1}{\sqrt{(2\pi)^n |\Sigma_i|}} \exp\left(-\frac{1}{2}(x - \mu_i)^T \Sigma_i^{-1} (x - \mu_i)\right)$$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;给定样本集 $D=\{x_i\}_{i=1}^N$, 基于 Gauss 混合模型的聚类算法假定样本 $x_j$ 依据 Gauss 混合分布生成, 即先以概率 $\alpha_i$ 选择一个高斯分布 $p(x | \mu_i, \Sigma_i)$, 然后从该高斯分布中生成样本 $x_j$.&lt;/p&gt;
&lt;p&gt;对 $x_j$, 设 $z_j$ 表示生成 $x_j$ 的分模型, 即 $p(z_j = i) = \alpha_i$. 后验概率最大化&lt;/p&gt;
$$
\lambda_j = \argmax_i p(z_j = i | x_j)
$$&lt;p&gt;由 Bayes 公式, 忽略相同的分母, 则&lt;/p&gt;
$$
\begin{aligned}
\lambda_j &amp;= \argmax_i p(x_j | z_j = i) p(z_j = i) \\
&amp;= \argmax_i p(x_j | \mu_i, \Sigma_i) \alpha_i
\end{aligned}
$$&lt;p&gt;考虑对数似然函数&lt;/p&gt;
$$
LL(\theta | D) = \sum_{j=1}^N \log p(x_j | \theta) = \sum_{j=1}^N \log \left( \sum_{i=1}^K \alpha_i p(x_j | \mu_i, \Sigma_i) \right)
$$&lt;p&gt;并不是很好求解. 我们引入隐变量 $z_{ji}$ 表示 $x_j$ 由第 $i$ 个高斯分布生成, 即&lt;/p&gt;
$$
z_{ji} = \begin{cases}
1, &amp; \text{if } z_j = i \\
0, &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;则这样的对数似然函数可以写成&lt;/p&gt;
$$
\begin{aligned}
LL(\theta D|Z)&amp;=\sum_{j=1}^N \sum_{i=1}^K z_{ji} \log \left( \alpha_i p(x_j | \mu_i, \Sigma_i) \right) \\
&amp;=\sum_{i=1}^K \left( \sum_{j=1}^N z_{ji} \right) \log \alpha_i + \sum_{i=1}^K \sum_{j=1}^N z_{ji} \log p(x_j | \mu_i, \Sigma_i)
\end{aligned}
$$&lt;p&gt;常采用 EM 算法迭代求解.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;EM&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;E步&lt;/strong&gt;, 求期望: 基于当前参数 $\theta^{(t)}$, 计算对数似然函数关于 $Z$ 的期望:
$$
    Q \left(\theta | \theta^{(t)}\right) = \mathbb{E}_{Z} \left[ LL(\theta | D, Z) | D, \theta^{(t)} \right] = \sum_Z LL(\theta | D, Z) p(Z | D, \theta^{(t)})
    $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M步&lt;/strong&gt;, 最大化: 通过最大化 $Q\left(\theta | \theta^{(t)}\right)$ 来更新参数 $\theta$:
$$
    \theta^{(t+1)} = \argmax_{\theta} Q\left(\theta | \theta^{(t)}\right)
    $$&lt;/li&gt;
&lt;li&gt;迭代直到收敛.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;用 EM 算法估计参数:&lt;/p&gt;
$$
LL(\theta|D,Z)=\sum_{i=1}^k\left\{\left(\sum_{j=1}^Nz_{ji}\right)\log\alpha_i+\sum_{j=1}^Nz_{ji}\log p(x_j|\mu_i,\sigma_i^2)\right\}
$$&lt;p&gt;令 $n_i=\sum_{j=1}^Nz_{ji}$, 则&lt;/p&gt;
$$
\begin{aligned}
&amp; LL(\theta|D,Z)=\sum_{i=1}^k\left\{n_i\log\alpha_i+\sum_{j=1}^Nz_{ji}\log p(x_j|\mu_i,\sigma_i^2)\right\} \\
&amp; =\sum_{i=1}^{k}\left\{n_{i}\log\alpha_{i}+\sum_{j=1}^{N}z_{ji}\left[\log\left(\frac{1}{\sqrt{2\pi}}\right)-\log\sigma_{i}-\frac{1}{2\sigma_{i}^{2}}(x_{j}-\mu_{i})^{2}\right]\right\}
\end{aligned}
$$&lt;p&gt;我们考虑 $z_{ji}$ 期望:&lt;/p&gt;
$$
\begin{aligned}
\gamma_{ji}^{(t)} &amp;= E_{Z} \left[ z_{ji} | D, \theta^{(t)} \right] = p\left(z_{ji} = 1 | D, \theta^{(t)}\right) \\
&amp;= p\left(z_j = i | D, \theta^{(t)}\right) = \frac{\alpha_i^{(t)} p\left(x_j | \mu_i^{(t)}, {\sigma_i^2}^{(t)}\right)}{\sum_{l=1}^k \alpha_i^{(t)} p\left(x_j | \mu_l^{(t)}, {\sigma_l^2}^{(t)}\right)}
\end{aligned}
$$&lt;p&gt;则对 $E$ 步, 有:&lt;/p&gt;
$$
\begin{aligned}
Q\left(\theta |\theta^{(t)}\right) &amp;= E_Z \left[ LL(\theta | D, Z) | D, \theta^{(t)} \right] \\
&amp;= \sum_{i=1}^k \left\{ \sum_{j=1}^N \gamma_{ji}^{(t)} \log \alpha_i + \sum_{j=1}^N \gamma_{ji}^{(t)} \left[\log\left(\frac{1}{\sqrt{2\pi}}\right)-\log\sigma_{i}-\frac{1}{2\sigma_{i}^{2}}(x_{j}-\mu_{i})^{2}\right]\right\} \\
\end{aligned}
$$&lt;p&gt;既然要极大化 $Q\left(\theta |\theta^{(t)}\right)$, 那么我们可以对 $\mu_i$, $\sigma_i^2$ 分别求偏导数, 令其为 $0$. 分别得到:&lt;/p&gt;
$$
\begin{aligned}
\mu_i^{(t+1)} &amp;= \frac{\sum_{j=1}^N \gamma_{ji}^{(t)} x_j}{\sum_{j=1}^N \gamma_{ji}^{(t)}} \\
{\sigma_i^2}^{(t+1)} &amp;= \frac{\sum_{j=1}^N \gamma_{ji}^{(t)} \left(x_j - \mu_i^{(t+1)}\right)^2}{\sum_{j=1}^N \gamma_{ji}^{(t)}}
\end{aligned}
$$&lt;p&gt;注意 $\alpha_i$ 还有约束 $\sum_{i=1}^k \alpha_i = 1$, 为此用 Lagrange 对偶, 令&lt;/p&gt;
$$
L(\theta, \beta) = Q\left(\theta |\theta^{(t)}\right) + \beta \left(1 - \sum_{i=1}^k \alpha_i\right)
$$&lt;p&gt;对 $\alpha_i$ 求偏导数, 令其为 $0$, 可得:&lt;/p&gt;
$$
n_i^{(t)} = \beta \alpha_i
$$&lt;p&gt;两边求和, 随后可以得出 $\alpha$:&lt;/p&gt;
$$
N = \sum_{i=1}^k n_i^{(t)} = \beta \sum_{i=1}^k \alpha_i = \beta
$$$$
\alpha_i^{(t+1)} = \frac{n_i^{(t)}}{\beta} = \frac{\sum_{j=1}^N \gamma_{ji}^{(t)}}{N}
$$&lt;p&gt;把这些综合起来, 就得到基于 Gauss 混合模型的 EM 算法 (GMM):&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;GMM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 数据集 $D=\{x_i\}_{i=1}^N$, 聚类簇个数 $K$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 簇划分 $\mathcal{C}=\{C_l\}_{l=1}^K$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化参数 $\theta =\{\alpha_i, \mu_i, \Sigma_i\}_{i=1}^K, C_l = \emptyset$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;E步&lt;/strong&gt;: 计算后验概率:
$$
    \gamma_{ji} = p(z_j = i | x_j, \theta) = \frac{\alpha_i p(x_j | \mu_i, \Sigma_i)}{\sum_{l=1}^K \alpha_l p(x_j | \mu_l, \Sigma_l)}
    $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M步&lt;/strong&gt;: 更新参数:
$$
    \begin{aligned}
    \mu_i &amp;= \frac{\sum_{j=1}^N \gamma_{ji} x_j}{\sum_{j=1}^N \gamma_{ji}} \\
    \Sigma_i &amp;= \frac{\sum_{j=1}^N \gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^T}{\sum_{j=1}^N \gamma_{ji}} \\
    \alpha_i &amp;= \frac{\sum_{j=1}^N \gamma_{ji}}{N}
    \end{aligned}
    $$&lt;/li&gt;
&lt;li&gt;重复步骤 2 和 3, 直到收敛.&lt;/li&gt;
&lt;li&gt;对于每个 $x_j$, 求 $x_j$ 的簇标记:
$$
    \lambda_j = \argmax_i \alpha_i p(x_j | \mu_i, \Sigma_i)
    $$
并将 $x_j$ 加入到 $C_{\lambda_j}$.&lt;/li&gt;
&lt;li&gt;返回 $\mathcal{C} = \{C_l\}_{l=1}^K$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h2 id=&#34;层次聚类算法&#34;&gt;层次聚类算法
&lt;/h2&gt;&lt;p&gt;允许在聚类过程中对已有的簇进行合并或分裂, 通过对样本集不同层次的划分形成树状结构.&lt;/p&gt;
&lt;p&gt;AGNES 算法是自底向上的层次聚类算法, 其基本思想是从每个样本点开始, 逐步合并最相近的簇. 关于衡量簇之间的距离, 可以有很多定义, 例如最小距离, 最大距离, 平均距离, 质心距离, 中心距离等. 如果一个聚类算法分别选用最小距离/最大距离/平均距离作为两个簇的距离, 则相应的算法分别被称为单连接算法/全连接算法/均连接算法.&lt;/p&gt;
&lt;p&gt;AGNES 算法采用距离 (相异性) 矩阵来保存当前簇之间的距离:&lt;/p&gt;
$$M(i,j)=d(C_i,C_j),\quad i,j=1,2,\cdots,N$$&lt;p&gt;随着每次距离最近的两个簇的合并, 对距离矩阵也作相应的修正. 不妨设当前距离最近的两个聚类簇为 $C_i^*$ 和 $C_j^*$ 且 $i^*&lt;j^*$, 则&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 $C_j^*$ 并入 $C_{i^*}$, 将合并以后的新簇仍然记作 $C_i^*$,并将所有 $j&gt;j^*$ 簇 $C_j$ 的下标减 $1$, 重新标记为 $C_{j-1}$;&lt;/li&gt;
&lt;li&gt;删除当前距离矩阵$M$的第 $j^*$ 行与第 $j^*$ 列;&lt;/li&gt;
&lt;li&gt;将 $M(i^*,j)$ 和 $M(j,i^*)$ 更新为 $d(C_{i^*},C_j)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;DIANA 算法恰好与 AGNES 相反, 它是自顶向下的层次聚类算法.&lt;/p&gt;
&lt;h2 id=&#34;基于密度的聚类方法&#34;&gt;基于密度的聚类方法
&lt;/h2&gt;&lt;p&gt;将簇看作是数据空间中被稀疏区域分开的稠密区域, 聚类就是发现并不断扩展稠密区域的过程. DBSCAN 算法是典型的基于密度的聚类算法.&lt;/p&gt;
&lt;p&gt;为了刻画稠密区域, DBSCAN 算法引入了密度可达性和密度相连的概念:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于样本点 $x_i \in D$, 在其 $\epsilon$ - 邻域&lt;/p&gt;
$$
N_\epsilon(x_i) = \{x_j \in D | \|x_i - x_j\| \leq \epsilon\}
$$&lt;p&gt;内, 包含至少 $\text{MinPts}$ 个样本点的点称为 &lt;strong&gt;核心点&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;如果 $x_j$ 位于核心点 $x_i$ 的 $\epsilon$ - 邻域内, 则称 $x_j$ 由 $x_i$  &lt;strong&gt;直接密度可达&lt;/strong&gt;, 一般地, 如果存在一个序列 $p_1=x_i, p_2, \cdots, p_k=x_j$, 使得 $p_{l+1}$ 由 $p_l$ 直接密度可达, 则称 $x_j$ 由 $x_i$ &lt;strong&gt;密度可达&lt;/strong&gt;. 如果存在 $p \in D$ 使得 $x_i$ 和 $x_j$ 都由 $p$ 密度可达, 则称 $x_i$ 和 $x_j$ &lt;strong&gt;密度相连&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;此时, 我们定义 &lt;strong&gt;簇&lt;/strong&gt; 是满足如下条件的样本点集合:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $x_i,x_j \in C$, 则 $x_i,x_j$ 是密度相连的;&lt;/li&gt;
&lt;li&gt;对任一 $x_i \in C$, 如果 $x_j$ 由 $x_i$ 密度可达, 则 $x_j \in C$;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;DBSCAN&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 数据集 $D=\{x_i\}_{i=1}^N$, $\epsilon$ - 邻域半径, 最小点数 $\text{MinPts}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 簇划分 $\mathcal{C}=\{C_l\}_{l=1}^K$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化簇划分 $\mathcal{C} = \emptyset$, 并将所有样本点标记为未访问.&lt;/li&gt;
&lt;li&gt;随机选择一个未访问的样本点 $x_i$ 访问: 如果 $x_i$ 是核心点, 则找出由该样本点密度可达的所有样本点, 将它们划分到同一个簇 $C_l$ 中, 否则将 $x_i$ 标记为噪声点.&lt;/li&gt;
&lt;li&gt;重复步骤 2, 直到所有样本点都被访问.&lt;/li&gt;
&lt;li&gt;返回 $\mathcal{C} = \{C_l\}_{l=1}^K$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(7) —— 集成学习</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/integrated-learning/</link>
        <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/integrated-learning/</guid>
        <description>&lt;h2 id=&#34;集成学习概述&#34;&gt;集成学习概述
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;集成学习&lt;/strong&gt; 是指通过将相对比较容易构建但泛化性能一般的多个学习器进行结合, 以获得比单一学习器更好的泛化性能的一种机器学习方法.&lt;/p&gt;
&lt;p&gt;根据个体分类器是否由同一学习算法, 集成学习可以分为 &lt;strong&gt;同质集成&lt;/strong&gt; 和 &lt;strong&gt;异质集成&lt;/strong&gt; 两大类; 根据个体分类器的依赖关系, 可以将学习方法分为 &lt;strong&gt;序列化方法(串行集成)&lt;/strong&gt; 和 &lt;strong&gt;并行化方法(并行集成)&lt;/strong&gt; 两种.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;提升方法&#34;&gt;提升方法
&lt;/h2&gt;&lt;p&gt;PAC 框架下, 概念类强可学习和其弱可学习等价, 但弱可学习实现更容易, 提升方法就是指将弱学习算法提升为强学习算法的方法.&lt;/p&gt;
&lt;h3 id=&#34;adaboost-算法&#34;&gt;AdaBoost 算法
&lt;/h3&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;AdaBoost&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 给定训练数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 其中 $x_i \in \mathcal{X}, y_i \in \mathcal{Y}=\{-1,+1\}, i=1,2,\cdots,N$; 弱学习算法 $\mathcal{L}$ 以及基本分类器个数 $T$;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 最终分类器 $f(x)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;准备一个权重向量 $W_t=(w_{i,t})_{i=1}^N$, 表示第 $t$ 轮训练数据的权重分布, 初始时 $w_{i,1}=\frac{1}{N}$;&lt;/li&gt;
&lt;li&gt;在第 $t$ 轮学习中, 应用算法 $\mathcal{L}$ 基于训练数据集 $D$ 和权重向量 $W_t$ 学得具有最小训练误差的基本分类器 $f_t(x)$, 即
$$f_t = \argmin_{f} \sum_{i=1}^N w_{i,t} \mathbb{I}(f(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算 $f_t(x)$ 的误差率
$$e_t = \sum_{i=1}^N w_{i,t} \mathbb{I}(f_t(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算 $f_t(x)$ 的权值
$$\alpha_t = \frac{1}{2} \ln \frac{1-e_t}{e_t}$$&lt;/li&gt;
&lt;li&gt;按照投票权值更新训练数据集的权重分布
$$w_{i,t+1} = \frac{w_{i,t}}{Z_t} \exp(-\alpha_t y_i f_t(x_i))$$
其中 $Z_t$ 是规范化因子, 使得 $w_{i,t+1}$ 成为一个概率分布.&lt;/li&gt;
&lt;li&gt;经过 $T$ 轮迭代后, 得到最终分类器
$$
    \begin{aligned}
    f(x) &amp;= \text{sign}(G(x)) \\
    G(x) &amp;= \sum_{t=1}^T \alpha_t f_t(x)
    \end{aligned}
    $$
$G(x)$ 的符号决定了 $x$ 的类别, $|G(x)|$ 表示分类的确信度.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;注意:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$e_t$ 越小, $\alpha_t$ 越大, 表示 $f_t(x)$ 的权重越大;&lt;/li&gt;
&lt;li&gt;$\alpha_t$ 不仅仅平衡了 $f_t(x)$ 的权重, 还调节了样本分布的权重:
$$w_{i,t+1}=\begin{cases}
    \frac{w_{i,t}}{Z_t} \exp(\alpha_t), &amp; y_i=f_t(x_i) \\
    \frac{w_{i,t}}{Z_t} \exp(-\alpha_t), &amp; y_i \neq f_t(x_i)
    \end{cases}
    $$
对于那些错误样本, 下次迭代时的权重会增大, 以便让弱分类器更关注这些样本.&lt;/li&gt;
&lt;li&gt;关于为什么要这样赋值 $\alpha_t$, 由表达式可以得到
$$\exp(\alpha_t)e_t = \exp(-\alpha_t)(1-e_t)$$
这表明分配给错误样本的权重之和与正确样本的权重之和相等.&lt;/li&gt;
&lt;li&gt;计算可知
$$Z_t = \sum_{i=1}^N w_{i,t} \exp(-\alpha_t y_i f_t(x_i)) = 2 \sqrt{e_t(1-e_t)}$$&lt;/li&gt;
&lt;li&gt;权值调整累计过程
$$
    \begin{aligned}
    w_{i,t+1}&amp;=w_{i,t} \frac{\exp(-\alpha_t y_i f_t(x_i))}{Z_t} \\
    &amp;=w_{i,1} \frac{\exp \left(-y_i \sum_{s=1}^t \alpha_s f_s(x_i)\right)}{\prod_{s=1}^t Z_s} \\
    \end{aligned}
    $$
取 $t=T$, 由 $w_{i,1}=\frac{1}{N}$ 可得
$$
    w_{i,T} = \frac{1}{N} \frac{\exp (-y_i G(x_i))}{\prod_{s=1}^T Z_s}
    $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;adaboost-误差分析&#34;&gt;AdaBoost 误差分析
&lt;/h3&gt;&lt;p&gt;集成分类器的训练误差为&lt;/p&gt;
$$
\begin{aligned}
\hat{R}(f) &amp;= \frac{1}{N} \sum_{i=1}^N \mathbb{I}(f(x_i) \neq y_i) =\frac{1}{N} \sum_{i=1}^N \mathbb{I}(y_iG(x_i) \le 0) \\
&amp;\le \frac{1}{N} \sum_{i=1}^N \exp(-y_iG(x_i)) = \frac{1}{N} \sum_{i=1}^N \left( N \prod_{t=1}^T Z_tw_{i,T+1} \right) \\
&amp;=\prod_{t=1}^T Z_t = \prod_{t=1}^T \left(2\sqrt{e_t(1-e_t)}\right) \le \exp\left(-2 \sum_{t=1}^T \left(\frac{1}{2}-e_t\right)^2\right)\\
\end{aligned}
$$&lt;p&gt;这说明 AdaBoost 的训练误差是负指数量级的. 而且不需要提前知道 $e_t$ 的值, 只需要知道 $e_t$ 的上界即可, 这也是 AdaBoost 的 Adaptive 性质所在.&lt;/p&gt;
&lt;h3 id=&#34;加法模型&#34;&gt;加法模型
&lt;/h3&gt;&lt;p&gt;AdaBoost 可以看作是 $\{\alpha_t, f_t(x)\}$ 的加法模型, 如果我们采用指数损失函数 $\exp(-y_if(x))$, 令&lt;/p&gt;
$$
G_t(\alpha, f) = \frac{1}{N} \sum_{i=1}^N \exp(-y_iG(x_i) + \alpha f(x_i))
$$&lt;p&gt;则可以证明第 $t$ 轮迭代得到的 $(\alpha_t,f_t)$ 是最小化 $G_t(\alpha, f)$ 的解, 过程略.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;提升树模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 给定训练数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 其中 $x_i \in \mathcal{X}, y_i \in \mathcal{Y}=\{-1,+1\}, i=1,2,\cdots,N$; 弱学习算法(一般是分类或回归树) $\mathcal{T}$, 损失函数 $L(y,f(x))$;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 最终分类器 $f(x)$&lt;/p&gt;
&lt;p&gt;第 $m$ 个基学习器 $T(x; \Theta_m)$ 由经验风险最小化得到, 即&lt;/p&gt;
$$\Theta_m = \argmin_{\Theta} \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \Theta))$$&lt;/div&gt;
&lt;p&gt;如果是平方损失, 提升树算法实际上就是在拟合当前模型的残差.&lt;/p&gt;
&lt;h2 id=&#34;bagging-方法&#34;&gt;Bagging 方法
&lt;/h2&gt;&lt;p&gt;对训练样本进行重采样, 利用不同的样本数据来学习不同的基学习器, 通过降低方差来提高集成学习器的泛化性能. Bagging (Bootstrap Aggregating) 就是一种典型的并行集成学习方法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Bagging&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 给定训练数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 其中 $x_i \in \mathcal{X}, y_i \in \mathcal{Y}, i=1,2,\cdots,N$; 弱学习算法 $\mathcal{L}$ 以及基分类器个数 $T$;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 最终分类器 $f(x)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;对 $t=1,2,\ldots,T$, 从$D$利用自助采样法随机抽取 $N$ 个样本得到 $D_t$. 从 $D_t$ 依学习算法 $\mathcal{L}$ 学得基分类器 $f_t(x)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回集成分类器&lt;/p&gt;
$$f(x)=\argmax_{y\in\mathcal{Y}} \sum_{t=1}^T I(f_t(x)=y)$$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;对样本 $x_i$ 来说，我们采用 $x_i$ 未参与训练的基学习器在 $x_i$ 上的预测的简单投票结果作为 $x_i$ 的包外预测&lt;/p&gt;
$$f_{\text{oob}}(x_i) = \argmax_{y \in Y} \sum_{t=1}^{T} \mathbb{I} (f_t(x_i) = y) \mathbb{I} (x_i \notin D_t)$$&lt;p&gt;相应的 Bagging 算法泛化误差的包外估计 (out-of-bag estimate) 为&lt;/p&gt;
$$\epsilon^{\text{oob}} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}(f_{\text{oob}}(x_i) \neq y_i)$$&lt;h2 id=&#34;随机森林&#34;&gt;随机森林
&lt;/h2&gt;&lt;p&gt;所谓随机森林, 就是在 Bagging 方法中以决策树算法作为基学习器, 并引入随机属性选择, 即在选择划分特征时, 先从当前节点的所有 $d$ 个特征中随机选择 $k$ 个特征, 再从这 $k$ 个特征中选择最优划分特征. 随机选取的目的是为了增加基学习器之间的差异性, 使得集成学习器的泛化性能更好.&lt;/p&gt;
&lt;p&gt;当 $k=d$ 时, 随机森林退化为 Bagging 方法, 一般取 $k=\log_2 d$.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(6) —— 神经网络学习初步</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/nn-beginner/</link>
        <pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/nn-beginner/</guid>
        <description>&lt;h2 id=&#34;特征的线性组合&#34;&gt;特征的线性组合
&lt;/h2&gt;&lt;p&gt;之前的二分类问题中, 如果把 $z=w^Tx+b$ 看做是衍生的新特征, 实际上感知机的模型就是 $y=\text{sign}(z)$. 二项逻辑斯蒂回归模型中, $P(y=1 \mid x) = \sigma(z)=\frac{1}{1+e^{-z}}$. 相当于引入了一个 sigmoid 函数进行非线性变换.&lt;/p&gt;
&lt;p&gt;因此, 神经网络应运而生, 主要想法:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过各维特征线性组合得到新特征&lt;/li&gt;
&lt;li&gt;基于衍生特征通过非线性变换得到新特征&lt;/li&gt;
&lt;li&gt;再对新特征进行线性组合和非线性变换, 逐层叠加&lt;/li&gt;
&lt;li&gt;通过嵌套逼近复杂函数&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;多层前馈神经网络&#34;&gt;多层前馈神经网络
&lt;/h2&gt;&lt;p&gt;设当前的 (衍生) 特征向量是&lt;/p&gt;
$$
z = \left(z^{(1)},z^{(2)},\cdots,z^{(m)}\right)^T
$$&lt;p&gt;进行线性组合&lt;/p&gt;
$$
v \cdot z - \theta = \sum_{i=1}^m v_i z^{(i)} - \theta
$$&lt;p&gt;再通过非线性变换 (考虑到数学性质, 通常是 sigmoid 函数):&lt;/p&gt;
$$
t = g(v \cdot z - \theta)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多层前馈神经网络&lt;/strong&gt; 是常见的神经网络模型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;逐层排列神经元, 仅限于相邻层之间的完全连接;&lt;/li&gt;
&lt;li&gt;接受外部输入信号的神经元在同一层, 称为 &lt;strong&gt;输入层&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;最后一层神经元输出网络的结果, 称为 &lt;strong&gt;输出层&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;输入层和输出层之间的神经元称为 &lt;strong&gt;隐藏层&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;输入层直接接受激活函数, 输出层和隐藏层都对接受到的信号做激活函数变换.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所谓 &lt;strong&gt;感知机&lt;/strong&gt;, 就是没有隐藏层的前馈神经网络.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;前面学到的感知机学习能力有限, 例如它无法解决异或问题. 但是, 只要再加一层隐藏层, 就可以解决.&lt;/p&gt;
&lt;p&gt;考虑一个单隐层的神经网络:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入层有 $n$ 个神经元来接受输入信号;&lt;/li&gt;
&lt;li&gt;输出层有 $k$ 个神经元来输出结果, 且第 $l$ 个神经元的阈值是 $\theta_l$;&lt;/li&gt;
&lt;li&gt;隐藏层有 $m$ 个神经元, 第 $t$ 个神经元的阈值是 $\gamma_t$.&lt;/li&gt;
&lt;li&gt;输入层到隐藏层的权重是 $w_{jt}$, 隐藏层到输出层的权重是 $v_{tl}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因而, 隐藏层的输出是&lt;/p&gt;
$$
z^{(t)}(x)=\sigma \left(\sum_{j=1}^n w_{jt} x^{(j)} - \gamma_t \right)
$$&lt;p&gt;输出层的输出是&lt;/p&gt;
$$
y^{(l)}(x) = \sigma \left( \sum_{t=1}^m v_{tl} z^{(t)} - \theta_l \right)
$$&lt;p&gt;参数集为 $\Theta = \{w_{jt},v_{tl},\gamma_t,\theta_l\}$&lt;/p&gt;
&lt;h2 id=&#34;误差反向传播算法&#34;&gt;误差反向传播算法
&lt;/h2&gt;&lt;p&gt;我们采用平方误差作为预测损失函数, 则&lt;/p&gt;
$$
R(\Theta) = \sum_{i=1}^N R_i(\Theta) = \sum_{i=1}^N \| y_i - \hat{y}_i \| ^2 = \sum_{i=1}^N \sum_{l=1}^k (y_i^{(l)} - \hat{y}_i^{(l)})^2
$$&lt;p&gt;依然采用经验风险最小化策略, 通过梯度下降法来求解参数集 $\Theta$. 求偏导可得:&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial R_i(\Theta)}{\partial v_{tl}} &amp;= \delta_i^{(l)}z^{(t)}(x_i) \\
\frac{\partial R_i(\Theta)}{\partial \theta_l} &amp;= -\delta_i^{(l)} \\
\frac{\partial R_i(\Theta)}{\partial w_{jt}} &amp;= s_i^{(t)} x_i^{(j)} \\
\frac{\partial R_i(\Theta)}{\partial \gamma_t} &amp;= -s_i^{(t)}
\end{aligned}
$$&lt;p&gt;其中&lt;/p&gt;
$$
\begin{aligned}
\delta_i^{(l)}&amp;=-2(y_i^{(l)}-\hat{y}_i^{(l)})\hat{y}_i^{(l)}(1-\hat{y}_i^{(l)}) \\
s_i^{(t)} &amp;= z^{(t)}(x_i)(1-z^{(t)}(x_i))\sum_{l=1}^k v_{tl}\delta_i^{(l)}
\end{aligned}
$$&lt;p&gt;给定学习率 $\eta$, 按照 $\alpha = \alpha - \eta \frac{\partial R_i(\Theta)}{\partial \alpha}$ 进行迭代更新.&lt;/p&gt;
&lt;p&gt;采用正则化策略来缓解过拟合问题:&lt;/p&gt;
$$
\hat{\Theta} = \argmin_{\Theta} (R(\Theta) + \lambda J(\Theta))
$$&lt;p&gt;其中 $J(\Theta)$ 是正则化项, 通常是参数的 $L_2$ 范数, 所有参数的平方和.&lt;/p&gt;
&lt;p&gt;关于激活函数, 除了 sigmoid 函数, 还有 tanh 函数, ReLU 函数等. 前两者函数性质连续, 但是在部分情况可能导数接近 $0$, 从而导致梯度消失问题. 相对之下, ReLU 函数梯度计算简单. 还有带泄漏的 ReLU 函数:&lt;/p&gt;
$$
f(x) = \begin{cases}
x &amp; x&gt;0 \\
\lambda x &amp; x \leq 0
\end{cases}
$$&lt;p&gt;等等.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>最优化方法(5) —— 最优性理论</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/opt-theory/</link>
        <pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/opt-theory/</guid>
        <description>&lt;h2 id=&#34;对偶理论&#34;&gt;对偶理论
&lt;/h2&gt;&lt;p&gt;对于一般的约束优化问题:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f(x) \\
\text{s.t.} \quad &amp; c_i(x) \leq 0, \quad i \in \mathcal{I} \\
&amp; c_i(x) = 0, \quad i \in \mathcal{E}
\end{aligned}
$$&lt;p&gt;Lagrange 函数为:&lt;/p&gt;
$$
L(x, \lambda, \nu) = f(x) + \sum_{i \in \mathcal{I}} \lambda_i c_i(x) + \sum_{i \in \mathcal{E}} \nu_i c_i(x)
$$&lt;p&gt;其中 $\lambda_i \ge 0$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lagrange 对偶函数&lt;/strong&gt; $g: \mathbb{R}_+^m \times \mathbb{R}^p \to [-\infty, +\infty)$ 定义为:&lt;/p&gt;
$$
g(\lambda, \nu) = \inf_{x \in \mathbb{R}^n} L(x, \lambda, \nu)
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;弱对偶原理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若 $\lambda \ge 0$, 则 $g(\lambda, \nu) \le p^\ast$.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;对 $x_0 \in \mathcal{X}$, 有:&lt;/p&gt;
$$
g(\lambda, \nu) = \inf_{x \in \mathbb{R}^n} L(x, \lambda, \nu) \le L(x_0, \lambda, \nu) \le f(x_0)
$$&lt;p&gt;对 $x_0$ 取 $\inf$ 得到&lt;/p&gt;
$$
g(\lambda, \nu) \le \inf_{x \in \mathcal{X}} f(x) = p^\ast
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lagrange 对偶问题&lt;/strong&gt; 形式如下:&lt;/p&gt;
$$
\max_{\lambda \ge 0, \nu} g(\lambda, \nu)= \max_{\lambda \ge 0, \nu} \inf_{x \in \mathbb{R}^n} L(x, \lambda, \nu)
$$&lt;p&gt;称 $\lambda, \nu$ 为对偶变量, 设最优值为 $q^\ast$. $q^\ast$ 是 $p^\ast$ 的最优下界, 称 $p^\ast-q^\ast$ 为 &lt;strong&gt;对偶间隙&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;示例-线性规划问题&#34;&gt;示例: 线性规划问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; c^T x \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; x \ge 0
\end{aligned}
$$&lt;p&gt;Lagrange 函数为:&lt;/p&gt;
$$
\begin{aligned}
L(x, \lambda, \nu) &amp;= c^T x + \lambda^T (Ax - b) - \nu^T x \\
&amp;= -b^T \lambda + (c + A^T \lambda - \nu)^T x
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
g(\lambda, \nu) = \inf_{x \ge 0} L(x, \lambda, \nu) =
\begin{cases}
-b^T \lambda, &amp; c + A^T \lambda - \nu = 0 \\
-\infty, &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; -b^T \lambda \\
\text{s.t.} \quad &amp; c + A^T \lambda - \nu = 0 \\
&amp; \nu \ge 0
\end{aligned}
$$&lt;p&gt;可以计算证明, 线性规划问题和对偶问题互为对偶.&lt;/p&gt;
&lt;h3 id=&#34;示例-范数最小化问题&#34;&gt;示例: 范数最小化问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; \|x\| \\
\text{s.t.} \quad &amp; Ax = b
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
g(\nu) = \inf_x(\|X\| - \nu^T (Ax - b)) =
\begin{cases}
b^T\nu, &amp; \|A^T \nu\|_* \le 1 \\
-\infty, &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;其中 $\|v\|_*=\sup_{\|x\|\le 1} x^T v$ 为 $v$ 的对偶范数 (关于为什么自证). 因此对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; b^T \nu \\
\text{s.t.} \quad &amp; \|A^T \nu\|_* \le 1
\end{aligned}
$$&lt;h3 id=&#34;示例-最大割问题&#34;&gt;示例: 最大割问题
&lt;/h3&gt;&lt;p&gt;上回说到, 最大割问题可以写成:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; x^T W x \\
\text{s.t.} \quad &amp; x_i^2 = 1
\end{aligned}
$$&lt;p&gt;首先加负号变成 $\min$, Lagrange 函数为:&lt;/p&gt;
$$
\begin{aligned}
L(x, y) &amp;= -x^T W x + \sum_{i=1}^n y_i (x_i^2 - 1) \\
&amp;= x^T(\text{diag}(y) - W) x - \mathbf{1}^T y
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
g(y) = \inf_x L(x, y) =
\begin{cases}
-\mathbf{1}^T y, &amp; \text{diag}(y) - W \succeq 0 \\
-\infty, &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; -\mathbf{1}^T y \\
\text{s.t.} \quad &amp; \text{diag}(y) - W \succeq 0
\end{aligned}
$$&lt;p&gt;再来考虑这个对偶问题的对偶问题.&lt;/p&gt;
&lt;p&gt;对偶函数:&lt;/p&gt;
$$
\begin{aligned}
g(X)&amp;=\inf_y(\mathbf{1}^Ty) - \left&lt;\text{diag}(y) - W, X\right&gt; \\
&amp;= \inf \left(\sum_{i=1}^n (1-X_{ii})y_i + \left&lt; W, X \right&gt; \right) \\
&amp;= \begin{cases}
\left&lt; W, X \right&gt;, &amp; X_{ii} = 1, i = 1, \ldots, n \\
-\infty, &amp; \text{otherwise}
\end{cases}
\end{aligned}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \left&lt; W, X \right&gt; \\
\text{s.t.} \quad &amp; X_{ii} = 1, i = 1, \ldots, n \\
&amp; X \succeq 0
\end{aligned}
$$&lt;h3 id=&#34;示例-共轭函数&#34;&gt;示例: 共轭函数
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; f(x) \\
\text{s.t.} \quad &amp; Ax \le b \\
&amp; Cx = d
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
\begin{aligned}
g(\lambda, \nu) &amp;= \inf_x(f(x) + \lambda^T (Ax - b) + \nu^T (Cx - d)) \\
&amp;=\inf_x(f(x) + (A^T \lambda + C^T \nu)^T x - b^T \lambda - d^T \nu) \\
&amp;=-f^\ast(-A^T \lambda - C^T \nu) - b^T \lambda - d^T \nu
\end{aligned}
$$&lt;p&gt;其中 $f^\ast(v) = \sup_x(x^T v - f(x))$ 为 $f$ 的共轭函数.&lt;/p&gt;
&lt;h2 id=&#34;对偶性&#34;&gt;对偶性
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;弱对偶性&lt;/strong&gt;: $d^\ast \le p^\ast$. 对一般约束优化问题成立.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强对偶性&lt;/strong&gt;: $d^\ast = p^\ast$, 且若一个线性规划问题有最优解, 则其对偶问题有最优解, 且最优值相等. 一般不成立, 但通常对凸优化问题成立.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;称保证凸问题强对偶性成立的条件为 &lt;strong&gt;约束品性&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;考虑下面这个不满足强对偶性的例子.&lt;/p&gt;
$$
\begin{aligned}  
\min \quad &amp; x_0-x_1 \\
\text{s.t.} \quad &amp; x_0 \ge \sqrt{x_1^2+1}
\end{aligned}
$$&lt;p&gt;显然, $x_0-x_1 &gt; 0$, 但当 $x_0 \to \infty$ 时, $x_0-x_1 \to 0$, 因此 $p^\ast = 0$, 但是不可达. 而对偶问题是:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \lambda \\
\text{s.t.} \quad &amp; 1 \ge \sqrt{1 + \lambda^2}
\end{aligned}
$$&lt;p&gt;
因此 $\lambda \le 0$, $d^\ast = 0$. 虽然 $d^\ast = p^\ast$, 但原问题是没有最优解的.&lt;/p&gt;
&lt;p&gt;当然, 也有使得 $p^\ast \ne d^\ast$ 的例子.&lt;/p&gt;
&lt;h2 id=&#34;改写问题形式&#34;&gt;改写问题形式
&lt;/h2&gt;&lt;p&gt;当对偶问题难以推导或没有价值时, 可以尝试改写原问题的形式.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入新变量与等式约束;&lt;/li&gt;
&lt;li&gt;将显式约束隐式化或将隐式约束显式化;&lt;/li&gt;
&lt;li&gt;改变目标函数或者约束函数的形式. 例如, 用 $\phi(f(x))$ 取代 $f(x)$, 其中 $\phi$ 是凸的增函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先来看几个引入等式约束的例子.&lt;/p&gt;
&lt;h3 id=&#34;示例-函数值最小化问题&#34;&gt;示例: 函数值最小化问题
&lt;/h3&gt;$$
\min f(Ax+b)
$$&lt;p&gt;直接做对偶是常数, 没有意义. 可以改写为:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f(y) \\
\text{s.t.} \quad &amp; y = Ax + b
\end{aligned}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; b^T \nu-f^\ast(\nu)  \\
\text{s.t.} \quad &amp; A^T \nu = 0
\end{aligned}
$$&lt;h3 id=&#34;示例-范数逼近问题&#34;&gt;示例: 范数逼近问题
&lt;/h3&gt;$$
\min \|Ax-b\|_2
$$&lt;p&gt;改写成:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \|y\| \\
\text{s.t.} \quad &amp; y = Ax - b
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
\begin{aligned}
g(\nu) &amp;= \inf_{x,y}(\|y\| + \nu^Ty - \nu^TAx + \nu^Tb) \\
&amp;= \begin{cases}
\nu^T b + \inf_y(\|y\| + \nu^Ty), &amp; A^T \nu = 0 \\
-\infty, &amp; \text{otherwise}
\end{cases} \\
&amp;= \begin{cases}
\nu^T b, &amp; A^T \nu = 0, \|\nu\|_* \le 1 \\
-\infty, &amp; \text{otherwise}
\end{cases}
\end{aligned}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \nu^T b \\
\text{s.t.} \quad &amp; A^T \nu = 0 \\
&amp; \|\nu\|_* \le 1
\end{aligned}
$$&lt;h3 id=&#34;示例-l1-正则化问题&#34;&gt;示例: L1 正则化问题
&lt;/h3&gt;$$
\min_{x\in \mathbb{R}^n} \frac{1}{2}\|Ax-b\|^2 + \mu \|x\|_1
$$&lt;p&gt;改写成:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \frac{1}{2}\|r\|^2 + \mu \|x\|_1 \\
\text{s.t.} \quad &amp; r = Ax - b
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
\begin{aligned}
g(\nu) &amp;= \inf_{x,r}\left(\frac{1}{2}\|r\|_2^2 + \mu \|x\|_1 + \lambda^T(r-Ax+b)\right) \\
&amp;= \inf_{x,r}\left(\frac{1}{2}\|r\|^2 + \lambda^Tr + \mu \|x\|_1 - (A^T\lambda)^Tx + b^T\lambda\right) \\
&amp;= \begin{cases}
b^T \lambda - \frac{1}{2} \| \lambda \|^2, &amp; \| A^T \lambda \|_{\infty} \le \mu \\
-\infty, &amp; \text{otherwise}
\end{cases}
\end{aligned}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; b^T \lambda - \frac{1}{2} \| \lambda \|^2 \\
\text{s.t.} \quad &amp; \| A^T \lambda \|_{\infty} \le \mu
\end{aligned}
$$&lt;p&gt;现在考虑显式和隐式约束转化的例子.&lt;/p&gt;
&lt;h3 id=&#34;示例-带边界约束的线性规划问题&#34;&gt;示例: 带边界约束的线性规划问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; c^T x \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; -\mathbf{1} \le x \le \mathbf{1}
\end{aligned}
$$&lt;p&gt;我们把边界要求隐藏在目标函数中:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f(x) =\begin{cases}
c^T x, &amp; -\mathbf{1} \le x \le \mathbf{1} \\
+\infty, &amp; \text{otherwise}
\end{cases} \\
\text{s.t.} \quad &amp; Ax = b
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
\begin{aligned}
g(\nu) &amp;= \inf_{-\mathbf{1} \le x \le \mathbf{1}}(c^Tx+\nu^T(Ax-b)) \\
&amp;= -b^T \nu - \|A^T \nu + c\|_1
\end{aligned}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; -b^T \nu - \|A^T \nu + c\|_1 \\
\text{s.t.} \quad &amp; \nu \ge 0
\end{aligned}
$$&lt;h3 id=&#34;示例-广义不等式约束优化问题&#34;&gt;示例: 广义不等式约束优化问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; f(x) \\
\text{s.t.} \quad &amp; c_i(x) \preceq_{K_i} 0, \quad i \in \mathcal{I} \\
&amp; c_i(x) = 0, \quad i \in \mathcal{E}
\end{aligned}
$$&lt;p&gt;其中对于 $i \in \mathcal{I}$, $c_i: \mathbb{R}^n \to \mathbb{R}^{k_i}$ 是向量值函数, $K_i$ 是适当锥. 对于 $i \in \mathcal{E}$, $c_i: \mathbb{R}^n \to \mathbb{R}$ 是标量函数. 实际上这种情况下, 其 Lagrange 函数为:&lt;/p&gt;
$$
L(x, \lambda, \nu) = f(x) + \sum_{i \in \mathcal{I}} \left&lt;\lambda_i, c_i(x)\right&gt; + \sum_{i \in \mathcal{E}} \nu_i c_i(x), \quad \lambda_i \in K_i^\ast
$$&lt;p&gt;这里 $K_i^\ast$ 是 $K_i$ 的对偶锥. 此时也依然满足 $L(x, \lambda, \nu) \le f(x)$. 对偶问题依然是 $\max_{\lambda \in K_i^\ast, \nu} g(\lambda, \nu)$.&lt;/p&gt;
&lt;h3 id=&#34;示例-半定规划问题&#34;&gt;示例: 半定规划问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; \left&lt; C, X \right&gt; \\
\text{s.t.} \quad &amp; \left&lt; A_i, X \right&gt; = b_i, \quad i = 1, \ldots, m \\
&amp; X \succeq 0
\end{aligned}
$$&lt;p&gt;对偶函数:&lt;/p&gt;
$$
\begin{aligned}
g(\lambda, \nu) &amp;= \inf_X \left( \left&lt; C, X \right&gt; + \sum_{i=1}^m \lambda_i (\left&lt; A_i, X \right&gt; - b_i) - \left&lt;\nu,X\right&gt; \right) \\
&amp;= \begin{cases}
b^T\lambda, &amp; \sum_{i=1}^m \lambda_i A_i - C +\nu \succeq 0 \\
-\infty, &amp; \text{otherwise}
\end{cases}
\end{aligned}
$$&lt;p&gt;对偶问题:&lt;/p&gt;
$$
\max \quad b^T\lambda \\
\text{s.t.} \quad \sum_{i=1}^m \lambda_i A_i - C +\nu \succeq 0
$$&lt;p&gt;可以证明, 半定规划问题和对偶问题互为对偶.&lt;/p&gt;
&lt;h2 id=&#34;带约束凸优化问题的最优性理论&#34;&gt;带约束凸优化问题的最优性理论
&lt;/h2&gt;&lt;p&gt;综合来看, 前面的问题其实都可以写成如下的形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f(x) \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; c_i(x) \le 0, i=1, \ldots, m
\end{aligned}
$$&lt;p&gt;其中 $f(x)$ 为适当的凸函数, $c_i(x)$ 也是凸函数且 $\text{dom} c_i = \mathbb{R}^n$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定集合 $\mathcal{D}$, 设其仿射包为 $\text{affine} \mathcal{D}$, 则其 &lt;strong&gt;相对内点集&lt;/strong&gt; 定义为:&lt;/p&gt;
$$
\text{relint} \mathcal{D} = \{x \in \mathcal{D} \mid \exists r &gt; 0, B(x,r) \cap \text{affine} \mathcal{D} \subset \mathcal{D}\}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;若对于凸优化问题&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f(x) \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; c_i(x) \le 0, i=1, \ldots, m
\end{aligned}
$$&lt;p&gt;存在 $x \in \text{relint} \mathcal{D}$, 使得&lt;/p&gt;
$$c_i(x) &lt; 0, i=1, \ldots, m$$&lt;p&gt;则称对于该问题 &lt;strong&gt;Slater 约束品性&lt;/strong&gt; 成立. 该品性也称为 &lt;strong&gt;Slater 条件&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;注意: 如果某个不等式约束是仿射函数时, Slater 可以对这个不等式约束放宽到 $c_i(x) \le 0$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;若 Slater 约束品性成立, 则强对偶性成立.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;这里假设 $\mathcal{D}$ 内部非空, $A$ 是行满秩的 (否则可以去掉冗余约束), $p^\ast$ 是有限的. 要证明当 $d^\ast &gt; -\infty$ 时, 存在对偶可行解 $(\lambda^\ast, \nu^\ast)$ 使得 $g(\lambda^\ast, \nu^\ast) = d^\ast=p^\ast$.&lt;/p&gt;
&lt;p&gt;定义集合&lt;/p&gt;
$$
\begin{aligned}
\mathbb{A}&amp;= \{(u,v,t) \mid \exists x \in \mathcal{D}, c_i(x) \le u_i (i=1, \ldots, m), Ax-b=v, f(x) \le t\} \\ \mathbb{B}&amp;= \{(0,0,s) \mid s \le p^\ast\}
\end{aligned}
$$&lt;p&gt;若 $(0,0,t) \in \mathbb{A} \cap \mathbb{B}$, 则 $f(x) \le t&lt; p^\ast$ 矛盾. 因而两集合不交. 由于两者都是凸集, 由超平面分离定理, 存在 $(\lambda, \nu, \mu) \ne 0$ 和 $\alpha$ 使得&lt;/p&gt;
$$
\begin{aligned}
\lambda^Tu + \nu^Tv + \mu t &amp;\ge \alpha, \quad \forall (u,v,t) \in \mathbb{A} \\
\lambda^Tu + \nu^Tv + \mu t &amp;\le \alpha, \quad \forall (u,v,t) \in \mathbb{B}
\end{aligned}
$$&lt;p&gt;显见, $\lambda \ge 0, \mu \ge 0$, 否则可以让 $\mu_i, t \to +\infty$ 使得集合 $\mathbb{A}$ 上不等式左侧无下界. 由 $\mathbb{B}$ 的不等式, 立得 $\mu p^\ast \le \alpha$.&lt;/p&gt;
&lt;p&gt;对于 $(u,v,t)=(c_i(x), Ax-b, f(x))$, 代入有&lt;/p&gt;
$$
\sum_{i=1}^m \lambda_i c_i(x) + \nu^T(Ax-b) + \mu f(x) \ge \alpha \ge \mu p^\ast
$$&lt;p&gt;若 $\mu &gt; 0$, 则上式恰好对应 Lagrange 函数:&lt;/p&gt;
$$
L(x, \frac{\lambda}{\mu}, \frac{\nu}{\mu}) \ge p^\ast
$$&lt;p&gt;故 $g(\frac{\lambda}{\mu}, \frac{\nu}{\mu}) \ge p^\ast$, 再结合弱对偶性, $g(\frac{\lambda}{\mu}, \frac{\nu}{\mu}) = p^\ast$. 则此情况下强对偶性成立, 且最优解可达.&lt;/p&gt;
&lt;p&gt;若 $\mu=0$, 可以得到对于所有 $x \in \mathcal{D}$, 有&lt;/p&gt;
$$
\sum_{i=1}^m \lambda_i c_i(x) + \nu^T(Ax-b) \ge 0
$$&lt;p&gt;令 $x_S$ 为满足 Slater 条件的点, $Ax_S=b, c_i(x_S) &lt; 0$, 但 $\lambda \ge 0$ 则必须 $\lambda = 0$. 因此&lt;/p&gt;
$$
\nu^T(Ax-b) \ge 0, \quad \forall x \in \mathcal{D}
$$&lt;p&gt;$x_S$ 恰好是谷底的 $0$, 四周全部都 $\ge 0$ 不太可能. 更具体地, 由于 $(\lambda, \nu, \mu) \ne 0$, 则 $\nu \ne 0$. $A$ 是行满秩的, 则 $A^T \nu \ne 0$, 因为 $x_S \in \text{int}\mathcal{D}$, 则存在微扰 $e$ 使得&lt;/p&gt;
$$\widetilde{x} = x_S + e \in \mathcal{D}, \quad v^TAe &lt; 0$$&lt;p&gt;但是&lt;/p&gt;
$$
v^TAe = v^TA(\widetilde{x}-x_S) = v^T(A\widetilde{x} - b) \ge 0$$&lt;p&gt;矛盾. 因此 $\mu &gt; 0$, 强对偶性成立.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;现在假设 Slater 成立, $x^\ast, \lambda^\ast$ 是原问题和对偶问题的最优解. 由强对偶性,&lt;/p&gt;
$$
\begin{aligned}
f(x^\ast) = g(\lambda^\ast) &amp;= \inf_{x} f(x) + \sum_{i \in \mathcal{I}} \lambda_i c_i(x) + \sum_{i \in \mathcal{E}} \lambda_i c_i(x) \\
&amp;\le f(x^\ast) + \sum_{i \in \mathcal{I}} \lambda_i c_i(x^\ast) + \sum_{i \in \mathcal{E}} \lambda_i c_i(x^\ast) \\
&amp;\le f(x^\ast)
\end{aligned}
$$&lt;p&gt;因此等号要成立, 即 $\lambda_i c_i(x^\ast) = 0, i \in \mathcal{I}$, 这称为互补条件 (complementary slackness).&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;凸优化问题的一阶充要条件&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于凸优化问题, 用 $a_i$ 表示矩阵 $A^T$ 的第 $i$ 列, 如
果 Slater 条件成立, 那么$x^\ast, \lambda^\ast$ 分别是原始, 对偶全局最优解当且仅当&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;稳定性条件: $ 0 = \nabla f(x^\ast) + \sum_{i \in \mathcal{I}} \lambda_i^\ast \nabla c_i(x^\ast) + \sum_{i \in \mathcal{E}} \lambda_i^\ast a_i$&lt;/li&gt;
&lt;li&gt;原始可行性条件: $Ax^\ast = b, i \in \mathcal{E}; c_i(x^\ast) \le 0, i \in \mathcal{I}$&lt;/li&gt;
&lt;li&gt;对偶可行性条件: $\lambda_i^\ast \ge 0, i \in \mathcal{I}$&lt;/li&gt;
&lt;li&gt;互补松弛条件: $\lambda_i^\ast c_i(x^\ast) = 0, i \in \mathcal{I}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;必要性已知, 只证明充分性.&lt;/p&gt;
&lt;p&gt;考虑 Lagrange 函数&lt;/p&gt;
$$
L(x, \lambda) = f(x) + \sum_{i \in \mathcal{I}} \lambda_i c_i(x) + \sum_{i \in \mathcal{E}} \lambda_i (a_i^T x - b_i)
$$&lt;p&gt;固定 $\lambda = \bar{\lambda}$, 易见 $L$ 是关于 $x$ 的凸函数. 由凸函数全局最优点的一阶充要性可知, 此时 $\bar{x}$ 就是全局的极小点. 由 Lagrange 对偶函数的定义, 有&lt;/p&gt;
$$
L(\bar{x}, \bar{\lambda}) = \inf_x L(x, \bar{\lambda}) = g(\bar{\lambda})
$$&lt;p&gt;又由原始可行性条件和互补松弛条件, 有&lt;/p&gt;
$$
L(\bar{x}, \bar{\lambda}) = f(\bar{x})
$$&lt;p&gt;由弱对偶性&lt;/p&gt;
$$
L(\bar{x}, \bar{\lambda}) = f(\bar{x}) \ge p^\ast\ge d^\ast \ge g(\bar{\lambda}) = L(\bar{x}, \bar{\lambda})
$$&lt;p&gt;因此等号成立, $p^\ast = d^\ast$, 强对偶性成立, $\bar{x}, \bar{\lambda}$ 是全局最优解.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;例子-仿射空间的投影问题&#34;&gt;例子: 仿射空间的投影问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; \|x - y\|^2 \\
\text{s.t.} \quad &amp; Ax = b
\end{aligned}
$$&lt;p&gt;Lagrange 函数为&lt;/p&gt;
$$
L(x, \lambda) = \|x - y\|^2 + \lambda^T(Ax - b)
$$&lt;p&gt;Slater 条件成立, 由一阶充要条件, 有&lt;/p&gt;
$$
\begin{aligned}
x^\ast - y + A^T \lambda^\ast &amp;= 0 \\
Ax^\ast &amp;= b
\end{aligned}
$$&lt;p&gt;解之&lt;/p&gt;
$$
\begin{aligned}
\lambda^\ast &amp;= (AA^T)^{-1}(Ay-b) \\
x^\ast &amp;= y - A^T(AA^T)^{-1}(Ay-b)
\end{aligned}
$$&lt;p&gt;显然, $x^\ast$ 是 $y$ 在 $Ax=b$ 上的投影.&lt;/p&gt;
&lt;h3 id=&#34;例子-基追踪问题&#34;&gt;例子: 基追踪问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; \|x\|_1 \\
\text{s.t.} \quad &amp; Ax = b
\end{aligned}
$$&lt;p&gt;这个函数实际是不光滑的. 把 $x$ 写作 $x^+-x^-$, 再令 $y=[x^+, x^-]$, 等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \mathbf{1}^T y \\
\text{s.t.} \quad &amp; [A, -A] y = b \\
&amp; y \ge 0
\end{aligned}
$$&lt;p&gt;按照 KKT 条件, 有&lt;/p&gt;
$$
\begin{aligned}
\mathbf{1} + [A, -A]^T \lambda^\ast - \nu^\ast &amp;= 0 \\
[A, -A] y^\ast &amp;= b \\
y^\ast &amp;\ge 0 \\
\nu^\ast &amp;\ge 0 \\
\nu^\ast \odot y^\ast &amp;= 0
\end{aligned}
$$&lt;p&gt;直接推导也得到相应结果, 二者实际上是等价的. (利用 $x^\ast=y_i^\ast-y_{i+n}^\ast$, 代入验证最优点处方向导数为 $0$ 即可)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;最优化问题解的存在性&#34;&gt;最优化问题解的存在性
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;推广的 Weierstrass 定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果函数 $f: \mathcal{X} \to (-\infty, +\infty]$ 适当且闭, 且以下条件中至少一个成立:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{dom} f = \{ x\in \mathcal{X}: f(x) &lt; +\infty \} $ 是有界的;&lt;/li&gt;
&lt;li&gt;存在一个常数 $\bar{\g `2amma}$ 使得下水平集 $C_{\bar{\gamma}} = \{ x \in \mathcal{X}: f(x) \le \bar{\gamma} \}$ 是非空且有界的;&lt;/li&gt;
&lt;li&gt;$f$ 是强制的, 即对于任一满足 $\| x^k \| \to +\infty$ 的点列 $\{ x^k \} \subset \mathcal{X}$, 都有 $\lim_{k \to \infty} f(x^k) = +\infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则函数 $f$ 的最小值点集 $\{x \in \mathcal{X} \mid f(x) \le f(y), \forall y \in \mathcal{X}\}$ 非空且紧.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二个&lt;/strong&gt;: 又有&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(5) —— 决策树模型</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/</link>
        <pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/</guid>
        <description>&lt;h2 id=&#34;特征的分类能力评估&#34;&gt;特征的分类能力评估
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 其中 $x_i=\left(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(m)}\right) \in \mathcal{X}$ 是第 $i$ 个样本的特征向量, $y_i \in \mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$ 是第 $i$ 个样本的标签. 假设数据集 $D$ 根据特征分成了 $K$ 个子集 $D_1,D_2,\cdots,D_K$, 定义 &lt;strong&gt;经验熵&lt;/strong&gt; 为&lt;/p&gt;
$$
H(D) = -\sum_{k=1}^K \frac{|D_k|}{|D|} \log_2 \frac{|D_k|}{|D|}
$$&lt;p&gt;现在给定某维特征 $A$ 和其取值集合 $\{a_1,a_2,\cdots,a_m\}$, 根据 $A$ 的取值将数据集 $D$ 分成了 $m$ 个子集 $D_1^A,D_2^A,\cdots,D_m^A$, 并进一步考虑 $D_i^A$ 中的标签分布, 定义 &lt;strong&gt;条件经验熵&lt;/strong&gt; 为&lt;/p&gt;
$$
H(D|A) = \sum_{i=1}^m \frac{|D_i^A|}{|D|} H(D_i^A)
$$&lt;/div&gt;
&lt;p&gt;如果条件经验熵和经验熵之差越大, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;属性 $A$ 对数据集 $D$ 的 &lt;strong&gt;信息增益&lt;/strong&gt; $g(D,A)$ 定义为&lt;/p&gt;
$$
g(D,A) = H(D) - H(D|A)
$$&lt;/div&gt;
&lt;p&gt;考虑到信息增益的计算会偏向于选择取值较多的特征, 为了避免这种情况, 引入信息增益率来评估特征的分类能力.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;特征 $A$ 的 &lt;strong&gt;分裂信息&lt;/strong&gt; $IV(A)$ 定义为&lt;/p&gt;
$$
IV(A) = -\sum_{i=1}^m \frac{|D_i^A|}{|D|} \log_2 \frac{|D_i^A|}{|D|}
$$&lt;p&gt;特征 $A$ 的 &lt;strong&gt;信息增益率&lt;/strong&gt; $g_R(D,A)$ 定义为&lt;/p&gt;
$$
g_R(D,A) = \frac{g(D,A)}{IV(A)}
$$&lt;/div&gt;
&lt;p&gt;分裂信息其实就是按照 $A$ 取值作划分的经验熵.&lt;/p&gt;
&lt;p&gt;除了信息增益和信息增益率, 还有 Gini 指数可以用来评估特征的分类能力.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;数据集 $D$ 的 &lt;strong&gt;Gini 指数&lt;/strong&gt; $\text{Gini}(D)$ 定义为&lt;/p&gt;
$$
\text{Gini}(D) = 1 - \sum_{k=1}^K \left(\frac{|D_k|}{|D|}\right)^2
$$&lt;p&gt;特征 $A$ 的 &lt;strong&gt;Gini 指数&lt;/strong&gt; $\text{Gini}(D,A)$ 定义为&lt;/p&gt;
$$
\text{Gini}(D,A) = \sum_{i=1}^m \frac{|D_i^A|}{|D|} \text{Gini}(D_i^A)
$$&lt;p&gt;如果按照特征 $A$ 是否取值为 $a_i$ 对数据集 $D$ 进行划分 $D=D_i^A \cup (D-D_i^A)$, 则 $A=a_i$ 的 &lt;strong&gt;Gini 指数&lt;/strong&gt; $\text{Gini}_d(D,A=a_i)$ 定义为&lt;/p&gt;
$$
\text{Gini}_d(D,A=a_i) = \frac{|D_i^A|}{|D|} \text{Gini}(D_i^A) + \frac{|D-D_i^A|}{|D|} \text{Gini}(D-D_i^A)
$$&lt;/div&gt;
&lt;p&gt;Gini 指数可以看作任取两个样本, 它们的标签不一致的概率. 如果 Gini 指数越小, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.&lt;/p&gt;
&lt;h2 id=&#34;决策树模型&#34;&gt;决策树模型
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;生成决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 特征集 $\mathcal{A}=\{A_1,A_2,\cdots,A_m\}$, 最优特征选择函数 $F$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 决策树 $T$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若数据集 $D$ 中所有样本的标签都是 $c_k$, 则生成一个类标记为 $c_k$ 的叶结点, 返回 $T$;&lt;/li&gt;
&lt;li&gt;若 $A=\emptyset$, 且 $D$ 非空, 则生成一个单节点树, 并以 $D$ 中样本数最多的类标记作为该节点的类标记, 返回 $T$;&lt;/li&gt;
&lt;li&gt;计算 $A^\ast=F(D,\mathcal{A})$;&lt;/li&gt;
&lt;li&gt;对 $A^\ast$ 的每一个取值 $a_i$, 构造一个对应于 $D_i$ 的子节点;&lt;/li&gt;
&lt;li&gt;若 $D_i=\emptyset$, 则将子节点标记为叶结点, 类标记为 $D$ 中样本数最多的类标记;&lt;/li&gt;
&lt;li&gt;否则, 将 $D_i$ 中样本数最多的类标记作为该节点的类标记&lt;/li&gt;
&lt;li&gt;对每个 $D_i$ 对应的非叶子节点, 以 $D_i$ 为训练集, 以 $\mathcal{A}-\{A^\ast\}$ 为特征集, 递归调用 1-6 步, 构建决策树 $T$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;如果以信息增益为特征选择函数, 即 $A^\ast = \arg\max_{A \in \mathcal{A}} g(D,A)$, 则算法对应于 ID3 算法; 如果以信息增益率为特征选择函数, 即 $A^\ast = \arg\max_{A \in \mathcal{A}} g_R(D,A)$, 则算法对应于 C4.5 算法.&lt;/p&gt;
&lt;p&gt;二路划分会采用以特征的可能取值为切分点的二分法划分当前数据集, 例如与选择 Gini 指数最小的特征和切分点对应的特征值, 即 $(A^\ast,a^\ast) = \arg\min_{A \in \mathcal{A},a \in V(A)} \text{Gini}_d(D,A=a)$, 则算法对应于 CART 算法.&lt;/p&gt;
&lt;p&gt;为了降低过拟合风险, 可以对决策树进行剪枝. 常用的是后剪枝, 即先生成一棵完全生长的决策树, 然后根据泛化性能决定是否剪枝. 也可以采用正则化方法, 例如, 定义决策树 $T$ 的损失或代价函数:&lt;/p&gt;
$$
C_\alpha(T) = C(T) + \alpha |T|
$$&lt;p&gt;其中 $C(T)$ 用于衡量 $T$ 对 $D$ 的拟合程度, $|T|$ 表示 $T$ 的叶结点个数, $\alpha \geq 0$ 用于权衡拟合程度和模型复杂度.&lt;/p&gt;
&lt;p&gt;CART 算法有特别的剪枝处理: 从 CART 算法生成得到完整决策树 $T_0$ 开始, 产生一个递增的权衡系数序列 $0=\alpha_0 &lt; \alpha_1 &lt; \cdots &lt; \alpha_n &lt; +\infty$ 和一个嵌套的子树序列 $\{T_0, T_1, \cdots, T_n\}$, $T_i$ 为 $\alpha \in [\alpha_i, \alpha_{i+1})$ 时的最优子树, $T_n$ 是根节点单独构成的树.&lt;/p&gt;
&lt;p&gt;如果是连续特征, 则可以考虑将其离散化, 例如, 通过二分法将其划分为两个区间, 选择最优划分点.&lt;/p&gt;
&lt;p&gt;现在继续从经验风险的角度来看决策树模型.采用 $0-1$ 损失函数, 设节点 $t$ 设置的标记是 $c_k$, 则在 $t$ 对应的数据集上的经验风险为&lt;/p&gt;
$$
\frac{1}{|D_t|} \sum_{i=1}^{|D_t|} I(y_i \neq c_k)
$$&lt;p&gt;显见, 等价于&lt;/p&gt;
$$
\max_{c_k \in \mathcal{Y}} \frac{1}{|D_t|} \sum_{i=1}^{|D_t|} I(y_i = c_k)
$$&lt;p&gt;从现在来看, 决策树构造过程中划分的单元都是矩形的, 即分类边界是若干与特征坐标轴平行的边界组成. 多变量决策树模型允许用若干特征的线性组合来划分数据集, 对每个非叶结点学习一个线性分类器.&lt;/p&gt;
&lt;h2 id=&#34;最小二乘回归树模型&#34;&gt;最小二乘回归树模型
&lt;/h2&gt;&lt;p&gt;CART 算法用于回归问题时, 采用平方误差损失函数选择属性和切分点.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;CART&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集 $D=\{(x_i,y_i)\}_{i=1}^N$, 特征集 $\mathcal{A}=\{A_1,A_2,\cdots,A_m\}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 回归树 $T$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;设回归树将输入空间划分为 $M$ 个单元 $R_1,R_2,\cdots,R_M$, 并在每个单元上有一个固定的输出值 $c_m$, 则回归树模型可以表示为&lt;/p&gt;
$$
    f(x)=\sum_{m=1}^M c_m I(x \in R_m)
    $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果采用平方误差, 则 $R_m$ 的输出值 $c_m$ 应该是 $R_m$ 中所有样本输出值的均值, 即&lt;/p&gt;
$$
    \hat{c}_m = \frac{1}{|R_m|} \sum_{x_i \in R_m} y_i
    $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于一个输入空间, 若选用第 $j$ 维特征变量作为切分变量, $s$ 作为切分点, 则可以将输入空间划分为两个区域&lt;/p&gt;
$$
    R_1(j,s) = \{x|x^{(j)} \leq s\}, \quad R_2(j,s) = \{x|x^{(j)} &gt; s\}
    $$&lt;p&gt;则可以通过求解优化问题&lt;/p&gt;
$$
    \min_{j,s} \left[\min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2\right]
    $$&lt;p&gt;来确定最优切分变量 $j$ 和切分点 $s$. 实际上这里的 $c_i$ 就应该取 2 步中的 $\hat{c}_m$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从初始输入空间开始, 按照误差最小原则递归划分, 重复如上过程, 直到满足停止条件.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;对于剪枝, 和分类任务处理框架一致, 采用&lt;/p&gt;
$$
C_\alpha(T) = C(T) + \alpha |T|
$$&lt;p&gt;计算损失, 其中&lt;/p&gt;
$$C(T) = \sum_{t=1}^{|T|} N_tQ_t(T) = \sum_{t=1}^{|T|} \sum_{x_i \in R_t} (y_i-\hat{c}_t)^2$$&lt;p&gt;$N_t$ 表示叶结点 $t$ 中的样本数, $Q_t(T)$ 表示叶结点 $t$ 的均方损失, $\hat{c}_t$ 表示叶结点 $t$ 的输出值均值.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(4) —— 基于近邻的分类方法</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/knn/</link>
        <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/knn/</guid>
        <description>&lt;h2 id=&#34;k-近邻算法&#34;&gt;k-近邻算法
&lt;/h2&gt;&lt;p&gt;k-近邻算法的主要思想是, 对于一个给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点, 然后根据这 $k$ 个样本点的类别进行多数占优的投票方式来预测 $x$ 的类别.&lt;/p&gt;
&lt;p&gt;在 $n$ 维实数空间 $\mathbb{R}$ 中, 通常用 Minkowski 距离来度量两个点 $x_i, x_j$ 的相似性:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $x_i, x_j \in \mathbb{R}^n$, 则 $x_i, x_j$ 之间的 &lt;strong&gt;Minkowski 距离&lt;/strong&gt; $\text{dist}_p(x_i,x_j)$ 定义为&lt;/p&gt;
$$
\text{dist}_p(x_i,x_j) = \left( \sum_{l=1}^n |x_i^l - x_j^l|^p \right)^{1/p}
$$&lt;/div&gt;
&lt;p&gt;$p=1$ 时, 就是 Manhattan 距离; $p=2$ 时, 就是 Euclidean 距离; $p=\infty$ 时, 就是 Chebyshev 距离. 在必要时, 还可以给每个维度的特征值加权.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;k-近邻&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 给定训练样本集 $D = \{(x_i, y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_k\}$, 以及距离度量 $\text{dist}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 对于每个样本点 $x \in \mathbb{R}^n$, 预测其类别 $y$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;基于度量 $\text{dist}$, 对于给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点所构成的邻域 $N_k^{\text{dist}}(x)$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;采用如下的多数投票规则来预测 $x$ 的类别:&lt;/p&gt;
$$
    y = \argmax_{c_i} \sum_{x_j \in N_k^{\text{dist}}(x)} I(y_j = c_i)
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;如果把 0-1 作为损失函数, 那么 k-近邻算法实际上就是让经验风险最小化.&lt;/p&gt;
&lt;h2 id=&#34;最近邻算法&#34;&gt;最近邻算法
&lt;/h2&gt;&lt;p&gt;在 k-近邻算法中, 当 $k=1$ 时, 称为最近邻算法. 因此, 特点是偏差小, 方差大. 这其实是特征空间的一个划分 $\mathcal{X}=\bigcup_{i=1}^n \{R_i\}$. 对每个划分单元 $R_i$, 该单元的数据点到其他样本的距离都不会小于到 $x_i$ 的距离.&lt;/p&gt;
&lt;h2 id=&#34;最近邻算法的扩展&#34;&gt;最近邻算法的扩展
&lt;/h2&gt;&lt;p&gt;给定样本集 $D = \{(x_i,y_i)\}_{i=1}^n$, 以 $D_i$ 表示属于类 $c_i$ 的样本集, 希望找一个方式把每个 $D_i$ 分成 $k$ 个簇 $(D_{i1}, D_{i2}, \cdots, D_{ik})$, 使得数据分布的方差最小, 即&lt;/p&gt;
$$
(D^\ast_{i1}, D^\ast_{i2}, \cdots, D^\ast_{il}) = \argmin_{D_{i1}, D_{i2}, \cdots, D_{ik}} \sum_{j=1}^k \sum_{(x_t,y_t) \in D_{ij}} \Vert x_t-c_{ij} \Vert_2^2
$$&lt;p&gt;然而很难找到最优解, 因此采用迭代的方式来近似求解:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;K-means&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 给定训练样本集 $D = \{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_k\}$, 以及距离度量 $\text{dist}(x,y)=\Vert x-y \Vert_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 对于每个样本点 $x \in \mathbb{R}^n$, 预测其类别 $y$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化 $k$ 个簇的中心 $c_{ij}$;&lt;/li&gt;
&lt;li&gt;对每个 $(x_t, y_t) \in D_i$ (即 $y_t=c_i$), 令
$$I_{x_t}= \argmin_{j} \Vert x_t-c_{ij} \Vert_2^2$$
即将 $x_t$ 分配到最近的簇;&lt;/li&gt;
&lt;li&gt;对每个 $D_{ij}$, 更新均值
$$c_{ij} = \frac{1}{|D_{ij}|} \sum_{(x_t,y_t) \in D_{ij}} x_t$$&lt;/li&gt;
&lt;li&gt;重复 2, 3 直到收敛.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;有可能会使得某些离分类边界很近的点被错误分类. 引入学习向量量化方法 (LVQ 算法). 让同类和异类的点在构建过程中都能起作用.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;LVQ&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 给定训练样本集 $D = \{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_k\}$, 以及距离度量 $\text{dist}(x,y)=\Vert x-y \Vert_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 对于每个样本点 $x \in \mathbb{R}^n$, 预测其类别 $y$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对每个类 $c_m$ 随机选择 $k$ 个点 $I_{mi}$ 作为代表;&lt;/li&gt;
&lt;li&gt;对每个样本点 $x_t$, 找到最近的代表元 $I_{m^\ast i^\ast}$, 即
$$I_{m^\ast i^\ast} = \argmin_{m,i} \Vert x_t - I_{mi} \Vert_2^2$$&lt;/li&gt;
&lt;li&gt;如果 $y_t=c_{m^\ast}$, 则
$$I_{m^\ast i^\ast} \gets I_{m^\ast i^\ast} + \eta(x_t - I_{m^\ast i^\ast})$$
否则
$$I_{m^\ast i^\ast} \gets I_{m^\ast i^\ast} - \eta(x_t - I_{m^\ast i^\ast})$$&lt;/li&gt;
&lt;li&gt;重复 2, 3 直到收敛.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里 $\eta$ 是学习率.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;在 $\eta=1$ 时, LVQ 算法相当于逐步地进行 k-means 算法.&lt;/p&gt;
&lt;p&gt;在最近邻算法和其扩展方法中, 每个簇的代表点也称为相应单元的原型. 这种方法也常被称作原型方法或免模型方法.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(3) —— 基于后验概率最大化准则的分类模型</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/bayes/</link>
        <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/bayes/</guid>
        <description>&lt;h2 id=&#34;后验概率最大化准则&#34;&gt;后验概率最大化准则
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对训练样本集 $D=\{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathcal{X}$, $y_i \in \mathcal{Y} = \{c_1, c_2, \cdots, c_K\}$, 将 $x$ 的类别预测为 $c_i$ 的 &lt;strong&gt;风险&lt;/strong&gt; 为&lt;/p&gt;
$$
R(Y=c_i | x) = \sum_{j=1}^K \lambda_{ij} P(Y=c_j | x)
$$&lt;p&gt;其中 $\lambda_{ij}$ 是将属于 $c_j$ 的样本预测为 $c_i$ 的损失. &lt;strong&gt;最优预测&lt;/strong&gt; $\hat{y}$ 是使得风险最小的类别, 即&lt;/p&gt;
$$
\hat{y} = \argmin_{c_i} R(Y=c_i | x)
$$&lt;/div&gt;
&lt;p&gt;假设采用 $0-1$ 损失函数, 易知&lt;/p&gt;
$$
R(Y=c_i | x) = 1 - P(Y=c_i | x)
$$&lt;p&gt;即输入 $x$ 的最优预测 $\hat{y}$ 为使得后验概率 $P(y | x)$ 最大的类别.&lt;/p&gt;
&lt;h2 id=&#34;逻辑斯蒂回归模型&#34;&gt;逻辑斯蒂回归模型
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}=\mathbb{R}^n, \mathcal{Y}=\{c_1,c_2\}$, &lt;strong&gt;逻辑斯蒂回归模型&lt;/strong&gt; 是如下的后验概率分布:&lt;/p&gt;
$$
\begin{aligned}
P(Y=c_1 | x) &amp;= \frac{\exp(w \cdot x + b)}{1+\exp(w \cdot x + b)} \\
P(Y=c_2 | x) &amp;= \frac{1}{1+\exp(w \cdot x + b)
}
\end{aligned}
$$&lt;p&gt;其中 $w,b$ 是模型参数.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;按照后验概率最大化准则, 显然当 $w \cdot x + b &gt; 0$ 时, 预测为 $c_1$, 否则预测为 $c_2$.&lt;/p&gt;
&lt;p&gt;对于多类分类任务, 仍然可以使用逻辑斯蒂回归模型:&lt;/p&gt;
$$
\begin{aligned}
p(y=c_i | x) &amp;= \frac{\exp(w_i \cdot x + b_i)}{\sum_{j=1}^{K-1} \exp(w_j \cdot x + b_j)}, \quad i=1,2,\cdots,K-1 \\
p(y=c_K | x) &amp;= \frac{1}{\sum_{j=1}^{K-1} \exp(w_j \cdot x + b_j)}
\end{aligned}
$$&lt;p&gt;给定 $D=\{(x_i,y_i)\}_{i=1}^n$, 其中 $x_i \in \mathbb{R}^n$, $y_i \in \mathcal{Y} = \{0,1\}$, 用 $\theta=(w,b)$ 表示二项逻辑斯蒂回归模型的参数, 令&lt;/p&gt;
$$
p(x;\theta) = p(Y=1 | x;\theta)
$$&lt;p&gt;则考虑似然函数为&lt;/p&gt;
$$
\begin{aligned}
L(\theta) &amp;= \prod_{i=1}^n p(x_i;\theta)^{y_i} (1-p(x_i;\theta))^{1-y_i} \\
\log L(\theta) &amp;= \sum_{i=1}^n y_i \log p(x_i;\theta) + (1-y_i) \log (1-p(x_i;\theta)) \\
&amp;= \sum_{i=1}^N y_i(w \cdot x_i + b) - \log(1+\exp(w \cdot x_i + b))
\end{aligned}
$$&lt;p&gt;对 $w,b$ 求偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial \log L(\theta)}{\partial w} &amp;= \sum_{i=1}^n x_i(y_i - p(x_i;\theta)) = 0\\
\frac{\partial \log L(\theta)}{\partial b} &amp;= \sum_{i=1}^n (y_i - p(x_i;\theta)) = 0
\end{aligned}
$$&lt;h2 id=&#34;朴素-bayes-分类器&#34;&gt;朴素 Bayes 分类器
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Bayes 公式&lt;/span&gt;&lt;/p&gt;
$$
\begin{aligned}
P(Y=c_i | x) &amp;= \frac{P(x | Y=c_i) P(Y=c_i)}{P(x)} \\
&amp;= \frac{P(x | Y=c_i) P(Y=c_i)}{\sum_{j=1}^K P(x | Y=c_j) P(Y=c_j)}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;朴素 Bayes 假定特征之间相互独立, 即&lt;/p&gt;
$$
p(X^1=x^1, X^2=x^2, \cdots, X^n=x^n | Y=c_k) = \prod_{j=1}^n p(X^j=x^j | Y=c_k)
$$&lt;p&gt;对于输入实例 $x=(x^1,x^2,\cdots,x^n)$, 则后验概率&lt;/p&gt;
$$
p(Y=c_k|x)=\frac{\left( \prod_{i=1}^n p(X^i=x^i | Y=c_k) \right) P(Y=c_k)}{\sum_{j=1}^K \left( \prod_{i=1}^n p(X^i=x^i | Y=c_j) \right) P(Y=c_j)}
$$&lt;p&gt;分母是固定的, 只需比较分子的大小即可. 但是一旦某个特征取值和分类没有同时出现, 后验概率直接为 $0$, 为了避免这种情况, 通常引入一些平滑技术:&lt;/p&gt;
$$
p_{\lambda}(Y=c_k) = \frac{\sum_{j=1}^NI(y_j=c_k)+\lambda}{N+K\lambda}
$$&lt;p&gt;$\lambda=1$ 时称为 Laplace 平滑.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(2) —— 支持向量机</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/</link>
        <pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/</guid>
        <description>&lt;h2 id=&#34;线性可分支持向量机&#34;&gt;线性可分支持向量机
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于一个数据集 $D$, 如果能找到一个超平面 $H: w^Tx + b = 0$, 将数据分为两类. 即对任意 $(x_i, y_i) \in D$, 若 $y_i = 1$, 则 $w^Tx_i + b \geq 0$; 若 $y_i = -1$, 则 $w^Tx_i + b &lt; 0$. 则称 $D$ 是 &lt;strong&gt;线性可分的&lt;/strong&gt; , 超平面 $H$ 是 $D$ 的一个 &lt;strong&gt;分离超平面&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;最优超平面不仅要能够将数据分开, 还要使得两类数据点到超平面的距离尽可能远.&lt;/p&gt;
&lt;p&gt;考虑到 $w,b$ 任意缩放都不影响超平面的位置, 我们可以规定 $w^Tx + b = 1$ 为最近的正类数据点满足的方程. 此时距离为 $1/{\|w\|}$, 要最大化这个量, 即化归成凸二次规划问题:&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b} \frac{1}{2} \|w\|^2 \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1, \quad i = 1, 2, \cdots, n
\end{aligned}
$$&lt;p&gt;只要 $D$ 是线性可分的, 上述问题一定有解且唯一. 对应的分类决策函数&lt;/p&gt;
$$
f(x) = \text{sign}(w^Tx + b)
$$&lt;p&gt;称为 &lt;strong&gt;线性可分支持向量机&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;引入 Lagrange 乘子 $\alpha_i \geq 0$:&lt;/p&gt;
$$
L(w, b, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b) - 1)
$$&lt;p&gt;对 $w, b$ 求偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
&amp; w = \sum_{i=1}^n \alpha_i y_i x_i \\
&amp; 0 = \sum_{i=1}^n \alpha_i y_i
\end{aligned}
$$&lt;p&gt;代入 $L(w, b, \alpha)$, 得到对偶问题:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;线性可分对偶问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i \cdot x_j \\
&amp; \text{s.t.} \quad \alpha_i \geq 0, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;由 KKT 条件, 最优解一定满足&lt;/p&gt;
$$
\begin{aligned}
\alpha_i(y_i(w \cdot x_i + b) - 1) &amp;= 0 \\
y_i(w \cdot x_i + b) - 1 &amp;\geq 0 \\
\alpha_i &amp;\geq 0 \\
\end{aligned}
$$&lt;p&gt;由于 $\alpha_i$ 不全为 $0$, 存在 $j$ 使得 $y_j(w \cdot x_j + b) = 1$, 由此&lt;/p&gt;
$$
b = y_j - w \cdot x_j = y_j - \sum_{i=1}^n \alpha_i y_i x_i \cdot x_j
$$&lt;p&gt;乘上 $\alpha_jy_j$ 做累和, 有&lt;/p&gt;
$$
0=\sum_{j=1}^n \alpha_jy_jb = \sum_{j=1}^n \alpha_j - \| w \|^2
$$&lt;p&gt;上式中 $\alpha_i=0$ 的 $i$ 也成立, 因为都是 $0$ 不影响结果. 注意到 $w = \sum_{i=1}^n \alpha_i y_i x_i$ 也只收到 $\alpha_i &gt; 0$ 的影响, 而这些项的点都落在间隔边界&lt;/p&gt;
$$
H_1: w \cdot x + b = 1, \quad H_2: w \cdot x + b = -1
$$&lt;p&gt;上, 称这些点 $x_i$ 为 &lt;strong&gt;支持向量&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;支持向量机的留一误差&lt;/p&gt;
$$
\hat{R}_{\text{loo}} = \frac{1}{n} \sum_{i=1}^n I(f_{D-\{x_i\}}(x_i) \neq y_i)
$$&lt;p&gt;则 $\hat{R}_{\text{loo}} \le N_{SV}/n$, 其中 $N_{SV}$ 为支持向量的个数.&lt;/p&gt;
&lt;h2 id=&#34;线性支持向量机&#34;&gt;线性支持向量机
&lt;/h2&gt;&lt;p&gt;要求 $D$ 线性可分有点苛刻. 容忍一些误差, 引入松弛变量 $\xi_i \geq 0$, 使得约束条件变为&lt;/p&gt;
$$
y_i(w \cdot x_i + b) \geq 1 - \xi_i
$$&lt;p&gt;对于被错误分类的点, $\xi_i$ 可以大于 $1$. 把 $\xi_i \ne 0$ 的点视为特异点, 那么希望特异点尽可能少, 于是优化目标变为&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n I(\xi_i \ne 0) \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;直接用 $\xi_i$ 代替 $I(\xi_i \ne 0)$, 问题变为&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&amp; \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;既然要 $\xi_i$ 尽可能小, 不妨取 $\xi_i = 1 - y_i(w \cdot x_i + b)$,  引入合页损失函数 $h(z) = \max(0, 1-z)$, 即&lt;/p&gt;
$$\xi_i = h(y_i(w \cdot x_i + b))$$&lt;p&gt;则提出一个 $C$ 后, 优化目标变为&lt;/p&gt;
$$
\min_{w, b} \frac{1}{2C} \|w\|^2 + \sum_{i=1}^n h(y_i(w \cdot x_i + b))
$$&lt;p&gt;做了这么多, 只是相当于把 0-1 损失函数换成了合页损失函数.&lt;/p&gt;
&lt;p&gt;回到原问题, 引入 Lagrange 乘子 $\alpha_i, \beta_i \geq 0$, 得到&lt;/p&gt;
$$
L(w, b, \xi, \alpha, \beta) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b) - 1 + \xi_i) - \sum_{i=1}^n \beta_i \xi_i
$$&lt;p&gt;对 $w, b, \xi$ 偏导为 $0$, 得到&lt;/p&gt;
$$
\begin{aligned}
&amp; w = \sum_{i=1}^n \alpha_i y_i x_i \\
&amp; 0 = \sum_{i=1}^n \alpha_i y_i \\
&amp; \beta_i = C - \alpha_i
\end{aligned}
$$&lt;p&gt;代入 $L(w, b, \xi, \alpha, \beta)$, 得到对偶问题&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;线性支持向量机对偶问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i \cdot x_j \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;与线性可分支持向量机类似, 只是多了一个 $\alpha_i \leq C$ 的约束. 现在考虑 KKT 条件, 有&lt;/p&gt;
$$
\begin{aligned}
\alpha_i(y_i(w \cdot x_i + b) - 1 + \xi_i) &amp;= 0 \\
y_i(w \cdot x_i + b) - 1 + \xi_i &amp;\geq 0 \\
\beta_i \xi_i &amp;= 0 \\
\alpha_i &amp;\geq 0 \\
\beta_i &amp;\geq 0 \\
\alpha_i + \beta_i&amp;=C
\end{aligned}
$$&lt;p&gt;则 $\alpha_i &gt; 0$ 的点 $x_i$ 为支持向量, 满足 $y_i(w \cdot x_i + b) = 1 - \xi_i$. 这点与线性可分支持向量机的支持向量不同. 但进一步如果 $\alpha_i \lt C$ , 则 $\beta_i \gt 0$, 则 $\xi_i=0$, 从而 $y_i(w \cdot x_i + b) = 1$, 这样就一致了.&lt;/p&gt;
&lt;p&gt;进一步, 把 $y_i(w \cdot x_i + b) = 1$ 两边乘 $y_i$, 类似有&lt;/p&gt;
$$
b = y_j - \sum_{i=1}^n \alpha_i y_i x_i \cdot x_j
$$&lt;p&gt;因而最优分类超平面为&lt;/p&gt;
$$
\sum_{i=1}^n \alpha_i y_i x_i \cdot x + b = 0
$$&lt;p&gt;和决策函数&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i x_i \cdot x + b\right)
$$&lt;p&gt;超平面法向量可以被唯一确定, 但是偏置不唯一.&lt;/p&gt;
&lt;h2 id=&#34;smo-算法&#34;&gt;SMO 算法
&lt;/h2&gt;&lt;p&gt;SMO 算法是一种启发式算法, 用于求解支持向量机的对偶问题. SMO 算法的基本思想是: 每次选择两个变量, 固定其他变量, 优化这两个变量. 这样不断迭代, 直到收敛.&lt;/p&gt;
&lt;p&gt;设当前迭代的两个变量为 $\alpha_i, \alpha_j$, 则&lt;/p&gt;
$$
\alpha_1 y_1 + \alpha_2 y_2 = -\sum_{i=3}^n \alpha_i y_i
$$&lt;p&gt;同乘 $y_1$, 有&lt;/p&gt;
$$
\alpha_1 + \alpha_2 y_1y_2= -\sum_{i=3}^n \alpha_i y_1y_i
$$&lt;p&gt;记右边为 $\gamma$, $s=y_1y_2 \in \{-1, 1\}$, 则&lt;/p&gt;
$$
\alpha_1 + s\alpha_2 = \gamma
$$&lt;p&gt;记$K_{ij} = x_i \cdot x_j$, $v_i = \sum_{j=3}^{N} \alpha_j y_j K_{ij}$, 则对偶问题转化为&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha_1, \alpha_2} \alpha_1 + \alpha_2 - \frac{1}{2} K_{11}\alpha_1^2 - \frac{1}{2} K_{22}\alpha_2^2 - sK_{12}\alpha_1\alpha_2 - y_1v_1\alpha_1 - y_2v_2\alpha_2 \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \alpha_1 + s\alpha_2 = \gamma
\end{aligned}
$$&lt;p&gt;再由 $\alpha_1 = \gamma - s\alpha_2$, 代入目标函数, 并对 $\alpha_2$ 求导为 $0$, 得到&lt;/p&gt;
$$
\alpha_2 = \frac{s(K_{11}-K_{12})\gamma + y_2(v_1 - v_2) - s + 1}{K_{11} + K_{22} - 2K_{12}}
$$&lt;p&gt;代入 $v$ 的定义, 随后化简得&lt;/p&gt;
$$
\alpha_2 = \alpha_2^* + y_2 \frac{(y_2 - f(x_2))- (y_1-f(x_1))}{K_{11} + K_{22} - 2K_{12}}
$$&lt;p&gt;别忘了约束 $0 \le \alpha_1, \alpha_2 \le C$, 以及 $\alpha_1 + s\alpha_2 = \gamma$, 对 $\alpha_2$ 进行裁剪为 $\alpha_2^{\text{clip}}$. 相应地,&lt;/p&gt;
$$
\alpha_1 = \alpha_1^* + s(\alpha_2^* - \alpha_2^{\text{clip}})
$$&lt;p&gt;最后, 更新 $b$. 假设在 $\alpha_1, \alpha_2$ 中, $0 \lt \alpha_i \lt C$, 则&lt;/p&gt;
$$
b = y_i - \sum_{j=1}^n \alpha_j y_j K_{ij}
$$&lt;p&gt;关于选取 $\alpha_1, \alpha_2$, 一般有两个原则:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择违反 KKT 条件最严重的两个变量.&lt;/li&gt;
&lt;li&gt;选择两个变量使得目标函数有最大变化.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;核方法和非线性支持向量机&#34;&gt;核方法和非线性支持向量机
&lt;/h2&gt;&lt;p&gt;对于非线性问题, 可以通过核方法将数据映射到高维空间, 从而在高维空间中找到一个线性超平面.&lt;/p&gt;
&lt;p&gt;假设有一个映射 $\phi: \mathcal{X} \mapsto \mathcal{Z}$, 则在 $\mathcal{Z}$ 的线性支持向量机变为:&lt;/p&gt;
$$
\begin{aligned}
&amp; \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&amp; \text{s.t.} \quad y_i(w \cdot \phi(x_i) + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$&lt;p&gt;对应的对偶问题为&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \phi(x_i) \cdot \phi(x_j) \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;p&gt;相应的分类决策函数为&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i \phi(x_i) \cdot \phi(x) + b\right)
$$&lt;p&gt;然而, 直接计算 $\phi(x_i) \cdot \phi(x_j)$ 的复杂度很高. 为此, 引入核函数&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $\mathcal{X}$ 是输入空间, $\mathcal{Z}$ 是特征空间, 如果存在一个从 $\mathcal{X}$ 到 $\mathcal{Z}$ 的映射 $\phi$, 使得对任意 $x, x&#39; \in \mathcal{X}$, 都有&lt;/p&gt;
$$
K(x, x&#39;) = \phi(x) \cdot \phi(x&#39;)
$$&lt;p&gt;则称 $K$ 为 &lt;strong&gt;核函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;注意, 这里我们不再需要显式地计算 $\phi(x_i)$, 因为结果只与 $K(x_i, x_j)$ 有关.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;&lt;p class=&#34;math-block-title&#34;&gt;非线性支持向量机对偶问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j) \\
&amp; \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;此时, 分类决策函数为&lt;/p&gt;
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i K(x_i, x) + b\right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;$\mathcal{X}$ 上的函数 $K: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ 称为 &lt;strong&gt;正定对称核函数&lt;/strong&gt;, 如果对任意 $x_1, x_2, \cdots, x_n \in \mathcal{X}$, 核矩阵 (Gram 矩阵) $[K_{ij}]_{m \times m}$ 是半正定的.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;常见的核函数有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性核函数: $K(x, x&#39;) = x \cdot x&#39;$, 对应线性支持向量机.&lt;/li&gt;
&lt;li&gt;多项式核函数: $K(x, x&#39;) = (x \cdot x&#39; + 1)^d, c \gt 0$&lt;/li&gt;
&lt;li&gt;高斯核函数: $K(x, x&#39;) = \exp\left(-\frac{\|x-x&#39;\|^2}{2\sigma^2}\right), \sigma \gt 0$&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>机器学习基础(1) —— 概述</title>
        <link>https://LeoDreamer2004.github.io/p/machine-learning/intro/</link>
        <pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/machine-learning/intro/</guid>
        <description>&lt;h2 id=&#34;基础数学工具&#34;&gt;基础数学工具
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机变量 $X$ 的 &lt;strong&gt;期望&lt;/strong&gt; $E[X]$ 定义为&lt;/p&gt;
$$
E[X] = \sum_{x} x \cdot P(X=x)
$$&lt;p&gt;随机变量 $X$ 的 &lt;strong&gt;方差&lt;/strong&gt; $\text{Var}(X)$ 定义为&lt;/p&gt;
$$
\text{Var}(X) = E[(X - E[X])^2]
$$&lt;p&gt;&lt;strong&gt;标准差&lt;/strong&gt; $\sigma(X)$ 定义为&lt;/p&gt;
$$
\sigma(X) = \sqrt{\text{Var}(X)}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Markov 不等式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $X$ 是一个非负随机变量, 期望存在, 那么对于任意 $t &gt; 0$ 有&lt;/p&gt;
$$
P(X \geq t) \leq \frac{E[X]}{t}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Chebyshev 不等式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $X$ 是一个随机变量, 期望和方差都存在, 那么对于任意 $t &gt; 0$ 有&lt;/p&gt;
$$
P(|X - E[X]| \geq t) \leq \frac{\text{Var}(X)}{t^2}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机变量 $X$ 和 $Y$ 的 &lt;strong&gt;协方差&lt;/strong&gt; $\text{Cov}(X, Y)$ 定义为&lt;/p&gt;
$$
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]
$$&lt;p&gt;如果 $\text{Cov}(X, Y) = 0$, 则称 $X$ 和 $Y$ &lt;strong&gt;不相关&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;协方差具有对称性, 双线性.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;随机向量 $X=(X_1, X_2, \ldots, X_n)$ 的 &lt;strong&gt;协方差矩阵&lt;/strong&gt; $C(X)$ 定义为&lt;/p&gt;
$$
C(X) = E[(X - E[X])(X - E[X])^T] = (\text{Cov}(X_i, X_j))_{ij}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gauss 分布&lt;/strong&gt; (正态分布) 的概率密度函数为&lt;/p&gt;
$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$&lt;p&gt;&lt;strong&gt;Laplace 分布&lt;/strong&gt; 的概率密度函数为&lt;/p&gt;
$$
f(x) = \frac{1}{2b} \exp(-\frac{|x-\mu|}{b})
$$&lt;/div&gt;
&lt;p&gt;最优化问题&lt;/p&gt;
$$
\begin{aligned}
&amp; \min f(x) \\
\text{s.t. } &amp; c_i(x) \leq 0, i = 1, 2, \dots, k \\
&amp; h_j(x) = 0, j = 1, 2, \dots, l
\end{aligned}
$$&lt;p&gt;构造 Lagrange 函数&lt;/p&gt;
$$
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{k} \alpha_i c_i(x) + \sum_{j=1}^{l} \beta_j h_j(x)
$$&lt;p&gt;引入 Karush-Kuhn-Tucker (KKT) 条件&lt;/p&gt;
$$
\begin{aligned}
&amp; \nabla_x L(x, \alpha, \beta) = 0 \\
&amp; c_i(x) \leq 0, i = 1, 2, \dots, k \\
&amp; h_j(x) = 0, j = 1, 2, \dots, l \\
&amp; \alpha_i c_i(x) = 0, i = 1, 2, \dots, k \\
&amp; \alpha_i \geq 0, i = 1, 2, \dots, k
\end{aligned}
$$&lt;h2 id=&#34;基本概念和术语&#34;&gt;基本概念和术语
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;监督学习&lt;/strong&gt;: 基于标记数据 $T=\{ (x_i,y_i) \}_{i=1}^N$, 学习一个从输入空间到输出空间的映射 $f: \mathcal{X} \mapsto \mathcal{Y}$. 利用此对未见数据进行预测. 通常分为 &lt;strong&gt;回归&lt;/strong&gt; 和 &lt;strong&gt;分类&lt;/strong&gt; 两类.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt;: 基于未标记数据 $T=\{ x_i \}_{i=1}^N$, 发现其中隐含的知识模式. &lt;strong&gt;聚类&lt;/strong&gt; 是典型的无监督学习任务.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;半监督学习&lt;/strong&gt;: 既有标记数据又有未标记数据 (通常占比较大).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;: 通过观察环境的反馈, 学习如何选择动作以获得最大的奖励.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;模型评估与选择&#34;&gt;模型评估与选择
&lt;/h2&gt;&lt;h3 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h3&gt;&lt;p&gt;模型基于算法按照一定策略给出假设 $h \in \mathcal{H}$, 通过 &lt;strong&gt;损失函数&lt;/strong&gt; $L(h(x), y)$ 衡量假设的好坏.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0-1 损失函数:&lt;/li&gt;
&lt;/ul&gt;
$$L(h(x), y) = \mathbb{I}(h(x) \neq y) = \begin{cases} 0, &amp; h(x) = y \\ 1, &amp; h(x) \neq y \end{cases}$$&lt;ul&gt;
&lt;li&gt;平方损失函数:&lt;/li&gt;
&lt;/ul&gt;
$$L(h(x), y) = (h(x) - y)^2$$&lt;p&gt;平均损失 $R(h) = E_{x \sim D} [L(h(x), y)]$ 称为 &lt;strong&gt;泛化误差&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;容易验证, 对于 0-1 损失函数, 准确率 $a = 1-R(h)$.&lt;/p&gt;
&lt;h3 id=&#34;二分类&#34;&gt;二分类
&lt;/h3&gt;&lt;p&gt;对于二分类问题, 样本预测结果有四种情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;真正例&lt;/strong&gt; (True Positive, TP): 预测为正例, 实际为正例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;假正例&lt;/strong&gt; (False Positive, FP): 预测为正例, 实际为负例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真负例&lt;/strong&gt; (True Negative, TN): 预测为负例, 实际为负例.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;假负例&lt;/strong&gt; (False Negative, FN): 预测为负例, 实际为正例.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由此引入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准确率(查准率):&lt;/strong&gt; $P = \frac{TP}{TP+FP}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;召回率(查全率):&lt;/strong&gt; $R = \frac{TP}{TP+FN}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$F_1$ 度量:&lt;/strong&gt; 考虑到二者抵触, 引入调和均值 $F_1 = \frac{2PR}{P+R}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;过拟合和正则化&#34;&gt;过拟合和正则化
&lt;/h3&gt;&lt;p&gt;为了防止由于模型过于复杂而导致的过拟合, 可以通过 &lt;strong&gt;正则化&lt;/strong&gt; 方法来限制模型的复杂度.&lt;/p&gt;
$$
\min \sum_{i=1}^{N} L(h(x_i), y_i) + \lambda J(h)
$$&lt;p&gt;其中 $J(h)$ 是随着模型复杂度增加而增加的函数. $\lambda$ 是正则化参数.&lt;/p&gt;
&lt;p&gt;怎么选取合适的 $\lambda$ ? 一般是先给出若干候选, 在验证集上进行评估, 选取泛化误差最小的.&lt;/p&gt;
&lt;h3 id=&#34;数据集划分&#34;&gt;数据集划分
&lt;/h3&gt;&lt;p&gt;一般将数据集划分为 &lt;strong&gt;训练集&lt;/strong&gt; $T$ 和 &lt;strong&gt;测试(验证)集&lt;/strong&gt; $T^\prime$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;留出法 (hold-out)&lt;/strong&gt;: 分层无放回地随机采样. 也叫简单交叉验证.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$k$ 折交叉验证 ($k$-fold cross validation)&lt;/strong&gt;: 将数据集分为 $k$ 个大小相等的子集, 每次取其中一个作为验证集, 其余作为训练集, 最后以这 $k$ 次的平均误差作为泛化误差的估计. 当 $k=|D|$ 时称为留一 (leave-one-out) 验证法.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自助法 (bootstrapping)&lt;/strong&gt;: 从数据集中&lt;em&gt;有放回地&lt;/em&gt;采样 $|D|$ 个数据作为训练集, 没抽中的作为验证集. 因而训练集 $T$ 和原始数据集 $D$ 的分布未必一致, 对数据分布敏感的模型不适用.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;偏差-方差分解&#34;&gt;偏差-方差分解
&lt;/h2&gt;&lt;p&gt;为什么泛化误差会随着模型复杂度的增加而先减小后增大?&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;偏差&lt;/strong&gt; (bias): 模型预测值的期望与真实值之间的差异. 体现了模型的拟合能力.&lt;/p&gt;
$$\text{Bias}(x) = E_T[h_T(x)-c(x)] = \bar{h}(x) - c(x)$$&lt;p&gt;&lt;strong&gt;方差&lt;/strong&gt; (variance): 模型预测值的方差. 体现了模型的对数据扰动的稳定性.&lt;/p&gt;
$$\text{Var}(x) = E[(h(x) - \bar{h}(x))^2]$$&lt;/div&gt;
&lt;p&gt;现在对泛化误差进行分解:&lt;/p&gt;
$$
\begin{aligned}
R(h) &amp;= E_T[(h_T(x) - c(x))^2] \\
&amp;= E_T[h_T^2(x) - 2h_T(x)c(x) + c^2(x)] \\
&amp;= E_T[h_T^2(x)] - 2c(x)E_T[h_T(x)] + c^2(x) \\
&amp;= E_T[h_T^2(x)] - \bar{h}^2(x) + \bar{h}^2(x) - 2\bar{h}(x)c(x) + c^2(x) \\
&amp;= E_T[(h_T(x) - \bar{h}(x))^2] + (\bar{h}(x) - c(x))^2 \\
&amp;= \text{Var}(x) + \text{Bias}^2(x)
\end{aligned}
$$&lt;p&gt;当然, 由于噪声存在, $y$ 未必一定等于 $c(x)$, 不妨设 $y=c(x)+\varepsilon$, 其中 $\varepsilon \sim \Epsilon$ 期望为 $0$. 可以证明&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;偏差-方差分解&lt;/span&gt;&lt;/p&gt;
$$
E_{T \sim D^{|T|}, \varepsilon \sim \Epsilon} [(h_T(x)-y)^2] = \text{Bias}^2(x) + \text{Var}(x) + E[\varepsilon^2]
$$&lt;p&gt;即泛化误差可以分解为偏差、方差和噪声三部分.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;起初, 模型较为简单, 偏差在泛化误差起主导作用. 随着模型复杂度的增加, 拟合能力增强, 偏差减小, 但带来过拟合风险, 算法对数据扰动敏感, 方差增大. 方差占比逐渐增大, 最终导致泛化误差增大.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>最优化方法(4) —— 优化问题</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/opt-problem/</link>
        <pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/opt-problem/</guid>
        <description>&lt;h2 id=&#34;凸优化&#34;&gt;凸优化
&lt;/h2&gt;&lt;p&gt;凸问题的可行集都是凸集.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;凸优化问题的任意局部极小点都是全局最优点.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;假设 $x$ 是局部极小, $y$ 全局最优且 $f(y) &lt; f(x)$.&lt;/p&gt;
&lt;p&gt;考虑 $z = \theta x + (1-\theta) y$, 则由于 $z$ 是可行点的凸组合, 也是可行点. 由于 $f$ 是凸函数, 有&lt;/p&gt;
$$
f(z) \leq \theta f(x) + (1-\theta) f(y) &lt; f(x)
$$&lt;p&gt;取 $\theta \to 1$, 则 $f(z) \to f(x)$, 与局部最小性矛盾.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;线性规划&#34;&gt;线性规划
&lt;/h2&gt;&lt;p&gt;所谓 &lt;strong&gt;线性规划(LP)&lt;/strong&gt; 问题是指目标函数和约束条件都是线性的优化问题. 一般形式如下:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; c^T x \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; Gx \le e
\end{aligned}
$$&lt;p&gt;$\ell_1$ 和 $\ell_\infty$ 范数实际上也是线性的.&lt;/p&gt;
&lt;h3 id=&#34;示例-最大球问题&#34;&gt;示例: 最大球问题
&lt;/h3&gt;&lt;p&gt;凸多边形&lt;/p&gt;
$$P = \{ x \mid a_i^Tx \le b_i \}$$&lt;p&gt;的 Chebyshev 中心是最大半径内切球的中心. 代入得&lt;/p&gt;
$$
\sup \{a_i^T (x_c + u) \mid \Vert u \Vert_2 \le r \} = a_i^Tx_c + r \Vert a_i \Vert_2 \le b_i
$$&lt;p&gt;这也变成了一个线性规划问题.&lt;/p&gt;
&lt;h2 id=&#34;二次规划&#34;&gt;二次规划
&lt;/h2&gt;&lt;p&gt;二次规划问题是指目标函数是二次的的优化问题.&lt;/p&gt;
&lt;p&gt;例如, 对于线性约束条件的问题, 一般形式如下:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \frac{1}{2} x^T P x + q^T x + r \\
\text{s.t.} \quad &amp; Ax = b \\
&amp; Gx \le e
\end{aligned}
$$&lt;p&gt;也有 &lt;strong&gt;带二次约束的二次规划 (QCQP)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;我们归结为 &lt;strong&gt;二次锥规划 (SOCP)&lt;/strong&gt;:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; f^T x \\
\text{s.t.} \quad &amp; \Vert A_i x + b_i \Vert_2 \le c_i^T x + d_i, \quad i = 1, \ldots, m \\
&amp; Fx = g
\end{aligned}
$$&lt;h3 id=&#34;示例-最小范数问题&#34;&gt;示例: 最小范数问题
&lt;/h3&gt;&lt;p&gt;令 $\bar{v}_i = A_ix+b_i \in \mathbb{R}^{n_i}$, 则 $\min_x \sum_i \Vert \bar{v}_i \Vert_2$ 等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \sum_i v_{i0} \\
\text{s.t.} \quad &amp;\bar{v}_i = A_i x + b_i \\
&amp;(v_{i0}, \bar{v}_i) \succeq_\mathcal{Q} 0
\end{aligned}
$$&lt;p&gt;其中 $\mathcal{Q}$ 是二次锥.&lt;/p&gt;
&lt;h3 id=&#34;示例-最小化最大函数和问题&#34;&gt;示例: 最小化最大函数和问题
&lt;/h3&gt;&lt;p&gt;设 $\theta(x)=(\theta_1(x), \theta_2(x), \cdots, \theta_m(x))^T$. $\theta_{[i]}$ 是 $\theta_i$ 的非递增方式的排序. 则 $\min_{x\in Q} \sum_{i=1}^m \theta_{[i]}(x)$ 等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \sum_{i=1}^m u_i + kt \\
\text{s.t.} \quad &amp; x \in Q \\
&amp; \theta_i(x)\le u_i + t \\
&amp; u_i \ge 0
\end{aligned}
$$&lt;h2 id=&#34;半定优化&#34;&gt;半定优化
&lt;/h2&gt;&lt;p&gt;半定优化 (SDP) 一般形式如下:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; c^Tx \\
\text{s.t.} \quad &amp; x_1A_1 + \cdots + x_nA_n + B \succeq 0 \\
&amp; Gx=h
\end{aligned}
$$&lt;p&gt;其实是线性规划在矩阵空间的推广. 仍然考虑标准形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \left&lt; C, X \right&gt; \\
\text{s.t.} \quad &amp; \left&lt; A_i, X \right&gt; = b_i, \quad i = 1, \ldots, m \\
&amp; X \succeq 0
\end{aligned}
$$&lt;p&gt;和对偶形式:&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \sum_{i=1}^m b_i y_i \\
\text{s.t.} \quad &amp; C - \sum_{i=1}^m y_i A_i \succeq 0
\end{aligned}
$$&lt;h3 id=&#34;示例-二次约束二次规划问题&#34;&gt;示例: 二次约束二次规划问题
&lt;/h3&gt;$$
\begin{aligned}
\min \quad &amp; x^T A_0 x + 2b_0^T x + c_0 \\
\text{s.t.} \quad &amp; x^T A_i x + 2b_i^T x + c_i \le 0, \quad i = 1, \ldots, m
\end{aligned}
$$&lt;p&gt;其中 $A_i$ 是 $n \times n$ 的对称矩阵, 这个问题在 $A_i$ 不定时实际上是 NP-hard 的. 考虑它的半定松弛, 记 $X=x^Tx$ 注意到有&lt;/p&gt;
$$
x^T A_i x + 2b_i^T x + c_i = \left&lt; A_i, X \right&gt; + 2\left&lt; b_i, x \right&gt; + c_i = \left&lt;
\begin{bmatrix}
A_i &amp; b_i \\
b_i^T &amp; c_i
\end{bmatrix},
\begin{bmatrix}
X &amp; x \\
x^T &amp; 1
\end{bmatrix}
\right&gt;
$$&lt;p&gt;我们记作 $\left&lt; \bar{A}_i, \bar{X} \right&gt;$. 注意到, 现在唯一的非线性部分是约束 $X=xx^T$, 我们将其松弛成半正定约束 $X \succeq xx^T$. 可以证明, $\bar{X} \succeq 0$ 等价于 $X \succeq xx^T$. 这样我们就得到了一个半定优化问题:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \left&lt; A_0, X \right&gt; \\
\text{s.t.} \quad &amp; \left&lt; \bar{A}_i, \bar{X} \right&gt; \le 0, \quad i = 1, \ldots, m \\
&amp; \bar{X} \succeq 0 \\
&amp; \bar{X}_{n+1,n+1} = 1
\end{aligned}
$$&lt;h3 id=&#34;示例-最大割问题&#34;&gt;示例: 最大割问题
&lt;/h3&gt;&lt;p&gt;令 $G$ 为一个无向图, 节点集为 $V = \{1, 2, \cdots, n\}$, 边集为 $E$. 设 $w_{ij} = w_{ji} \ge 0$ 为边 $(i, j) \in E$ 上的权重, 要找 $S \subseteq V$ 使得 $S$ 与 $\bar{S}$ 之间相连边的权重之和最大化.&lt;/p&gt;
&lt;p&gt;我们定义 $x_j = 1, j \in S$ 和 $x_j = -1, j \in \bar{S}$, 则&lt;/p&gt;
$$
\begin{aligned}
\max \quad &amp; \sum_{(i, j) \in E} \frac{1}{2} (1-x_i x_j) w_{ij} \\
\text{s.t.} \quad &amp; x_i = \pm 1, \quad i = 1, \ldots, n
\end{aligned}
$$&lt;p&gt;然而这是一个离散优化问题, 考虑对它做松弛. 令 $W=(w_{ij}) \in \mathbb{S}^n$ 为权重矩阵, $C=-\frac{1}{4}(\text{Diag}(W\mathbf{1})-W)$ 是 Laplacian 矩阵的 $-1/4$ 倍. 则&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; x^T C x \\
\text{s.t.} \quad &amp; x_i^2 = 1, \quad i = 1, \ldots, n
\end{aligned}
$$&lt;p&gt;仍令 $X=x^Tx$, 则容易看出与下问题等价:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \left&lt; C, X \right&gt; \\
\text{s.t.} \quad &amp; X_{ii} = 1, \quad i = 1, \ldots, n \\
&amp; X \succeq 0 \\
&amp; \text{rank}(X) = 1
\end{aligned}
$$&lt;h3 id=&#34;示例-极小化最大特征值问题&#34;&gt;示例: 极小化最大特征值问题
&lt;/h3&gt;$$
\min \quad \lambda_{\max}(A_0 + \sum_{i=1}^m x_i A_i)
$$&lt;p&gt;注意到:&lt;/p&gt;
$$\lambda_{\max}(A) \le t \Leftrightarrow A \preceq tI$$&lt;p&gt;于是我们有 SDP 形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; z \\
\text{s.t.} \quad &amp; A_0 + \sum_{i=1}^m x_i A_i \preceq zI
\end{aligned}
$$&lt;h3 id=&#34;示例-极小化二范数问题&#34;&gt;示例: 极小化二范数问题
&lt;/h3&gt;$$
\min \quad \Vert A_0 + \sum_{i=1}^m x_i A_i \Vert_2
$$&lt;p&gt;记 $A = A_0 + \sum_{i=1}^m x_i A_i$. 注意到:&lt;/p&gt;
$$
\Vert A \Vert_2 \le t \Leftrightarrow A^TA \preceq t^2I \Leftrightarrow
\begin{bmatrix}
tI &amp; A \\
A^T &amp; tI
\end{bmatrix} \succeq 0
$$&lt;p&gt;于是我们有 SDP 形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; t \\
\text{s.t.} \quad &amp; \begin{bmatrix}
tI &amp; A \\
A &amp; tI
\end{bmatrix} \succeq 0
\end{aligned}
$$&lt;h3 id=&#34;示例-特征值优化问题&#34;&gt;示例: 特征值优化问题
&lt;/h3&gt;$$
\min \quad \sum_{i=1}^n \lambda_{[i]}(A_0 + \sum_{j=1}^m x_j A_j)
$$&lt;p&gt;其中 $\lambda_{[i]}(A)$ 表示 $A$ 的第 $i$ 大特征值. 前面的极小最大函数和提到它等价于&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; \sum_{i=1}^n u_i + kt \\
\text{s.t.} \quad &amp; u_i+t \ge \lambda_i(A_0 + \sum_{j=1}^m x_j A_j), \quad i = 1, \ldots, n \\
&amp; u_i \ge 0
\end{aligned}
$$&lt;p&gt;设 $u_i = \lambda_i(X)$, 则写成 SDP 形式:&lt;/p&gt;
$$
\begin{aligned}
\min \quad &amp; kt + \text{Tr}(X) \\
\text{s.t.} \quad &amp; tI + X \succeq A_0 + \sum_{j=1}^m z_j A_j \\
&amp; X \succeq 0
\end{aligned}
$$</description>
        </item>
        <item>
        <title>最优化方法(3) —— 凸函数</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/convex-function/</link>
        <pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/convex-function/</guid>
        <description>&lt;h2 id=&#34;基本线性代数知识&#34;&gt;基本线性代数知识
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定函数 $f: \mathbb{R}^n \mapsto \mathbb{R}$, 且 $f$ 在 $x$ 一个邻域内有定义, 若存在 $g \in \mathbb{R}^n$, 使得&lt;/p&gt;
$$
\lim_{p \to 0} \frac{f(x+p)-f(x)-g^Tp}{\Vert p \Vert} = 0
$$&lt;p&gt;其中 $\Vert \cdot \Vert$ 是向量范数, 则称 $f$ 在 $x$ 处 &lt;strong&gt;可微&lt;/strong&gt;. 此时, $g$ 称为 $f$ 在 $x$ 处的 &lt;strong&gt;梯度&lt;/strong&gt;, 记为 $\nabla f(x)$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;显然, 如果梯度存在, 令 $p = \varepsilon e_i$, 易得&lt;/p&gt;
$$
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果函数 $f(x): \mathbb{R}^n \mapsto \mathbb{R}$ 在点 $x$ 处的二阶偏导数 $\dfrac{\partial^2 f}{\partial x_i \partial x_j}$ 存在, 则称 $f$ 在 $x$ 处 &lt;strong&gt;二次可微&lt;/strong&gt;. 此时, $n \times n$ 矩阵&lt;/p&gt;
$$
\nabla^2 f(x) = \begin{pmatrix}
\dfrac{\partial^2 f}{\partial x_1^2} &amp; \dfrac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial x_1 \partial x_n} \\
\dfrac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \dfrac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dfrac{\partial^2 f}{\partial x_n \partial x_1} &amp; \dfrac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$&lt;p&gt;称为 $f$ 在 $x$ 处的 &lt;strong&gt;Hessian 矩阵&lt;/strong&gt;. 若 $\nabla^2 f(x)$ 在 $D$ 上连续, 则称 $f$ 在 $D$ 上 &lt;strong&gt;二次连续可微&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;可以证明, 若 $f$ 在 $D$ 上二次连续可微, 则 $\nabla^2 f(x)$ 为对称矩阵.&lt;/p&gt;
&lt;p&gt;多元函数的梯度可以推广到变量是矩阵的情形.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定函数 $f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$, 且 $f$ 在 $X$ 一个邻域内有定义, 若存在 $G \in \mathbb{R}^{m \times n}$, 使得&lt;/p&gt;
$$
\lim_{V \to 0} \frac{f(X+V)-f(X)-\left&lt; G, V \right&gt;}{\Vert V \Vert} = 0
$$&lt;p&gt;其中 $\Vert \cdot \Vert$ 是矩阵范数, 则称 $f$ 在 $X$ 处 &lt;strong&gt;(Fréchet)可微&lt;/strong&gt;. 此时, $G$ 称为 $f$ 在 $X$ 处的 &lt;strong&gt;梯度&lt;/strong&gt;, 记为 $\nabla f(X)$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;矩阵的可微有另一种较为简单常用的定义.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定函数 $f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$, 若存在矩阵 $G \in \mathbb{R}^{m \times n}$, 使得&lt;/p&gt;
$$
\lim_{t \to 0} \frac{f(X+tV)-f(X)}{t} = \left&lt; G, V \right&gt;
$$&lt;p&gt;则称 $f$ 在 $X$ 处 &lt;strong&gt;(Gâteaux)可微&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$f(X) = \text{tr}(AX^TB)$, 此时 $\nabla f(X) = BA$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f(X, Y)=\frac{1}{2} \Vert XY-A \Vert_F^2$. 此时&lt;/p&gt;
$$
    \begin{aligned}
    &amp;f(X,Y+tV)-f(X,Y) \\
    &amp;= \frac{1}{2} \Vert X(Y+tV)-A \Vert_F^2 - \frac{1}{2} \Vert XY-A \Vert_F^2 \\
    &amp;= \frac{1}{2} \Vert XY - A + tVX \Vert_F^2 - \frac{1}{2} \Vert XY - A \Vert_F^2 \\
    &amp;= \frac{1}{2} \Vert tVX \Vert_F^2 + \left&lt; XY-A, tVX \right&gt; \\
    &amp;= t \left&lt; X^T(XY-A), V \right&gt; + o(t)
    \end{aligned}
    $$&lt;p&gt;所以 $\frac{\partial f}{\partial Y} = X^T(XY-A)$, 类似地, $\frac{\partial f}{\partial X} = (XY-A)Y^T$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f(X)=\ln\text{det}(X)$, $X$ 为正定矩阵. 此时&lt;/p&gt;
$$
    \begin{aligned}
    &amp;f(X+tV)-f(X) \\
    &amp;= \ln\text{det}(X+tV) - \ln\text{det}(X) \\
    &amp;= \ln\text{det}(I+tX^{-1/2}VX^{-1/2})
    \end{aligned}
    $$&lt;p&gt;考虑 $X^{-1/2}VX^{-1/2}$ 的特征值 $\lambda_i$, 则由特征值之和为迹, 有&lt;/p&gt;
$$
    \begin{aligned}
    &amp;= \ln\text{det}\prod_{i=1}^n (1+t\lambda_i) \\
    &amp;= \sum_{i=1}^n \ln(1+t\lambda_i) \\
    &amp;= \sum_{i=1}^n t\lambda_i + o(t) \\
    &amp;= t\text{tr}(X^{-1/2}VX^{-1/2}) + o(t) \\
    &amp;= t\text{tr}(X^{-1}V) + o(t) \\
    &amp;= t\left&lt; X^{-T}, V \right&gt; + o(t)
    \end{aligned}
    $$&lt;p&gt;所以 $\nabla f(X) = X^{-T}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;广义实数&lt;/strong&gt; 是一种扩充实数域的数, 记为 $\bar{\mathbb{R}} = \mathbb{R} \cup \{ \pm \infty \}$. 映射 $f: \mathbb{R}^n \mapsto \bar{\mathbb{R}}$ 称为 &lt;strong&gt;广义实值函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定广义实值函数 $f$ 和非空集合 $X$. 如果存在 $x \in X$ 使得 $f(x) &lt; +\infty$, 并且对任意的 $x \in X$, 都有 $f(x) &gt; -\infty$, 那么称函数 $f$ 关于集合 $X$ 是 &lt;strong&gt;适当的&lt;/strong&gt;．&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于广义实值函数 $f: \mathbb{R}^n \mapsto \bar{\mathbb{R}}$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$C_\alpha = \{x \mid f(x) \le \alpha \}$ 称为 $f$ 的 &lt;strong&gt;$\alpha$-下水平集&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$\text{epi} f = \{ (x, t) \mid f(x) \le t \}$ 称为 $f$ 的 &lt;strong&gt;上方图&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;若 $\text{epi} f$ 为闭集, 则称 $f$ 为&lt;strong&gt;闭函数&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;若对任意的 $x \in \mathbb{R}^n$, 有 $\liminf_{y \to x} f(y) \ge f(x)$, 则称 $f$ 为 &lt;strong&gt;下半连续函数&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;对于广义实值函数 $f$, 以下命题等价:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f(x)$ 的任意 $\alpha$-下水平集都是闭集;&lt;/li&gt;
&lt;li&gt;$f(x)$ 是下半连续的;&lt;/li&gt;
&lt;li&gt;$f(x)$ 是闭函数.&lt;/li&gt;
&lt;/ol&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;(1) $\Rightarrow$ (2): 反证, 假设 $x_k \to \bar{x}$ 但 $\liminf_{k \to \infty} f(x_k) &lt; f(\bar{x})$. 取 $t$ 介于二者之间.&lt;/p&gt;
&lt;p&gt;考虑到 $\liminf_{k \to \infty} f(x_k) &lt; t$, 则有无穷多 $x_k$ 使得 $f(x_k) \le t$, 即这些 $x_k$ 在 $C_t$ 中. 由于 $C_t$ 是闭集, 则 $\bar{x} \in C_t$, 即 $f(\bar{x}) \le t$, 矛盾.&lt;/p&gt;
&lt;p&gt;(2) $\Rightarrow$ (3): 考虑 $(x_k,y_k) \in \text{epi} f \to (\bar{x},\bar{y})$, 由于 $f$ 下半连续, 则&lt;/p&gt;
$$ f(\bar{x}) \le \liminf_{k \to \infty} f(x_k) = \liminf_{k \to \infty} y_k = \bar{y} $$&lt;p&gt;即 $(\bar{x}, \bar{y}) \in \text{epi} f$.&lt;/p&gt;
&lt;p&gt;(3) $\Rightarrow$ (1): 考虑 $x_k \in C_\alpha \to \bar{x}$, 则 $(x_k, \alpha) \in \text{epi} f \to (\bar{x}, \alpha)$, 所以 $(\bar{x}, \alpha) \in \text{epi} f$, 即 $f(\bar{x}) \le \alpha$, 所以 $\bar{x} \in C_\alpha$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;适当闭函数的和, 复合, 逐点上确界仍然是闭函数.&lt;/p&gt;
&lt;h2 id=&#34;凸函数&#34;&gt;凸函数
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;适当函数 $f: \mathbb{R}^n \mapsto \mathbb{R}$ 称为 &lt;strong&gt;凸函数&lt;/strong&gt;, 如果 $\text{dom} f$ 是凸集, 且对任意的 $x, y \in \text{dom} f$ 和 $\theta \in [0,1]$, 有&lt;/p&gt;
$$
f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)
$$&lt;/div&gt;
&lt;p&gt;易知仿射函数既是凸函数又是凹函数. 所有的范数都是凸函数.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;若存在常数 $m &gt; 0$, 使得 $g(x) = f(x) - \frac{m}{2} \Vert x \Vert^2$ 是凸函数, 则称 $f$ 是 &lt;strong&gt;强凸函数&lt;/strong&gt;, $m$ 称为 &lt;strong&gt;强凸参数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;凸函数判定定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;适当函数 $f: \mathbb{R}^n \mapsto \mathbb{R}$ 是凸函数的充要条件是, 对任意的 $x \in \text{dom} f$, 函数 $g: \mathbb{R} \mapsto \mathbb{R}$ 是凸函数, 其中&lt;/p&gt;
$$g(t) = f(x+tv), \quad \text{dom}g = \{ t \mid x + tv \in \text{dom} f \}$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;一阶条件&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于定义在凸集上的可微函数 $f$, $f$ 是凸函数当且仅当&lt;/p&gt;
$$
f(y) \ge f(x) + \nabla f(x)^T(y-x), \quad \forall x, y \in \text{dom} f
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必要性&lt;/strong&gt;: 设 $f$ 凸, 则 $\forall x, y \in \text{dom} f, t \in [0,1]$, 有&lt;/p&gt;
$$tf(y)+(1-t)f(x) \ge f(x+t(y-x))$$&lt;p&gt;令 $t \to 0$, 即&lt;/p&gt;
$$f(y)-f(x) \ge \frac{f(x+t(y-x))-f(x)}{t} \to \nabla f(x)^T(y-x)$$&lt;p&gt;&lt;strong&gt;充分性&lt;/strong&gt;: $\forall x, y \in \text{dom}f, t\in (0,1)$, 取 $z = tx+(1-t)y$, 则&lt;/p&gt;
$$
\begin{aligned}
f(x) &amp;\ge f(z) + \nabla f(z)^T(x-z)  \\
f(y) &amp;\ge f(z) + \nabla f(z)^T(y-z)
\end{aligned}
$$&lt;p&gt;一式乘以 $t$, 二式乘以 $1-t$, 相加即得.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;梯度单调性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $f$ 为可微函数, 则 $f$ 为凸函数当且仅当 $\text{dom} f$ 为凸集且 $\nabla f$ 为单调映射.&lt;/p&gt;
$$(\nabla f(x) - \nabla f(y))^T(x-y) \ge 0$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必要性&lt;/strong&gt;: 根据一阶条件, 有&lt;/p&gt;
$$
\begin{aligned}
f(y) &amp;\ge f(x) + \nabla f(x)^T(y-x) \\
f(x) &amp;\ge f(y) + \nabla f(y)^T(x-y)
\end{aligned}
$$&lt;p&gt;相加即可.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;充分性&lt;/strong&gt;: 考虑 $g(t)=f(x+t(y-x))$, 则 $g^\prime(t)=\nabla f(x+t(y-x))^T (y-x)$, 从而 $g^\prime (t) \ge g^\prime (0)$.&lt;/p&gt;
$$
\begin{aligned}
f(y) &amp;= g(1) = g(0) + \int_{0}^1 g^\prime(t) dt \\
&amp;\ge g(0) + \int_{0}^1 g^\prime(0) dt = g(0) + g^\prime(0) \\ &amp;= f(x) + \nabla f(x)^T(y-x)
\end{aligned}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;函数 $f(x)$ 是凸函数当且仅当 $\text{epi}f$ 是凸集.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;二阶条件&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设 $f$ 为定义在凸集上的二阶连续可微函数, $f$ 是凸函数当且仅当 $\nabla^2 f(x) \succeq 0, \forall x \in \text{dom} f$. 若不取等, 则为严格凸函数.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必要性&lt;/strong&gt;: 反设 $f(x)$ 在 $x$ 处 $\nabla^2 f(x) \prec 0$, 则存在 $v \in \mathbb{R}^n$, 使得 $v^T \nabla^2 f(x) v &lt; 0$, 考虑 Peano 余项&lt;/p&gt;
$$
f(x+tv)=f(x)+t\nabla f(x)^Tv+\frac{t^2}{2}v^T\nabla^2 f(x+tv)v + o(t^2)
$$&lt;p&gt;取 $t$ 充分小,&lt;/p&gt;
$$
\frac{f(x+tv)-f(x)-t\nabla f(x)^T v}{t^2}=\frac{1}{2}v^T\nabla^2 f(x+tv)v + o(1) &lt; 0
$$&lt;p&gt;这和一阶条件矛盾.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;充分性&lt;/strong&gt;: 对于任意的 $x, y \in \text{dom} f$, 有&lt;/p&gt;
$$
\begin{aligned}
f(y) &amp;= f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^T\nabla^2 f(z)(y-x) \\
    &amp;\ge f(x)+\nabla f(x)^T(y-x)
\end{aligned}
$$&lt;p&gt;由一阶条件, $f$ 为凸函数.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;保凸运算&#34;&gt;保凸运算
&lt;/h2&gt;&lt;p&gt;下面举一些重要的例子.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;逐点取上界: 若对每个 $y \in A$, $f(x,y)$ 都是关于 $x$ 的凸函数, 则&lt;/p&gt;
$$g(x)=\sup_{y \in A} f(x,y)$$&lt;p&gt;也是凸函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$C$ 的支撑函数 $f(x)=\sup_{y \in C} y^Tx$ 是凸函数.&lt;/li&gt;
&lt;li&gt;$C$ 到 $x$ 的最远距离 $f(x)=\sup_{y \in C} \Vert x-y \Vert$ 是凸函数.&lt;/li&gt;
&lt;li&gt;对称阵 $X \in \mathbb{S}^n$ 的最大特征值 $\lambda_{\max}(X)=\sup_{\Vert x \Vert=1} x^TXx$ 是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标量函数的复合: 若 $g: \mathbb{R}^n \mapsto \mathbb{R}$ 是凸函数, $h: \mathbb{R} \mapsto \mathbb{R}$ 是单调不减的凸函数, 则&lt;/p&gt;
$$f(x) = h(g(x))$$&lt;p&gt;也是凸函数. 凹同理.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $g$ 凸, 则 $f(x) = \exp(g(x))$ 也是凸函数.&lt;/li&gt;
&lt;li&gt;如果 $g$ 凹, 则 $f(x) = 1/g(x)$ 也是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;取下确界: 若 $f(x, y)$ 关于 $(x, y)$ 整体是凸函数, $C$ 是凸集, 则&lt;/p&gt;
$$g(x) = \inf_{y \in C} f(x, y)$$&lt;p&gt;也是凸函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;凸集 $C$ 到 $x$ 的距离 $f(x)=\inf_{y \in C} \Vert x-y \Vert$ 是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;透视函数: 若 $f: \mathbb{R}^{n} \mapsto \mathbb{R}$ 是凸函数, 则&lt;/p&gt;
$$g(x, t) = tf(x/t), \quad \text{dom} g = \{ (x, t) \mid x / t \in \text{dom} f, t &gt; 0 \}$$&lt;p&gt;也是凸函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相对熵函数 $g(x,t)=t\log t-t\log x$ 是凸函数.&lt;/li&gt;
&lt;li&gt;若 $f$ 凸, 则 $g(x)=(c^T+d)f((Ax+b)/(c^T+d))$ 也是凸函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;共轭函数: 任意适当函数 $f$ 的共轭函数&lt;/p&gt;
$$f^\ast(y)=\sup_{x \in \text{dom} f} (\left&lt; x, y \right&gt; - f(x))$$&lt;p&gt;是凸函数.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;凸函数的推广&#34;&gt;凸函数的推广
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;$f: \mathbb{R}^n \mapsto \mathbb{R}$ 称为 &lt;strong&gt;拟凸的&lt;/strong&gt;, 如果 $\text{dom} f$ 是凸集, 且对任意 $\alpha$, 下水平集 $C_\alpha$ 是凸集.&lt;/p&gt;
&lt;p&gt;若 $f$ 是拟凸的, 则称 $-f$ 是 &lt;strong&gt;拟凹的&lt;/strong&gt;. 若 $f$ 既是拟凸又是拟凹的, 则称 $f$ 是 &lt;strong&gt;拟线性的&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;注意: 拟凸函数的和不一定是拟凸函数.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拟凸函数满足类 Jenson 不等式: 对拟凸函数 $f$ 和 $\forall x, y \in \text{dom} f, \theta \in [0,1]$, 有&lt;/li&gt;
&lt;/ul&gt;
$$f(\theta x + (1-\theta)y) \le \max\left\{f(x),f(y)\right\}$$&lt;ul&gt;
&lt;li&gt;拟凸函数满足一阶条件: 定义在凸集上的可微函数 $f$ 拟凸当且仅当&lt;/li&gt;
&lt;/ul&gt;
$$f(y) \le f(x) \Rightarrow \nabla f(x)^T(y-x) \le 0$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果正值函数 $f$ 满足 $\log f$ 是凸函数, 则 $f$ 称为 &lt;strong&gt;对数凸函数&lt;/strong&gt;; 若为凹函数, 则 $f$ 称为 &lt;strong&gt;对数凹函数&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;例如, 正态分布&lt;/p&gt;
$$f(x) = \frac{1}{\sqrt{(2\pi)^n \text{det} \Sigma}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$&lt;p&gt;是对数凹函数.&lt;/p&gt;
&lt;p&gt;对数凹函数的乘积, 积分都是对数凹的, 但加和不一定是对数凹的.&lt;/p&gt;
&lt;p&gt;在广义不等式下, 也可以定义凸凹性.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;$f: \mathbb{R}^n \mapsto \mathbb{R}^m$ 称为 &lt;strong&gt;$K$-凸函数&lt;/strong&gt;, 如果 $\text{dom} f$ 是凸集, 且
&lt;/p&gt;
$$f(\theta x+(1-\theta)y \preceq_K \theta f(x)+(1-\theta)f(y))$$&lt;p&gt;
对任意 $x,y \in \text{dom} f, 0 \le \theta \le 1$ 成立.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;例如, $f: \mathbb{S}^m \mapsto \mathbb{S}^m$, $f(X)=X^2$ 是 $\mathbb{S}^m_+$-凸函数. 这点利用 $z^TX^2z=\Vert Xz \Vert^2$ 是关于 $X$ 的凸函数即可得知.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>最优化方法(2) —— 凸集</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/convex-set/</link>
        <pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/convex-set/</guid>
        <description>&lt;h2 id=&#34;范数&#34;&gt;范数
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;记号 $\Vert \cdot \Vert: \mathbb{R}^n \mapsto \mathbb{R}$ 称为 &lt;strong&gt;向量范数&lt;/strong&gt;, 若满足:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正定性: $\Vert x \Vert \geq 0$, 且 $\Vert x \Vert = 0 \Leftrightarrow x = 0$;&lt;/li&gt;
&lt;li&gt;齐次性: $\Vert \alpha x \Vert = \vert \alpha \vert \Vert x \Vert$;&lt;/li&gt;
&lt;li&gt;三角不等式: $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;$\ell_p$ 范数是最常见的向量范数&lt;/p&gt;
$$
\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right) ^{\frac{1}{p}}
$$&lt;p&gt;特别地, 当 $p = \infty$ 时, $\Vert x \Vert_\infty = \max_i \vert x_i \vert$.&lt;/p&gt;
&lt;p&gt;向量范数可以自然地推广到矩阵范数. 常见的矩阵范数有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;和范数&lt;/strong&gt;: $\Vert A \Vert_1 = \sum_{i,j} \vert A_{ij} \vert$;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frobenius 范数&lt;/strong&gt;: $\Vert A \Vert_F = \sqrt{\sum_{i,j} A_{ij} ^2} = \sqrt{\text{tr}(A^T A)}$;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算子范数&lt;/strong&gt;: $\Vert A \Vert_{(m,n)}=\max_{\Vert x \Vert_n = 1} \Vert Ax \Vert_m$. 特别地, 当 $m = n = p$ 时:
&lt;ul&gt;
&lt;li&gt;$p=1$ 时, $\Vert A \Vert_{p=1} = \max_j \sum_i \vert A_{ij} \vert$;&lt;/li&gt;
&lt;li&gt;$p=2$ 时, $\Vert A \Vert_{p=2} = \sqrt{\lambda_{\max}(A^T A)}$, 亦称为 &lt;strong&gt;谱范数&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$p=\infty$ 时, $\Vert A \Vert_{p=\infty} = \max_i \sum_j \vert A_{ij} \vert$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核范数&lt;/strong&gt;: $\Vert A \Vert_\ast = \sum_i \sigma_i$, 其中 $\sigma_i$ 为 $A$ 的奇异值.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;Cauchy 不等式&lt;/span&gt;&lt;/p&gt;
$$\vert \left&lt; X, Y \right&gt; \vert \leq \Vert X \Vert \Vert Y \Vert$$&lt;p&gt;等号成立当且仅当 $X$ 与 $Y$ 线性相关.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;凸集&#34;&gt;凸集
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;如果对于任意 $x, y \in C$ 和 $\theta \in \mathbb{R}$, 都有 $\theta x + (1-\theta) y \in C$, 则称 $C$ 为 &lt;strong&gt;仿射集&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;如果对于任意 $x, y \in C$ 和 $\theta \in [0, 1]$, 都有 $\theta x + (1-\theta) y \in C$, 则称 $C$ 为 &lt;strong&gt;凸集&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;换言之, 仿射集要求过任意两点的直线都在集合内, 而凸集要求过任意两点的线段都在集合内. 显然, 仿射集都是凸集. 线性方程组的解集是一个仿射集, 而线性规划问题的可行域是一个凸集. 可以证明, 仿射集均可表示为某个线性方程组的解集.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 $S$ 是凸集, 则 $kS = \left\{ ks \mid k \in \mathbb{R}, s \in S \right\}$ 也是凸集;&lt;/li&gt;
&lt;li&gt;若 $S, T$ 是凸集, 则 $S + T = \left\{ s + t \mid s \in S, t \in T \right\}$ 也是凸集;&lt;/li&gt;
&lt;li&gt;若 $S, T$ 是凸集, 则 $S \cap T$ 也是凸集.&lt;/li&gt;
&lt;li&gt;凸集的内部和闭包均为凸集.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;可以证明, 任意多个凸集的交集仍为凸集.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;形如 $x=\theta_1x_1+\theta_2x_2+\cdots+\theta_kx_k$, 其中 $\theta_i \geq 0$ 且 $\sum_i \theta_i = 1$, 的表达式称为 $x$ 的 &lt;strong&gt;凸组合&lt;/strong&gt;. 集合 $S$ 的所有点的凸组合构成的集合称为 $S$ 的 &lt;strong&gt;凸包&lt;/strong&gt;, 记为 $\text{conv}(S)$.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;若 $\text{conv} S \subseteq S$, 则 $S$ 为凸集, 反之亦然.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;先证明正方向. 对任意 $x,y \in S, \theta \in [0,1]$, 有 $\theta x + (1-\theta) y \in \text{conv} S \subseteq S$, 故 $S$ 为凸集.&lt;/p&gt;
&lt;p&gt;再证明反方向, 对凸组合的维数 $k$ 采用数学归纳法证明之.&lt;/p&gt;
&lt;p&gt;若 $k=1$, 显然成立. 假设对于 $k-1$ 成立, 则对于 $k$, 考虑&lt;/p&gt;
$$
\begin{aligned}
x &amp;= \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_k x_k \\
  &amp;= (1-\theta_k)\left(\frac{\theta_1}{1-\theta_k} x_1 + \frac{\theta_2}{1-\theta_k} x_2 + \cdots + \frac{\theta_{k-1}}{1-\theta_k} x_{k-1}\right) + \theta_k x_k
\end{aligned}
$$&lt;p&gt;前面大括号内的表达式为 $k-1$ 个凸组合, 故在 $S$ 中. 于是 $x$ 又成为两个点的凸组合, 由于 $S$ 为凸集, 故 $x \in S$. 则 $\text{conv} S \subseteq S$.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{conv}S$ 是包含 $S$ 的最小凸集;&lt;/li&gt;
&lt;li&gt;$\text{conv}S$ 是所有包含 $S$ 的凸集的交集.&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;显然第一个是第二个的推论, 只证明第二个.&lt;/p&gt;
&lt;p&gt;已知凸集的交是凸集, 从而所有包含 $S$ 的凸集的交集 $X$ 是凸集. 且 $\text{conv} S$ 是包含 $S$ 的凸集, 则 $X \subseteq \text{conv} S$.&lt;/p&gt;
&lt;p&gt;另一方面, $S \subseteq X$, 则 $\text{conv} S \subseteq \text{conv}X$, 而 $X$ 是凸集, 则 $\text{conv}X = X$, 即 $\text{conv}S \subseteq X$. 综上, $\text{conv}S = X$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;仿照凸组合和凸包, 也可以定义仿射组合和仿射包 $\text{affine} S$, 不再赘述.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;形如 $x=\theta_1x_1+\theta_2x_2+\cdots+\theta_kx_k$, 其中 $\theta_i \geq 0$ 的表达式称为 $x$ 的 &lt;strong&gt;锥组合&lt;/strong&gt;. 若集合 $S$ 中任意点的锥组合都在 $S$ 中, 则称 $S$ 为凸锥.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;常见凸集&#34;&gt;常见凸集
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;任取非零向量 $a\in \mathbb{R}^n$, 形如&lt;/p&gt;
$$ \left\{ x \mid a^Tx =b \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;超平面&lt;/strong&gt;, 形如&lt;/p&gt;
$$ \left\{ x \mid a^Tx \le b \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;半空间&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;满足线性等式和不等式组的点的集合称为 &lt;strong&gt;多面体&lt;/strong&gt;, 即&lt;/p&gt;
$$ \left\{x \mid Ax \le b, Cx = d\right\} $$&lt;p&gt;其中 $A \in \mathbb{R}^{m \times n}, C \in \mathbb{R}^{p \times n}$.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对中心 $x_c$ 和半径 $r$, 形如&lt;/p&gt;
$$ B(x_c, r) = \left\{ x \mid \Vert x - x_c \Vert \le r \right\} = \left\{ x_c + ru \mid \Vert u \Vert \le 1 \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;球&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对中心 $x_c$ 和对称正定矩阵 $P$, 非奇异矩阵 $A$, 形如&lt;/p&gt;
$$ \left\{ x \mid (x-x_c)^TP(x-x_c) \le 1 \right\} = \left\{ x_c + Au \mid \Vert u \Vert \le 1 \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;椭球&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;形如&lt;/p&gt;
$$ \left\{(x,t) \mid \Vert x \Vert \le t \right\} $$&lt;p&gt;的集合称为 &lt;strong&gt;(范数)锥&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;保凸运算&#34;&gt;保凸运算
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;仿射运算保凸, 即对 $f(x)=Ax+b$, 则凸集在 $f$ 下的像是凸集, 凸集在 $f$ 下的原像是凸集.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;考虑双曲锥&lt;/p&gt;
$$
\left\{ x \mid x^TPx \le \left( c^Tx \right)^2, c^Tx \ge 0, P \in \mathbb{S}_+^n \right\}
$$&lt;p&gt;$\mathbb{S}_+^n$ 表示半正定矩阵. 双曲锥可以表示为二阶锥&lt;/p&gt;
$$
\left\{ x \mid \Vert Ax \Vert_2 \le c^Tx, c^Tx \ge 0, A^TA = P \right\}
$$&lt;p&gt;这个可以由二次范数锥得到.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;透视变换 $P: \mathbb{R}^{n+1} \mapsto \mathbb{R}^n$:&lt;/p&gt;
$$
  P(x,t) = \frac{x}{t}, \quad \text{dom} P = \left\{ (x,t) \mid t &gt; 0 \right\}
  $$&lt;p&gt;保凸.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分式线性变换 $f: \mathbb{R}^n \mapsto \mathbb{R}^m$:&lt;/p&gt;
$$
  f(x) = \frac{Ax+b}{c^Tx+d}, \quad \text{dom} f = \left\{ x \mid c^Tx+d &gt; 0 \right\}
  $$&lt;p&gt;保凸.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;广义不等式和对偶锥&#34;&gt;广义不等式和对偶锥
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;我们称一个凸锥 $K \subseteq \mathbb{R}^n$ 为 &lt;strong&gt;适当锥&lt;/strong&gt;, 当其还满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$K$ 是闭集;&lt;/li&gt;
&lt;li&gt;$K$ 是实心的, 即 $\text{int} K \neq \emptyset$;&lt;/li&gt;
&lt;li&gt;$K$ 是尖的, 即内部不包含直线: 若 $x \in \text{int} K, -x \in \text{int} K$. 则一定有 $x = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非负卦限 $K=\mathbb{R}_+^n=\left\{ x \in \mathbb{R}^n \mid x_i \ge 0 \right\}$ 是适当锥.&lt;/li&gt;
&lt;li&gt;半正定锥 $K=\mathbb{S}_+^n$ 是适当锥.&lt;/li&gt;
&lt;li&gt;$[0,1]$ 上的有限非负多项式 $K=\left\{ x \in \mathbb{R}^n \mid x_1 + x_2t + \cdots + x_nt^{n-1} \ge 0, t \in [0,1] \right\}$ 是适当锥.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以在 &lt;strong&gt;适当锥&lt;/strong&gt; 上定义广义不等式.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于适当锥 $K$ , 定义偏序 &lt;strong&gt;广义不等式&lt;/strong&gt; 为&lt;/p&gt;
$$x \preceq_K y \Leftrightarrow y - x \in K$$&lt;p&gt;严格版本:&lt;/p&gt;
$$x \prec_K y \Leftrightarrow y - x \in \text{int} K$$&lt;/div&gt;
&lt;p&gt;广义不等式是一个偏序关系, 具有自反性, 反对称性, 传递性, 可加性, 非负缩放性, 不再赘述.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;令锥 $K$ 为全空间 $\Omega$ 的子集, 则 $K$ 的 &lt;strong&gt;对偶锥&lt;/strong&gt; 为&lt;/p&gt;
$$
K^\ast = \left\{ y \mid \left&lt; x, y \right&gt; \ge 0, \forall x \in K \right\}
$$&lt;/div&gt;
&lt;p&gt;例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非负卦限是自对偶锥.&lt;/li&gt;
&lt;li&gt;半正定锥是自对偶锥.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $K$ 是一锥, $K^\ast$ 是其对偶锥, 则满足:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$K^\ast$ 是锥 (即使 $K$ 不是锥);&lt;/li&gt;
&lt;li&gt;$K^\ast$ 是凸且闭的;&lt;/li&gt;
&lt;li&gt;若 $\text{int} \neq \emptyset$, 则 $K^\ast$ 是尖的.&lt;/li&gt;
&lt;li&gt;若 $K$ 是尖的, 则 $\text{int} K^\ast \neq \emptyset$.&lt;/li&gt;
&lt;li&gt;若 $K$ 是适当锥, 则 $K^\ast$ 是适当锥.&lt;/li&gt;
&lt;li&gt;$K^{\ast\ast}$ 是 $K$ 的凸包. 特别地, 若 $K$ 是凸且闭的, 则 $K^\ast=K$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;适当锥的对偶锥仍是适当锥, 则适当锥 $K$ 的对偶锥 $K^\ast$ 也可以诱导广义不等式.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;对于适当锥 $K$, 定义其对偶锥 $K^\ast$ 上的 &lt;strong&gt;对偶广义不等式&lt;/strong&gt; 为:&lt;/p&gt;
$$x \preceq_{K^\ast} y \Leftrightarrow y - x \in K^\ast$$&lt;p&gt;其满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x \preceq_{K} y \Leftrightarrow \lambda^Tx \le \lambda^Ty, \forall \lambda \succeq_{K^\ast} K^\ast$.&lt;/li&gt;
&lt;li&gt;$y \succeq_{K^\ast} 0 \Leftrightarrow y^Tx \ge 0, \forall x \succeq_K 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id=&#34;分离超平面定理&#34;&gt;分离超平面定理
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;分离超平面定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 $C$ 和 $D$ 是不相交的凸集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 分开, 即存在 $a \neq 0, b$ 使得&lt;/p&gt;
$$
\begin{aligned}
a^Tx &amp;\le b, \quad \forall x \in C \\
a^Tx &amp;\ge b, \quad \forall x \in D
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;简要想法是找距离最近的一对点, 以这两点的中点为中心, 以两点的连线为法向量构造超平面.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;严格分离定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 $C$ 和 $D$ 是不相交的凸集, 且 $C$ 是闭集, $D$ 是紧集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 严格分开, 即存在 $a \neq 0, b$ 使得&lt;/p&gt;
$$
\begin{aligned}
a^Tx &amp;\lt b, \quad \forall x \in C \\
a^Tx &amp;\gt b, \quad \forall x \in D
\end{aligned}
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;给定集合 $C$ 和边界点 $x_0$, 如果 $a\ne 0$ 满足 $a^Tx \le a^T x_0, \forall x \in C$, 则称&lt;/p&gt;
$$
\left\{ x \mid a^Tx = a^T x_0 \right\}
$$&lt;p&gt;为 $C$ 的 &lt;strong&gt;支撑超平面&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;由分离超平面的特殊情况 ($D$ 为单点集) 可以得到支撑超平面的存在性.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;支撑超平面定理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若 $C$ 是凸集, 则 $C$ 的任意边界点处存在支撑超平面.&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>最优化方法(1) —— 简介</title>
        <link>https://LeoDreamer2004.github.io/p/opt-method/intro/</link>
        <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/opt-method/intro/</guid>
        <description>&lt;h2 id=&#34;概要&#34;&gt;概要
&lt;/h2&gt;&lt;p&gt;最优化问题的一般形式:&lt;/p&gt;
$$
\begin{aligned}
\min_{x} \quad &amp; f(x) \\
\text{s.t.} \quad &amp; g_i(x) \leq 0, \quad i = 1, 2, \ldots, m \\
&amp; h_j(x) = 0, \quad j = 1, 2, \ldots, p
\end{aligned}
$$&lt;h2 id=&#34;稀疏优化&#34;&gt;稀疏优化
&lt;/h2&gt;&lt;p&gt;考虑线性方程组 $Ax = b$, 优化函数 $\min_{x \in R^n} {\Vert x \Vert}_0, {\Vert x \Vert}_1, {\Vert x \Vert}_2$, 分别指代 $x$ 的非零元个数, $l_1, l_2$ 范数.
LASSO(least absolute shrinkage and selection
operator) 问题:&lt;/p&gt;
$$
\min_{x \in \mathbb{R}^n} \mu {\Vert x \Vert}_1 + \frac{1}{2} {\Vert Ax - b \Vert}_2^2
$$&lt;h2 id=&#34;低秩矩阵优化&#34;&gt;低秩矩阵优化
&lt;/h2&gt;&lt;p&gt;考虑矩阵 $M$, 希望 $X$ 在描述 $M$ 有效特征元素的同时, 尽可能保证 $X$ 的低秩性质. 低秩矩阵问题:&lt;/p&gt;
$$
\min_{X \in \mathbb{R}^{m \times n}} \text{rank}(X) \quad \text{s.t.} \quad X_{ij} = M_{ij}, \quad (i, j) \in \Omega
$$&lt;p&gt;核范数 ${\Vert X \Vert}_*$ 为所有奇异值的和. 也有二次罚函数的形式:&lt;/p&gt;
$$
\min_{X \in \mathbb{R}^{m \times n}} \mu {\Vert X \Vert}_* + \frac{1}{2} \sum_{(i,j)\in \Omega} (X_{ij} - M_{ij})^2
$$&lt;p&gt;对于低秩情形, $X=LR^T$, 其中 $L \in \mathbb{R}^{m \times r}, R \in \mathbb{R}^{n \times r}$, $r \ll m,n$ 为秩. 优化问题可写为:&lt;/p&gt;
$$
\min_{L,R} \alpha {\Vert L \Vert}^2_F + \beta {\Vert R \Vert}^2_F + \frac{1}{2} \sum_{(i,j)\in \Omega} ([LR^T]_{ij} - M_{ij})^2
$$&lt;p&gt;引入正则化系数 $\alpha, \beta$ 来消除 $L,R$ 在常数缩放下的不确定性.&lt;/p&gt;
&lt;h2 id=&#34;深度学习&#34;&gt;深度学习
&lt;/h2&gt;&lt;p&gt;机器学习的问题通常形如&lt;/p&gt;
$$
\min_{x \in W} \frac{1}{N} \sum_{i=1}^N \ell(f(a_i, x), b_i) + \lambda R(x)
$$&lt;hr&gt;
&lt;h2 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h2&gt;&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $f: \mathbb{R}^n \mapsto \mathbb{R}$, $x \in \mathbb{R}^n$ 的可行区域为 $S$. 若存在一个邻域 $N(x)$, 使得 $\forall x \in N(x) \cap S$, 有 $f(x^\ast) \leq f(x)$, 则称 $x^\ast$ 为 $f$ 的&lt;strong&gt;局部极小点&lt;/strong&gt;. 若 $\forall x \in S$, 有 $f(x^\ast) \leq f(x)$, 则称 $x^\ast$ 为 $f$ 的&lt;strong&gt;全局极小点&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;大多数的问题是不能显式求解的, 通常要使用迭代算法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;称算法是 &lt;strong&gt;Q-线性收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert}} \le a, \quad a \in (0, 1)
$$&lt;p&gt;称算法是 &lt;strong&gt;Q-超线性收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\lim_{k \to \infty} \frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert}} = 0
$$&lt;p&gt;称算法是 &lt;strong&gt;Q-次线性收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\lim_{k \to \infty} \frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert}} = 1
$$&lt;p&gt;称算法是 &lt;strong&gt;Q-二次收敛&lt;/strong&gt; 的, 若对充分大的 $k$ 有&lt;/p&gt;
$$
\frac{{\Vert x_{k+1} - x^\ast \Vert}}{{\Vert x_k - x^\ast \Vert^2}} \le a, \quad a &gt; 0
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;设 $x_k$ 是迭代算法产生的序列且收敛到 $x^\ast$, 如果存在 Q-线性收敛于 $0$ 的非负序列 $t_k$, 且&lt;/p&gt;
$$
\Vert x_k - x^\ast \Vert \le t_k
$$&lt;p&gt;则称 $x_k$ 是 &lt;strong&gt;R-线性收敛&lt;/strong&gt; 的.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;一般来说, 收敛准则可以是&lt;/p&gt;
$$
\frac{f(x_k) - f^\ast}{\max\left\{\left|f^\ast \right|, 1\right\}} \le \varepsilon
$$&lt;p&gt;也可以是&lt;/p&gt;
$$
\nabla f(x_k) \le \varepsilon
$$&lt;p&gt;如果有约束要求, 还要同时考虑到约束违反度. 对于实际的计算机算法, 会设计适当的停机准则, 例如&lt;/p&gt;
$$
\frac{{\Vert x_{k+1} - x_k \Vert}}{\max\left\{\Vert x_k \Vert, 1\right\}} \le \varepsilon
$$</description>
        </item>
        
    </channel>
</rss>
