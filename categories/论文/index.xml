<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>论文 on LeoDreamer</title>
        <link>https://LeoDreamer2004.github.io/categories/%E8%AE%BA%E6%96%87/</link>
        <description>Recent content in 论文 on LeoDreamer</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>LeoDreamer</copyright>
        <lastBuildDate>Thu, 29 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://LeoDreamer2004.github.io/categories/%E8%AE%BA%E6%96%87/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>论文阅读 - 测试时强化学习</title>
        <link>https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/</link>
        <pubDate>Thu, 29 May 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/</guid>
        <description>&lt;h2 id=&#34;测试时强化学习&#34;&gt;测试时强化学习
&lt;/h2&gt;&lt;h3 id=&#34;tta&#34;&gt;TTA
&lt;/h3&gt;&lt;p&gt;通常情况下, 深度学习模型在训练完成后就固定了参数, 在测试或部署阶段不再更新. 但在实际应用中, 测试数据可能与训练数据的分布存在差异, 导致模型性能下降. 因此后续的微调显得非常重要.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试时适应 (Test-time Adaption, TTA)&lt;/strong&gt; 算法指在不使用真实标签的前提下, 利用当前测试样本或其增强版本来在线微调模型, 使其更适应当前的输入分布.&lt;/p&gt;
&lt;p&gt;常见的测试时适应算法包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应批归一化&lt;/strong&gt;: 在测试阶段对批归一化层的均值和方差进行调整, 使其更适应当前输入分布, 同时不修改学习参数 &lt;code&gt;gamma&lt;/code&gt; 和 &lt;code&gt;beta&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;熵最小化&lt;/strong&gt;: 在测试阶段通过最小化模型输出的熵来提高模型的自信度.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;过往的 TTA 算法一般基于无监督学习, 即便是强化学习算法, 需要辛苦设计奖励函数, RLHF 需要人工标注数据, 成本高昂.&lt;/p&gt;
&lt;h3 id=&#34;ttrl&#34;&gt;TTRL
&lt;/h3&gt;&lt;p&gt;论文 &lt;a class=&#34;link cite-1&#34;&gt;[1]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 开创性地提出了 &lt;strong&gt;测试时强化学习 (Test-Time Reinforcement Learning, TTRL)&lt;/strong&gt; 算法 (后面几篇论文都是在此基础上进行改进). TTRL 通过强化学习的方式, 在测试时对模型进行微调, 使其更好地适应当前输入分布.&lt;/p&gt;
&lt;p&gt;在 &lt;strong&gt;无监督&lt;/strong&gt; 的情况下, 怎么设置奖励函数? 论文的策略非常简单: 多数投票.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;TTRL&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 一个模型 $f_{\theta}$, 测试样本 $x$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 微调后的模型 $f_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对输入 $x$ 做多次预测, 得到预测结果 $y_i$.&lt;/li&gt;
&lt;li&gt;统计每个预测结果的出现次数, 设最常见的预测结果为 $y^*$, 称为一致动作.&lt;/li&gt;
&lt;li&gt;计算奖励函数 $R(y_i)$:
$$
    R(y_i) = \mathbb{I}(y_i = y^*)
    $$&lt;/li&gt;
&lt;li&gt;通过梯度上升更新模型参数 $\theta$ 为 $\theta&#39;$:
$$
    \theta&#39; = \theta + \eta \nabla_{\theta} \mathbb{E}_{y_i \sim f_{\theta}(x)}[R(y_i)]
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/TTRL.png&#34;
	width=&#34;1923&#34;
	height=&#34;762&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/TTRL_hu_f4f946abf4795f84.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/TTRL_hu_3334de21ebd2a53.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;TTRL&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;效果非常显著, 甚至可以与带有有一定数据泄漏的监督方案相媲美.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/TTRL-result.png&#34;
	width=&#34;1048&#34;
	height=&#34;347&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/TTRL-result_hu_a86b3d16c865f70.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/TTRL-result_hu_5e6e1e57de726b73.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;TTRL 结果&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;为什么能做这么好? 论文 &lt;a class=&#34;link cite-1&#34;&gt;[1]
    
&lt;/a&gt; 给出了三个原因:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签估计&lt;/strong&gt;: TTRL 引入标签估计, 尽管存在不确定性, RL 仍具有一定鲁棒性, 且通常比 SFT 具有更好的泛化能力.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励函数设计&lt;/strong&gt;: &amp;ldquo;幸运命中&amp;rdquo; (lucky hit) 现象, 即便预测不准确, 只要估计标签与预测答案不同, 验证器就能分配正确的 $0$ 奖励. 实验表明, 尽管多数投票的标签估计可能不准确, 但奖励函数的估计却非常准确. 原因是模型输出概率非常分散, 因此即使标签未被准确估计, 由于 &amp;ldquo;幸运命中&amp;rdquo;, 大多数输出仍然可以收到正确的奖励.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在线学习&lt;/strong&gt;: TTRL 是在线学习算法, 可以在测试时不断更新模型参数, 使其更好地适应当前输入分布.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;基于熵最小化的强化学习&#34;&gt;基于熵最小化的强化学习
&lt;/h2&gt;&lt;p&gt;论文 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 给出基于最小化熵的强化学习算法 (Reinforcement Learning via Entropy Minimization, RENT). 基于 GRPO 框架测试, 把奖励函数设置为负熵, 认为只通过最小化输出的熵, 即可提高模型推理能力.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22660/x1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RENT&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;h2 id=&#34;内部反馈的强化学习&#34;&gt;内部反馈的强化学习
&lt;/h2&gt;&lt;p&gt;除了 KL 正则化等等项之外, 我们关心奖励函数的设计. 这个奖励要与任务无关, 而由模型内部的反馈来决定. 与 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt; 提出的负熵奖励不同, 论文 &lt;a class=&#34;link cite-3&#34;&gt;[3]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 给出了另一个置信度函数:&lt;/p&gt;
$$
S(o) = \frac{1}{|o|}\sum_{i=1}^{|o|}KL(U \| p_{\pi_{\theta}}(\cdot|o_{\lt i})) = -\frac{1}{|o| \cdot |V|} \sum_{i=1}^{|o|}\sum_{j=1}^{|V|} \log \left( |V| \cdot p_{\pi_{\theta}} (j|o_{\lt i}) \right)
$$&lt;p&gt;其中 $o$ 是 token 序列, $U$ 表示均匀分布.&lt;/p&gt;
&lt;h2 id=&#34;带有-clip-反馈的强化学习&#34;&gt;带有 CLIP 反馈的强化学习
&lt;/h2&gt;&lt;p&gt;对于一般任务, 传统的测试时适应算法要最小化熵, 但很显然这个方式容易陷入错误的模型预测中. 与带有反馈的学习模型相比, 监督微调模型有更好的泛化能力.&lt;/p&gt;
&lt;h3 id=&#34;clip&#34;&gt;CLIP
&lt;/h3&gt;&lt;p&gt;文章 &lt;a class=&#34;link cite-4&#34;&gt;[4]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 通过引入 CLIP 反馈来解决置信度过高问题, 称为 RLCF(如下图).&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2305.18010/x1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;TPT 与 RLCF 对比&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;除了分类任务外, 通过特定任务的采样策略和适当的选择奖励基线, RLCF 可以很容易地扩展到不仅仅是检索这样的区分任务, 还可以扩展到图像字幕这样的泛化任务.&lt;/p&gt;
&lt;p&gt;我们现在关心视觉语言模型 (VLM), 因此要衡量跨模态的相似性. &lt;strong&gt;对比语言-图像预训练 (Contrastive Language-Image Pre-training, CLIP)&lt;/strong&gt; 模型通过对图像和文本进行编码, 使得它们在同一个共享的向量空间中具有相似的表示.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;CLIP&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 图像 $v$ 和文本 $t$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 图像和文本的相似度分数 $s(v,t)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CLIP 训练两个编码器: 图像编码器 $g$ 和文本编码器 $h$.&lt;/li&gt;
&lt;li&gt;二者的输出分别为 $g(v)$ 和 $h(t)$.&lt;/li&gt;
&lt;li&gt;计算相似度分数, 常用的是余弦相似度:
$$s(v,t) = \frac{g(v) \cdot h(t)}{\|g(v)\| \|h(t)\|}$$&lt;/li&gt;
&lt;li&gt;返回相似度分数 $s(v,t)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h3 id=&#34;rlcf-算法&#34;&gt;RLCF 算法
&lt;/h3&gt;&lt;p&gt;对于 VLM, 训练集 $\mathcal{D}_\text{train}$ 和测试集 $\mathcal{D}_\text{test}$ 都是图像和文本对 $(v,t)$ 的集合. 需要注意, 算法的微调是在 &lt;strong&gt;单个&lt;/strong&gt; 测试样本上进行的.&lt;/p&gt;
&lt;p&gt;对于奖励函数 $R$, 我们希望学习到最好的概率分布 $f_{\theta}(v) = [p(t|v,\theta)]_{t \in T}$ 使得其能最大化奖励:&lt;/p&gt;
$$\max_{\theta} \mathbb{E}_{t \sim f_{\theta}(v)}R(t,v)$$&lt;p&gt;我们正式引入 &lt;strong&gt;带有 CLIP 反馈的强化学习 (Reinforcement Learning with CLIP Feedback, RLCF)&lt;/strong&gt; 算法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;RLCF (分类任务)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 一个已经训练好的 VLM 模型 $f_{\theta}$, 测试样本 $v$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 微调后的模型 $f_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对测试样本 $v$ 进行数据增强, 生成多个增强样本 $\tau_i(v)$.&lt;/li&gt;
&lt;li&gt;按照 CLIP 的编码器编码 $v$ 和 $\tau_i(v)$, 计算当前模型的预测 $P(t|v,\theta)$. 注意此时训练文本应当是类似于 prompt + label 的形式, 如 &amp;ldquo;a photo of a cat&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;做置信度筛选, 只保留预测熵足够低的样本 $\tau_i(v)$. 在这些样本中, 按照 top-K 策略选择预测结果, 得到 K 对文本和图像 $(\tau_i(v), t_j)_{j=1}^K$. 暂记为 $(v,t)$ 以进行后续计算.&lt;/li&gt;
&lt;li&gt;按照先前的工作, 根据 CLIP 模型计算 CLIPScore:
$$
    \text{CLIP-S}(t,v) = w \times \max(\text{CLIP}(t,v), 0)
    $$
其中 $w=2.5$ 是一个常数.&lt;/li&gt;
&lt;li&gt;由于 CLIPScore 永远是非负的, 加入一个奖励基线增加稳定性:
$$
    R(t,v) = \text{CLIP-S}(t,v) - \mathbb{E}_{t&#39; \sim f_{\theta}(v)}[\text{CLIP-S}(t&#39;,v)]
    $$&lt;/li&gt;
&lt;li&gt;通过 REINFORCE 策略梯度更新模型参数 $\theta$ 为 $\theta&#39;$, 使得模型能够最大化奖励, 注意此时 &lt;strong&gt;只&lt;/strong&gt; 更新图像编码器 $g$ 的参数:
$$
    \nabla_{\theta} \mathbb{E}_{t \sim f_{\theta}(v)}[R(t,v)] = \mathbb{E}_{t \sim f_{\theta}(v)}[R(t,v) \nabla_{\theta} \log f_{\theta}(t|v, \theta)]
    $$&lt;/li&gt;
&lt;li&gt;返回微调后的模型 $f_{\theta&#39;}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2305.18010/x3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RLCF 分类任务&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;相较于监督学习, 基于反馈的强化学习更加通用, 例如可以进行图像描述的任务.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;RLCF (图文转换)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;基本可以从上面的 RLCF 算法中直接泛化修改. 只需要注意如果是文本生成图片时, 应该固定图像编码编码器 $g$ 而微调文本编码器 $h$, 且此时不做数据增强.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2305.18010/x4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RLCF 图文转换&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;技巧和变体&#34;&gt;技巧和变体
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;使用多个奖励模型及权重&lt;/strong&gt;: 默认情况下, 使用单个 CLIP-ViT-L/14. 可以使用多个 CLIP 模型, 并对它们的输出进行加权平均, 以获得更好的奖励信号.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;片段式测试时适应 (Episodic TTA)&lt;/strong&gt;: 假定模型泛化能力很强, 测试时只在测试集上微调, 随后丢弃重置为原参数 $\theta^*$, 防止污染大模型.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动量缓冲 (Momentum Buffer)&lt;/strong&gt;: 尽管片段式测试时适应确保可靠性, 但影响了模型增量学习能力. 因此引入一个动量缓冲, 在每次 TTA 中, 按照移动平均的方式更新缓冲 $\xi \leftarrow m\xi + (1-m)\theta$, 每经过若干次样本后, 再将缓冲 $\eta$ 作为新的参数 $\theta$ 进行更新.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验&#34;&gt;实验
&lt;/h3&gt;&lt;p&gt;RLCF 方法可以通用地建立在常用的架构上. 在零样本分类任务, 零样本图文检索和图像描述任务上, RLCF 都能显著提升模型的性能.&lt;/p&gt;
&lt;h2 id=&#34;引入协方差正则化的强化学习&#34;&gt;引入协方差正则化的强化学习
&lt;/h2&gt;&lt;p&gt;与论文 &lt;a class=&#34;link cite-4&#34;&gt;[4]
    
&lt;/a&gt; 不同, 论文 &lt;a class=&#34;link cite-5&#34;&gt;[5]
    
&lt;/a&gt; 通过熵动力学来研究熵崩溃的问题, 最终的目的依然是控制熵.&lt;/p&gt;
&lt;h3 id=&#34;熵崩溃&#34;&gt;熵崩溃
&lt;/h3&gt;&lt;p&gt;强化学习过程中对于高置信度的策略会愈发增强其使用概率, 导致熵变得更加降低. 以下图揭示了熵崩溃和性能饱和的关系. 当熵下降到某个阈值时, 性能会达到饱和点.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;熵崩溃和性能饱和&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;论文定量分析认为, 如果没有像熵损失或者 KL 散度这样的正则化, 下游性能完全可以通过策略熵来预测, 精确来说可以拟合成指数函数:&lt;/p&gt;
$$
R = -a \exp(\mathcal{H}) + b
$$&lt;p&gt;$R$ 是验证集的性能, $\mathcal{H}$ 是策略的熵.&lt;/p&gt;
&lt;h3 id=&#34;熵-性能函数&#34;&gt;熵-性能函数
&lt;/h3&gt;&lt;p&gt;这个函数可以用来分析模型的性能和熵之间的关系, 有几个特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;系数与算法无关&lt;/strong&gt;: 下面这个图几个算法得到的曲线是类似的, 这表明 $a,b$  可能是模型和数据的固有属性.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;熵-性能函数&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预测不同模型的函数系数&lt;/strong&gt;: 显然 $a$ 是模型将熵转化为下游性能的速度. $−a+b$ 是当熵归零时模型可以达到的最大验证性能. 理论上个更大的性能应该对应更大的 $a$ 和 $b$. 此外不同的任务也会有不同的系数
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;系数 $a$&lt;/th&gt;
          &lt;th&gt;系数 $b$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;数学任务&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x12.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;a-math&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x13.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;b-math&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;代码任务&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x14.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;a-code&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x15.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;b-code&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结, 在策略熵减少过程中, 性能天花板不仅存在, 而且可以被预测.&lt;/p&gt;
&lt;h3 id=&#34;熵动力学&#34;&gt;熵动力学
&lt;/h3&gt;&lt;p&gt;我们主要关注相邻两次迭代的熵变化 $\mathcal{H}(\pi_{\theta}^{k+1}) - \mathcal{H}(\pi_{\theta}^{k})$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;策略梯度下的熵变化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令行为策略 $\pi_{\theta}$ 为一个 softmax 策略, 并通过标准策略梯度更新, 两个连续步骤中给定状态 $s$ 的策略熵之差满足:&lt;/p&gt;
$$
\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx -\eta \text{Cov}_{a \sim \pi_{\theta}^{k}(\cdot|s)} \left( \log \pi_{\theta}^{k}(a|s), \pi_{\theta}^k(a|s) \cdot A(s,a) \right)
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;自然策略梯度下的熵变化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令行为策略 $\pi_{\theta}$ 为一个 softmax 策略, 并通过标准策略梯度更新, 两个连续步骤中给定状态 $s$ 的策略熵之差满足:&lt;/p&gt;
$$
\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx -\eta \text{Cov}_{a \sim \pi_{\theta}^{k}(\cdot|s)} \left( \log \pi_{\theta}^{k}(a|s), A(s,a) \right)
$$&lt;/div&gt;
&lt;p&gt;揭示了当前策略下的动作概率 $P(a)$ 与相应的优势函数 $A(a)$ 之间的强正相关性. 作者做了实验验证了这个定理估计的正确性.&lt;/p&gt;
&lt;h3 id=&#34;协方差正则化&#34;&gt;协方差正则化
&lt;/h3&gt;&lt;p&gt;论文认为直接采用传统强化学习中的熵正则化技术难以解决 LLMs 的熵瓶颈问题, 过高的熵正则化甚至会导致熵爆炸.&lt;/p&gt;
&lt;p&gt;实验表明, 小部分 token 的协方差极高, 在触发熵崩溃中占据了主导地位. 受到 PPO 策略的启发, 论文提出两种协方差感知方法: &lt;strong&gt;Clip-Cov&lt;/strong&gt; 和 &lt;strong&gt;KL-Cov&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对于 token $y_i$ 的协方差, 定义为:&lt;/p&gt;
$$
\text{Cov}(y_i) = \left( \log \pi_{\theta}(y_i) - \mathbb{E}_{i \in [N]}\left[ \log \pi_{\theta}(y_i) \right] \right) \left(A(y_i) - \mathbb{E}_{i \in [N]}\left[A(y_i)\right]\right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Clip-Cov&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 策略 $\pi_{\theta}$, 协方差阈值 $\omega_l, \omega_h$ (两个都远超均值), 剔除比例 $r$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 更新后的策略 $\pi_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算每个 token 的协方差 $\text{Cov}(y_i)$.&lt;/li&gt;
&lt;li&gt;从 $y_i$ 中随机选取 $r \cdot N$ 个满足 $\omega_l \le \text{Cov}(y_i) \le \omega_h $ 的 token, 设索引集为 $I_{\text{clip}}$.&lt;/li&gt;
&lt;li&gt;将选择的这些 token 从策略梯度中移除, 其余仍然正常更新:
$$
    L_{\text{clip}}(\theta) = \begin{cases}
    \mathbb{E}\left[ \frac{\pi_{\theta&#39;}(y_i)}{\pi_{\theta}(y_i)} A(y_i) \right] &amp; \text{if } i \notin I_{\text{clip}} \\
    0 &amp; \text{if } i \in I_{\text{clip}}
    \end{cases}
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;KL-Cov&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 策略 $\pi_{\theta}$, 剔除比例 $k\ll 1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 更新后的策略 $\pi_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算每个 token 的协方差 $\text{Cov}(y_i)$.&lt;/li&gt;
&lt;li&gt;从 $y_i$ 选取方差最大的 $k \cdot N$ 个 token, 设索引集为 $I_{\text{KL}}$.&lt;/li&gt;
&lt;li&gt;将选择的这些 token 在策略梯度中施加 KL 惩罚:
$$
    L_{\text{KL}}(\theta) = \begin{cases}
    \mathbb{E}\left[ \frac{\pi_{\theta&#39;}(y_i)}{\pi_{\theta}(y_i)} A(y_i) \right] &amp; \text{if } i \notin I_{\text{KL}} \\
    \mathbb{E}\left[ \frac{\pi_{\theta&#39;}(y_i)}{\pi_{\theta}(y_i)} A(y_i) \right] - \beta KL(\pi_{\theta}(y_i) || \pi_{\theta&#39;}(y_i)) &amp; \text{if } i \in I_{\text{KL}}
    \end{cases}
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h3 id=&#34;实验-1&#34;&gt;实验
&lt;/h3&gt;&lt;p&gt;与一般的熵正则化方法相比, 协方差正则化方法在多个任务上都能显著提升模型性能. 且能一定程度上避免瓶颈问题.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;策略熵&lt;/th&gt;
          &lt;th&gt;LLM 响应长度&lt;/th&gt;
          &lt;th&gt;准确率&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Qwen-7B&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x22.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;7B-entropy&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x23.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;7B-response&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x24.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;7B-accuracy&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Qwen-32B&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x25.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;32B-entropy&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x26.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;32B-response&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
          &lt;td&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.22617/x27.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;32B-accuracy&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;测试时样本特定语言模型优化&#34;&gt;测试时样本特定语言模型优化
&lt;/h2&gt;&lt;p&gt;论文 &lt;a class=&#34;link cite-6&#34;&gt;[6]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 提出了 &lt;strong&gt;测试时样本特定语言模型优化 (Sample-specific Language Model Optimization at Test-time, SLOT)&lt;/strong&gt; 算法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;SLOT&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 预训练语言模型 $f_{\theta}$, 输入 token 序列 $x=(x_1, x_2, \ldots, x_n)$, 优化步数 $T$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 拓展生成的文本 $x$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化样本特定参数 $\delta=\mathbf{0}\in \mathbb{R}^{1 \times d}$.&lt;/li&gt;
&lt;li&gt;计算最后一层的隐藏特征 $H = f_{\text{pre}}(x) \in \mathbb{R}^{n \times d}$.&lt;/li&gt;
&lt;li&gt;修改 $H&#39; = H + \delta$, 这里是广播加法.&lt;/li&gt;
&lt;li&gt;计算 logits $L = W_{\text{LM}} H&#39; \in \mathbb{R}^{n \times |V|}$ 和其对应的交叉熵损失 $\mathcal{L}$, 并根据损失 $\mathcal{L}$ 优化 $\delta$.&lt;/li&gt;
&lt;li&gt;重复步骤 2-4, 直到达到优化步数 $T$, 最后得到 $\delta_{\text{opt}}$.&lt;/li&gt;
&lt;li&gt;计算最后一个 token 的隐藏特征 $H_{\text{last}} = f_{\text{pre}}(x) [-1] \in \mathbb{R}^{1 \times d}$.&lt;/li&gt;
&lt;li&gt;修改 $H_{\text{last}}&#39; = H_{\text{last}} + \delta_{\text{opt}}$.&lt;/li&gt;
&lt;li&gt;计算下一个 token 的 logits $L_{\text{next}} = W_{\text{LM}} H_{\text{last}}&#39;$, 随后按 softmax 选择下一个 token $x_{\text{next}}$.&lt;/li&gt;
&lt;li&gt;把 $x_{\text{next}}$ 添加到输入序列 $x$ 中, 并重复步骤 6-8, 直到生成满足条件的文本.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.12392/x2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;SLOT&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;特意把参数 $\delta$ 放在预测头之前, 是为了减小计算量. 称这个增量为 &lt;strong&gt;概率向量调制向量 (Logit Modulation Vector, LMV)&lt;/strong&gt;:&lt;/p&gt;
$$
\text{LMV} = W_{\text{LM}}\delta \in \mathbb{R}^{|V|}
$$&lt;p&gt;测试表明, 与推理过程相关的词如 &amp;ldquo;think&amp;rdquo; 和 &amp;ldquo;reasoning&amp;rdquo; 在 LMV 的作用下得到了显著增强.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://arxiv.org/html/2505.12392/x3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;LMV 调节&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;我的理解是和直接插入一层网络的区别是, 这个反向传播只更新 $\delta$ 而不更新模型参数, 且是一次性的, 只在测试时进行微调. 这只是一个测试时微调, 似乎不是强化学习.&lt;/p&gt;
&lt;h2 id=&#34;虚假奖励也能训练&#34;&gt;虚假奖励也能训练?!
&lt;/h2&gt;&lt;p&gt;说了这么多, 其实都是在说如何设计奖励函数. 但是, 论文 &lt;a class=&#34;link cite-7&#34;&gt;[7]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 提出了一个非常反直觉的问题: 即使在使用与正确答案几乎没有或甚至负相关关系的虚假奖励下训练, RLVR 仍能在某些模型中激发强烈的数学推理能力!&lt;/p&gt;
&lt;p&gt;论文给出了五种奖励函数:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实标签 (Ground Truth) 奖励: 直接用真实标签作为奖励函数, 这标定了 RLVR 的上限.&lt;/li&gt;
&lt;li&gt;多数投票 (Majority Vote) 奖励: 通过多数投票的方式估计标签 (标签很可能是错误的), 以此作为奖励函数.&lt;/li&gt;
&lt;li&gt;格式化 (Format) 奖励: 当模型输出最后包含 &lt;code&gt;\box{}&lt;/code&gt; 时, 给予奖励, 否则不奖励. 这个奖励函数与正确答案无关.&lt;/li&gt;
&lt;li&gt;随机 (Random) 奖励: 随机生成奖励.&lt;/li&gt;
&lt;li&gt;错误 (Incorrect) 奖励: 只对错误的答案给予奖励, 正确答案不奖励.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/spurious-reward.png&#34;
	width=&#34;1208&#34;
	height=&#34;1042&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/spurious-reward_hu_87f3118ec7eac58a.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/spurious-reward_hu_797e55031f22f02e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;虚假奖励&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;论文围绕了一个小问题展开: 问大模型 $(2,-6)$ 和 $(-4,3)$ 的距离是多少?&lt;/p&gt;
&lt;h3 id=&#34;不同模型在推理策略上存在先存差异&#34;&gt;不同模型在推理策略上存在先存差异
&lt;/h3&gt;&lt;p&gt;有些强模型会尝试写 Python 代码来计算距离, 尽管实际上它们并没有代码运行环境. 这种行为称为代码推理 (Code Reasoning), 且实验表明代码推理与准确率呈现强正相关性. 有些弱模型不生成代码, 或者对于代码生成性能弱.&lt;/p&gt;
&lt;h3 id=&#34;rlvr-在引入虚假奖励时可以增强预存的推理策略&#34;&gt;RLVR 在引入虚假奖励时可以增强预存的推理策略
&lt;/h3&gt;&lt;p&gt;在进行 RLVR 训练后, 代码推理的频率迅速增加, 与准确度提升高度相关; 随机奖励则相对缓慢, 但最终也达到了相似的水平. 此后随着模型自然语言推理准确度的提高, 这一频率逐渐下降, 这表明模型正在从高质量的真实标签奖励中学习真正的知识.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/code-reasoning.png&#34;
	width=&#34;1827&#34;
	height=&#34;832&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/code-reasoning_hu_71a4530fb9d54bca.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/code-reasoning_hu_8b8cf624e5e6a2bd.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;代码推理&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;h3 id=&#34;推理策略切换对性能的细化影响&#34;&gt;推理策略切换对性能的细化影响
&lt;/h3&gt;&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/code-policy-switch.png&#34;
	width=&#34;2244&#34;
	height=&#34;480&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/code-policy-switch_hu_2c99c1110751da97.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/code-policy-switch_hu_eddac65eea7e766c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;推理策略切换&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;对于所有较弱和虚假的奖励, 模型在 RLVR 后更倾向于使用代码推理. 虚假奖励上的准确度提升主要是通过激发模型使用正确的推理策略实现的.&lt;/p&gt;
&lt;h3 id=&#34;随机奖励与策略裁剪&#34;&gt;随机奖励与策略裁剪
&lt;/h3&gt;&lt;p&gt;关于随机奖励的问题, 论文证明了尽管优势期望值为零, 但由于损失函数中的 clip 机制, GRPO 损失的期望梯度并非为零.&lt;/p&gt;
&lt;p&gt;为了验证这个想法, 论文进行了一组对比, 同样使用随机奖励, 区别是是否进行 $(1-\epsilon, 1+\epsilon)$ 重要性采样比裁剪. 结果表明, 在没有裁剪的情况下, 随机奖励不能给模型带来任何提升.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/random-reward-clip.png&#34;
	width=&#34;1820&#34;
	height=&#34;764&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/random-reward-clip_hu_694331c2a1779c35.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/random-reward-clip_hu_27d878a2a934c9ec.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;随机奖励与策略裁剪&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;因此综合来看, 论文推测, 在随机奖励训练中, 看似 &amp;ldquo;训练信号&amp;rdquo; 实际上是优化算法偏向利用预训练中学习到的先验知识的结果.&lt;/p&gt;
&lt;h2 id=&#34;sherlock-自我纠正推理&#34;&gt;Sherlock: 自我纠正推理
&lt;/h2&gt;&lt;p&gt;论文 &lt;a class=&#34;link cite-8&#34;&gt;[8]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 指出, 使用 SFT 或 RL 训练的模型缺乏逐步和响应层面自我纠正的能力. 一旦出现错误, 模型难以修正其推理, 往往无法从错误中恢复.&lt;/p&gt;
&lt;h3 id=&#34;自我纠正&#34;&gt;自我纠正
&lt;/h3&gt;&lt;p&gt;对于推理模型, 自我纠正行为可以有两种实现方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;逐步骤纠正&lt;/strong&gt;: 模型在其单次思考过程中反思其之前的第 i 步错误, 并对其进行修正:&lt;/p&gt;
$$
  (r, y_{i+1}, \cdots, y_n; a) \sim \pi(\cdot| x_{I \&amp; T}; y_1, \cdots, y_i^*)
  $$&lt;p&gt;其中 $y_i$ 代表第 $i$ 步推理, $a$ 是最终答案, $r$ 是模型的反思提示词 (如 &amp;ldquo;但是&amp;rdquo;, &amp;ldquo;等等&amp;rdquo;), $x_{I \&amp; T}$ 是输入的图像和文本, $y_i^*$ 是错误的推理.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;逐响应纠正&lt;/strong&gt;: 模型尝试纠正其之前的错误响应:&lt;/p&gt;
$$
  (y_1^2, \cdots, y_n^2; a) \sim \pi(\cdot| x_{I \&amp; T}; y_1^1, \cdots, y_n^1; t)
  $$&lt;p&gt;其中 $y^j, a^j$ 是模型的第 $j$ 次尝试响应.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sherlock&#34;&gt;Sherlock
&lt;/h3&gt;&lt;p&gt;为解决这一局限, 论文 &lt;a class=&#34;link cite-8&#34;&gt;[8]
    
&lt;/a&gt; 引入所谓 &lt;strong&gt;Sherlock&lt;/strong&gt; 算法来教导模型自我纠正, 从而增强其推理能力.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Sherlock&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I: &lt;strong&gt;SFT 冷启动&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从已知数据集中随机采样样本, 形成训练集 $\mathcal{D}_A$; 再次采样形成 $\mathcal{D}_B$, 这些样本包含高质量的 COT.&lt;/li&gt;
&lt;li&gt;在 $\mathcal{D}_A$ 上使用普通监督微调 (SFT) 训练基础 VLM，得到模型 $R0_{\text{VLM}}$。&lt;/li&gt;
&lt;li&gt;对于每个样本 $(x_{I\&amp;T}, Y^w)$ 在 $\mathcal{D}_B$ 中, 保留原本标签 $Y^w$, 同时用 $R0_{\text{VLM}}$ 生成一个推理轨迹 $Y^l$, 组合成新数据集 $\mathcal{D}_{\text{Sherlock}} = (x_{I\&amp;T}, Y^w, Y^l)$。&lt;/li&gt;
&lt;li&gt;使用如下公式中的损失函数, 联合 &lt;strong&gt;直接生成 (Direct Generation)&lt;/strong&gt; 和 &lt;strong&gt;自我纠正 (Self-Correction)&lt;/strong&gt; 两个任务.:&lt;/li&gt;
&lt;/ol&gt;
$$
  \mathcal{L}_{\text{Sherlock-SFT}}(\pi) = -\mathbb{E}_{(x_{I\&amp;T}, Y^w, Y^l) \sim \mathcal{D}_{\text{Sherlock}}} \left[ \log \pi(Y^w | x_{I\&amp;T}) + \log \pi(Y^l | x_{I\&amp;T}, Y^l, t) \right]
  $$&lt;p&gt;II. &lt;strong&gt;离线偏好训练&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;现在对于初始轨迹 $Y^1 = (y_1^1, \cdots, y_n^1;a^1)$, 我们假定此时已经有一部分推理正确, 需要在生成一个更好的轨迹 $Y^2 = (y_1^2, \cdots, y_n^2;a^2)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;随机在 $1 \sim n$ 中采样一个整数 $i$, 此时我们假定 $Y^1_{\lt i}$ 是正确的, 希望生成更好的 $Y^2_{\ge i}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;按照如下公式:&lt;/p&gt;
$$
    \max_{\pi}\mathbb{E}_{Y_{\geq i}^{2}\sim\pi(\cdot|[x_{I\&amp;T},Y^{1},t;Y_{\lt i}^{2}])}\left[p(Y_{\geq i}^{2}\succ Y_{\geq i}^{1}|x_{I\&amp;T};Y_{\lt i}^{2})-\beta D_{\text{KL}}(\pi\|\pi_{\text{ref}}|[x_{I\&amp;T},Y^{1},t;Y_{\lt i}^{2}])\right]\\+\mathbb{E}_{Y_{\geq i}^{2}\sim\pi(\cdot|[x_{I\&amp;T},Y^{1},t;Y_{\lt i}^{1}])}\left[p(Y_{\geq i}^{2}\succ Y_{\geq i}^{1}|x_{I\&amp;T};Y_{\lt i}^{1})-\beta D_{\text{KL}}(\pi\|\pi_{\text{ref}}|[x_{I\&amp;T},Y^{1},t;Y_{\lt i}^{1}])\right]
    $$&lt;p&gt;$t$ 是一个指令. 即希望 $Y_{\geq i}^{2}$ 在 $Y_{\lt i}^{2}$ 的条件下, 能够比 $Y_{\geq i}^{1}$ 更好, 且与参考模型 $\pi_{\text{ref}}$ 尽量接近. 要最大化此式, 设:&lt;/p&gt;
$$
    v(x, Y^1, t; Y_{\lt i}^{1}, Y_{\lt i}^{2}; \pi_{\theta}) = \beta \log \rho(Y_{\geq i}^{2} | [x, Y^1, t; Y_{\lt i}^{2}]) - \beta \log \rho(Y_{\geq i}^{1} | [x, Y^1, t; Y_{\lt i}^{1}]) \\
    u(x, Y^1, t; Y_{\lt i}^{1}, Y_{\lt i}^{2}; \pi_{\theta}) = \beta \log \rho(Y_{\geq i}^{2} | [x, Y^1, t; Y_{\lt i}^{1}]) - \beta \log \rho(Y_{\geq i}^{1} | [x, Y^1, t; Y_{\lt i}^{2}])
    $$&lt;p&gt;其中 $\rho$ 表示 $\pi_{\theta}$ 和 $\pi_{\text{ref}}$ 的重要性采样比, 前一项是鼓励生成更好的轨迹, 后一项是惩罚生成更差的轨迹. 论文证明采用 MSE 误差函数:&lt;/p&gt;
$$
    \begin{aligned}
    L_{\text{SC}}(\pi_{\theta}; \pi_{\text{ref}}) &amp;= \mathbb{E}_{(x, Y^w, Y^l) \sim \mathcal{D}} \left[ 1 - v(x_{I\&amp;T}, Y^l, t; Y_{\lt i}^{l}, Y_{\lt i}^{w}; \pi_{\theta}) - u(x_{I\&amp;T}, Y^l, t; Y_{\lt i}^{l}, Y_{\lt i}^{w}; \pi_{\theta}) \right]^2 \\
    &amp; + \mathbb{E}_{(x, Y^w, Y^l) \sim \mathcal{D}} \left[ 1 + v(x_{I\&amp;T}, Y^w, t; Y_{\lt i}^{w}, Y_{\lt i}^{l}; \pi_{\theta}) + u(x_{I\&amp;T}, Y^w, t; Y_{\lt i}^{w}, Y_{\lt i}^{l}; \pi_{\theta}) \right]^2
    \end{aligned}
    $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;随后再加上 DPO 的损失函数 (前面做的工作已经是偏好优化策略, 再加上这个的目的存疑):&lt;/p&gt;
$$
    L_{\text{Sherlock}}(\pi_{\theta}; \pi_{\text{ref}})  = L_{\text{SC}}(\pi_{\theta}; \pi_{\text{ref}}) + \alpha L_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}})
    $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在此过程中可以根究不同的 $i$ 采用不同的 $\beta$:&lt;/p&gt;
$$
    \beta(i, n, \epsilon) = \frac{1}{4\left( 0.5 + \left( \frac{i}{n} \right)^{0.5 + \epsilon / 2} \right)}
    $$&lt;p&gt;当截断较早, $i$ 较小, $\beta$ 较大, 使得模型更倾向于靠拢 $\pi_{\text{ref}}$, 产生更谨慎的更新, 反之亦然.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;III. &lt;strong&gt;迭代在线偏好训练&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在线迭代训练与离线阶段唯一的区别是没有 ground-truth 的回答 $Y^w$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于每个直接生成的 $Y^1$, 我们进行三轮自我纠正以获得 $Y^2, Y^3, Y^4$.&lt;/li&gt;
&lt;li&gt;应用自我一致性过滤策略: 如果三个纠正响应的最终答案在语义上相同 ($a^2 = a^3 = a^4$), 则认为 $Y^4$ 是偏好回应, $Y_1$ 是非偏好回应. 否则跳过此次训练.&lt;/li&gt;
&lt;li&gt;为进一步减小模型偏好优化的噪声, 让初始的 $Y^1$ 变得更差: 维持 $Y^{l}_{\lt i} = Y^{1}_{\lt i}$, 但对于 $Y^{l}_{\ge i}$ 则在 $Y^{1}_{\ge i}$ 的基础上进行扰动.&lt;/li&gt;
&lt;li&gt;随后按照离线偏好训练的方式继续进行.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/Sherlock.png&#34;
	width=&#34;2278&#34;
	height=&#34;912&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/Sherlock_hu_eafe89ecb6e28239.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/Sherlock_hu_94b896caa624b201.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Sherlock&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;实验-2&#34;&gt;实验
&lt;/h3&gt;&lt;p&gt;论文指出现有的模型并不能通过自我纠正提高推理能力, 经过 Sherlock 进行训练后, 再进行自我纠正, 模型的推理能力有了显著提升.&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/Sherlock-result.png&#34;
	width=&#34;1185&#34;
	height=&#34;651&#34;
	srcset=&#34;https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/Sherlock-result_hu_245149195dfce731.png 480w, https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/img/Sherlock-result_hu_f85b5011af6983e0.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Sherlock 实验结果&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;论文进行了消融实验, 验证了 DPO 损失, SC 损失和动态 $\beta$ 的有效性.&lt;/p&gt;
&lt;h2 id=&#34;objection&#34;&gt;Objection!
&lt;/h2&gt;&lt;p&gt;刚才提及的多数论文都是错的! 一篇未正式发布的文章 &lt;a class=&#34;link cite-9&#34;&gt;[9]
    
&lt;/a&gt; 要打假, 尤其是有关随机奖励的内容, 批驳的论文如下图:&lt;/p&gt;
&lt;p&gt;



&lt;img src=&#34;https://safe-lip-9a8.notion.site/image/attachment%3Ad9cf43ed-51bc-4e52-977c-84589883eb8b%3Aimage.png?table=block&amp;amp;id=2022f1fb-f0ee-805a-acb7-e3701d4b482b&amp;amp;spaceId=50ca3720-be45-4590-b18e-7dba386545c2&amp;amp;width=1420&amp;amp;userId=&amp;amp;cache=v2&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Fake news!&#34;
	
	class=&#34;gallery-image&#34; 
&gt;&lt;/p&gt;
&lt;p&gt;文章声称, 这是因为预-RL 模型的 Baseline 相比 Qwen 发布的官方数据或其它标准化评估被严重低估了, 在很多情况下，经过 RL 后的模型性能实际上比它们开始时的 (正确评估的) 预-RL Baseline 还要差!&lt;/p&gt;
&lt;p&gt;主要问题在于这些论文没有开放数据权重, 导致不能测试, 因而论文的证据不具有说服力. 作者提出了几个可能错误低估的原因:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;格式&lt;/strong&gt;: 数学基准测试使用精确匹配评估, 模型需要以特定格式作答, 例如在 &lt;code&gt;\boxed{}&lt;/code&gt; 内, 有时模型未能遵循格式. 如果模型解决了问题并得到正确答案, 但未能正确格式化, 并不意味着模型的推理能力存在问题. 应该事先通过示例提示或在进行格式演示的少量样本 SFT 来解决 LLMs 的格式问题.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;温度&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/Qwen/Qwen3-8B&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen3 模型页面&lt;/a&gt; 上关于最佳设置有明确的建议. 有几篇论文把温度设置太低, 导致降低准确率.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;测试规模&lt;/strong&gt;: 有的论文使用的 Benchmark 规模太小, 方差太大.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;token 长度&lt;/strong&gt;: 有些模型需要较长的推理流程, 截断较小时会导致模型无法完成响应, 进而降低准确率.&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>论文阅读 - Adam 的收敛性分析</title>
        <link>https://LeoDreamer2004.github.io/p/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-adam-%E7%9A%84%E6%94%B6%E6%95%9B%E6%80%A7%E5%88%86%E6%9E%90/</link>
        <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-adam-%E7%9A%84%E6%94%B6%E6%95%9B%E6%80%A7%E5%88%86%E6%9E%90/</guid>
        <description>&lt;h2 id=&#34;介绍&#34;&gt;介绍
&lt;/h2&gt;&lt;p&gt;在论文 &lt;a class=&#34;link cite-1&#34;&gt;[1]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 中, 作者首次介绍了 Adam 优化器. 此算法一经出现立刻爆火, 现在在深度学习当中已经成为一种最常用的优化算法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Adam&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Adam 的更新公式如下:&lt;/p&gt;
$$
\begin{aligned}
m_t &amp; = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &amp; = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &amp; = \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &amp; = \frac{v_t}{1 - \beta_2^t} \\
\theta_t &amp; = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot \hat{m}_t
\end{aligned}
$$&lt;p&gt;其中 $\odot$ 表示逐元素相乘. 超参数通常取 $\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;可以认为 Adam 本身直接由 SGD 而来. 在此基础上 Adam 引入了几个重要技术:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;移动平均 (Moving Average)&lt;/strong&gt;: $m_t$ 不是通过对梯度直接求和, 而是按照 $\beta_1$ 和 $\beta_2$ 的比例进行移动平均, 保证了梯度的稳定性.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自适应学习率 (Adaptive Learning Rate)&lt;/strong&gt;: $v_t$ 通过对梯度的二阶矩进行估计, 使得学习率可以自适应地调整.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏差修正 (Bias Correction)&lt;/strong&gt;: 由于在训练开始移动平均几乎为 0, 对其引入偏差修正可以加快初始化时刻的收敛速度.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然还有加上衰减的 AdamW, 以及其他的变种, 以适应 Transformer 等模型的训练.&lt;/p&gt;
&lt;h2 id=&#34;收敛--吗&#34;&gt;收敛 &amp;hellip; 吗?
&lt;/h2&gt;&lt;p&gt;论文 &lt;a class=&#34;link cite-1&#34;&gt;[1]
    
&lt;/a&gt; 中提到我们可以引入误差量来衡量收敛性:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;称 &lt;strong&gt;累积误差&lt;/strong&gt; 为&lt;/p&gt;
$$R(T) = \sum_{t=1}^T (f_t(\theta_t) - f_t(\theta^*))$$&lt;p&gt;其中 $\theta^*$ 是最优解, 即 $\theta^* = \argmin_{\theta} \sum_{t=1}^T f_t(\theta)$.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;可以认为, 当 $R(T)/T \to 0$ 时算法收敛. 作者在文献中对 Adam 的收敛性给了自己证明, 里面的细节太多, 这里只给粗略过程.&lt;/p&gt;
&lt;p&gt;鉴于偏差修正只在初期有较大影响, 之后对于收敛性的讨论, 以下证明对其不予考虑 (原论文有), 此外忽略微小项 $\epsilon$.&lt;/p&gt;
&lt;p&gt;在原始的 Adam 中 $\beta_1, \eta$ 都是常数, 实际上此时难以证明. 因此原文中, 对参数做了随时间动态调整:&lt;/p&gt;
$$\eta_t = \frac{\eta}{\sqrt{t}}, \beta_{1,t}=\beta_1 \lambda^{t-1}, \lambda \in (0,1)$$&lt;p&gt;&lt;strong&gt;注意: 以下定理的证明有争议!&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;假设 $f_t$ 梯度有界, $\theta_t$ 之间的距离有界, 即 $\| g_t \|_{\infty} \le G, \|\theta_i-\theta_j\|_{\infty} \le D$, 且 $\beta_1^4 &lt; \beta_2$, Adam 中超参数 $\eta, \beta_1$ 遵从如上动态调整, 则 $R(T) \le \mathcal{O}(\sqrt{T})$, 因而 Adam 收敛.&lt;/p&gt;
&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;首先, 可以证明:&lt;/p&gt;
$$
f_t(\theta_t) - f_t(\theta^*) \le g_t^T(\theta_t - \theta^*) = \sum_{i=1}^d g_{t,i}(\theta_{t,i} - \theta^*_i)
$$&lt;p&gt;$d$ 个分量求和并不会影响量级, 从而我们只需要关心第 $i$ 个分量, 因而下面我们不妨设 $\theta_t, g_t, m_t, v_t$ 等都是一维的. (或者可以用 $\theta_t = \theta_{t,i}$ 来表示), 那么我们只要证明:&lt;/p&gt;
$$
\sum_{t=1}^{T} g_t(\theta_t - \theta^*) \le \mathcal{O}(\sqrt{T})
$$&lt;p&gt;既然要估计 $\theta_t - \theta^*$, 由学习率公式, 我们可以得到:&lt;/p&gt;
$$(\theta_{t+1} - \theta^*) = (\theta_t - \theta^*) - \eta_t \frac{m_t}{\sqrt{v_t}}$$&lt;p&gt;取平方, 有:&lt;/p&gt;
$$(\theta_{t+1} - \theta^*)^2 = (\theta_t - \theta^*)^2 - 2\eta_t \frac{m_t}{\sqrt{v_t}}(\theta_t - \theta^*) + \eta_t^2 \frac{m_t^2}{v_t}$$&lt;p&gt;要把 $m_t$ 换成 $g_t$, 由 $m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$ 代入得到:&lt;/p&gt;
$$(\theta_{t+1} - \theta^*)^2 = (\theta_t - \theta^*)^2 - 2\eta_t \frac{\beta_{1,t} m_{t-1} + (1 - \beta_{1,t}) g_t}{\sqrt{v_t}}(\theta_t - \theta^*) + \eta_t^2 \frac{m_t^2}{v_t}
$$&lt;p&gt;把需要处理的量放在左边:&lt;/p&gt;
$$
(1-\beta_{1,t})g_t(\theta_t - \theta^*) = \frac{\sqrt{v_t}\left((\theta_t - \theta^*)^2-(\theta_{t+1} - \theta^*)^2\right)}{2\eta_t} - \beta_{1,t} m_{t-1}(\theta_t - \theta^*) + \frac{\eta_t m_t^2}{2\sqrt{v_t}}
$$&lt;p&gt;这左边可以由 $1 \ge 1-\beta_{1,t} \ge 1-\beta_{1,1}$ 直接看作 $g_t(\theta_t - \theta^*)$ 量级, 右边现在已经分成三个部分了, 只需要累和来看每个部分的量级.&lt;/p&gt;
&lt;p&gt;一个显然的结论是, 在移动平均下, 易见 $m_t \le G (m_0 \le G), v_t \le G^2 (v_0 \le G^2)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一项&lt;/strong&gt;: 忽略常数 $2$, 暂记 $\gamma_t = \frac{\sqrt{v_t}}{\eta_t} = \mathcal{O}(\sqrt{T})$, 只要考虑:&lt;/p&gt;
$$
M_1=\sum_{t=1}^{T} \gamma_t \left((\theta_t - \theta^*)^2-(\theta_{t+1} - \theta^*)^2\right)
$$&lt;p&gt;利用 Abel 求和法则, 可以得到:&lt;/p&gt;
$$
M_1 =\gamma_1(\theta_1 - \theta^*)^2 - \gamma_{t+1}(\theta_{T+1} - \theta^*)^2 + \sum_{t=1}^{T} (\gamma_{t+1} - \gamma_t)(\theta_t - \theta^*)^2
$$&lt;p&gt;一般来说 $\gamma_t = \mathcal{O}(\sqrt{T})$ 应该是单调不减的, 在 $\gamma_t \le \gamma_{t+1}$ 的情况下, 可以得到:&lt;/p&gt;
$$
\begin{aligned}
M_1 &amp;\le \gamma_1(\theta_1 - \theta^*)^2 +  \sum_{t=1}^{T} (\gamma_{t+1} - \gamma_t)D^2 \\
&amp;= C + (\gamma_{t+1} - \gamma_1)D^2 = \mathcal{O}(\sqrt{T})
\end{aligned}
$$&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;问题就出在这个 &amp;ldquo;一般来说&amp;rdquo; 上&lt;/strong&gt;, 因为尽管引入参数衰减, 实际上 Adam 并不能保证 $\gamma_t$ 是单调不减的. 后面会提到这里的争议.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;第二项&lt;/strong&gt;: 直接放缩即可:&lt;/p&gt;
$$
\begin{aligned}
M_2 &amp;=\sum_{t=1}^{T} \beta_{1,t}m_{t-1}(\theta_t - \theta^*) \le \sum_{t=1}^{T} \beta_{1,t} |m_{t-1}| D \\
&amp;\le GD \sum_{t=1}^{T} \beta_{1,t} = G D \beta_1 \frac{1-\lambda^T}{1-\lambda} = \mathcal{O}(1)
\end{aligned}
$$&lt;p&gt;&lt;strong&gt;第三项&lt;/strong&gt;: $v_t$ 未必有下界, 有点麻烦! 直接写通式:&lt;/p&gt;
$$
\begin{aligned}
m_t &amp;= \sum_{s=1}^t (1-\beta_{1,s})\left(\prod_{r=s+1}^{t}\beta_{1,r}\right)g_s \le \sum_{s=1}^t \beta_1^{t-s}g_s\\
v_t &amp;= (1-\beta_2)\sum_{s=1}^t \beta_2^{t-s}g_s^2
\end{aligned}
$$&lt;p&gt;既然要控制 $\frac{m_t^2}{\sqrt{v_t}}$, 结合 $\beta_1^4 &lt; \beta_2$, 那么考虑 Young 不等式:&lt;/p&gt;
$$
\begin{aligned}
m_t^4 &amp;\le \left(\sum_{s=1}^t \beta_2^{t-s}g_s^2 \right) \left(\sum_{s=1}^t \frac{\beta_1^{\frac{4}{3}(t-s)}}{\beta_2^{\frac{1}{3}(t-s)}}g_s^{\frac{2}{3}} \right)^{3} \\
&amp;\le v_t^2 G^2 \left(\frac{1-\mu^t}{1-\mu}\right)^3 = v_t^2 \mathcal{O}(1)
\end{aligned}
$$&lt;p&gt;这里 $\mu = \beta_1^{\frac{4}{3}} / \beta_2^{\frac{1}{3}} &lt; 1$. 因此:&lt;/p&gt;
$$
M_3 =\sum_{t=1}^{T} \eta_t^2 \frac{m_t^2}{\sqrt{v_t}} \le C \sum_{t=1}^{T} \eta_t^2 = C \sum_{t=1}^{T} \mathcal{O}\left(\frac{1}{t}\right) = \mathcal{O}(\ln T)
$$&lt;p&gt;自此, 三项综合可以得到:&lt;/p&gt;
$$
R(T) \le \mathcal{O}(\sqrt{T}) + \mathcal{O}(1) + \mathcal{O}(\ln T) = \mathcal{O}(\sqrt{T})
$$&lt;/div&gt;
&lt;h2 id=&#34;objection&#34;&gt;Objection!
&lt;/h2&gt;&lt;p&gt;论文 &lt;a class=&#34;link cite-1&#34;&gt;[1]
    
&lt;/a&gt; 的这个漏洞显然为人诟病. 于是论文 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 中, 作者指出 Adam 并不总是收敛的. 论文中给出了一个反例:&lt;/p&gt;
&lt;p&gt;我们取 $\beta_1=0, \beta_2=\frac{1}{1+C^2}$. 设考虑在时间 $t$ 观测到的函数 $f_t$ 为:&lt;/p&gt;
$$
f_t(x) = \begin{cases}
Cx &amp; t \equiv 1 \pmod {3} \\
-x &amp; \text{Otherwise} \\
\end{cases}, \quad x \in [-1,1]
$$&lt;p&gt;其中 $C&gt;2$ 是一个常数. 从宏观尺度上, $f=\frac{1}{3}(C-2)x$, 最低点在 $x=-1$ 处. 显然 $f$ 和其他超参数满足定理条件, 然而经过 (冗长枯燥的) 计算可以得知, 由于每三次迭代中 $f$ 就会有两次向错误的方向更新, 加上移动平均导致的历史遗忘, 会导致无法正确收敛. 具体过程可以参考原文附录.&lt;/p&gt;
&lt;p&gt;为了解决这个问题, 作者提出为了确保 $\gamma$ 是单调不减的, 可以在更新时让 $v_{t+1}$ 与 $v_t$ 取一个最大值作为更新值. 由此, 引出 Amsgrad 算法:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Amsgrad&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Amsgrad 的更新公式如下:&lt;/p&gt;
$$
\begin{aligned}
m_t &amp; = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &amp; = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{v}_t &amp;= \max(v_t, \hat{v}_{t-1}) \\
\theta_t &amp; = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot m_t
\end{aligned}
$$&lt;p&gt;此处省略了偏差修正.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;论文 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt; 采取实验证明, Amsgrad 具有更好的收敛性.&lt;/p&gt;
&lt;h2 id=&#34;辩护与和解&#34;&gt;辩护与和解
&lt;/h2&gt;&lt;p&gt;既然 Adam 可能不收敛, 那为什么在实际中表现良好呢? 一个直观的批驳是, 论文 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt; 中的反例取的参数相当极端: $\beta_1, \beta_2$ 都很小. 在实际中, 我们通常取 $\beta_1=0.9, \beta_2=0.999$, 这便是论文 &lt;a class=&#34;link cite-3&#34;&gt;[3]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 的切入点. 作者认为, 只要 $\beta_1^2 &lt; \beta_2$, 且 $\beta_2$ 充分大, 就可以保证收敛.&lt;/p&gt;
&lt;p&gt;和 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt;不同, 论文 &lt;a class=&#34;link cite-3&#34;&gt;[3]
    
&lt;/a&gt; 更注重实际. 从训练角度上, 一般是分成若干个 Epoch, 每个 Epoch 内处理的都是相同的 $n$ 个函数. 作者对 $f$ 以及其内的 $n$ 个分函数提出了如下要求:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;定义集合 $\mathcal{F}$ 为满足如下条件的函数 $f: \mathbb{R}^d \mapsto \mathbb{R}$ 集合:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f$ 有界, 即 $$f \le f^\ast$$&lt;/li&gt;
&lt;li&gt;所有 $n$ 个时间刻的 $f_t$ 是 Lipschitz 连续的, 即 $$\|f_t(x) - f_t(y)\| \le L \|x - y\|$$&lt;/li&gt;
&lt;li&gt;所有 $n$ 个时间刻的 $f_t$ 满足 $$\sum_{i=0}^{n-1} \| \nabla f_i(x) \|^2 \le D_1 \| \nabla f(x) \|^2 + D_0 $$&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;作者称第三个要求实际上是非常容易的, 事实上确实如此.&lt;/p&gt;
&lt;p&gt;我们记第 $k$ 个 Epoch 开始时的变量为 $x_k$, 运行到第 $0 \le i &lt; n$ 个函数时为 $x_{k,i}$, 即 $x_k=x_{k,0}=x_{k-1,n}$. 现在, 作者称梯度的上界有保证:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $f(x) \in \mathcal{F}$, 遵守 $\mathcal{F}$ 内定义中的记号. 设 $\beta_1^2 &lt; \beta_2 &lt; 1$, 且 $\beta_2 \ge \gamma(n)$, 学习率衰减 $\eta_k= \frac{\eta}{\sqrt{nk}}$, 则存在某个充分大的 $K$, 对于 $T&gt;K$ 均有:&lt;/p&gt;
$$
\min_{k \in [K,T]} \mathbb{E} \left\{ \min \left[ \sqrt{\frac{2D_1d}{D_0}} \| \nabla f(x_k) \|^2, \| \nabla f(x_k) \| \right] \right\} = \mathcal{O} \left( \frac{\log T}{\sqrt{T}} + \sqrt{D_0}\right)
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;我们先澄清, 由 $R(T)$ 中的证明, $m_t$ 和 $v_t$ 有上界, 且 $\frac{m_t}{\sqrt{v_t}}$ 也有上界 (证明依然类似, 把 Young 特化成 Cauchy 即可, 此时条件变为 $\beta_1^2 \le \beta_2$).&lt;/p&gt;
&lt;p&gt;从下降引理出发:&lt;/p&gt;
$$
\mathbb{E}f(x_{k+1}) - \mathbb{E}f(x_k) \le \mathbb{E} \left&lt; \nabla f(x_k), x_{k+1} - x_k \right&gt; + \frac{L}{2} \mathbb{E} \|x_{k+1} - x_k\|^2
$$&lt;p&gt;做累加, 有:&lt;/p&gt;
$$
\mathbb{E} \sum_{k=t_0}^T \left&lt; \nabla f(x_k), x_k - x_{k+1} \right&gt; \le \frac{L}{2} \mathbb{E} \sum_{k=t_0}^T \|x_{k+1} - x_k\|^2 + \mathbb{E}f(x_{t_0}) - \mathbb{E}f(x_{T+1})
$$&lt;p&gt;首先考虑右侧的上界, 这个相对容易, 注意到由于 $m_k$, $v_k$ 都是常量级:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \|x_{k+1} - x_k\|^2 &amp;\le \mathbb{E} \sum_{i=0}^{n-1} \|x_{k,i+1} - x_{k,i}\|^2 \le n \max_{0 \le i &lt; n} \mathbb{E} \|x_{k,i+1} - x_{k,i}\|^2 \\
&amp;= n\mathcal{O}\left(\frac{\eta_k^2m^2_{k,i}}{v_{k,i}}\right) = \mathcal{O}\left(\frac{1}{k}\right)
\end{aligned}
$$&lt;p&gt;因此:&lt;/p&gt;
$$
\frac{L}{2} \mathbb{E} \sum_{k=t_0}^T \|x_{k+1} - x_k\|^2 \le \sum_{k=t_0}^T \mathcal{O}\left(\frac{1}{k}\right) = \mathcal{O}(\log T)
$$&lt;p&gt;关于左侧的下界, 按每一维展开:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \left&lt; \nabla f(x_k), x_k - x_{k+1} \right&gt; &amp;= \eta_k \mathbb{E} \left&lt; \nabla f(x_k), \sum_{i=0}^{n-1} \frac{m_{k,i}}{\sqrt{v_{k,i}}} \right&gt; \\
&amp;= \eta_k \mathbb{E} \sum_{l=1}^d \sum_{i=0}^{n-1} \partial_l f(x_k) \frac{m_{k,i}^{(l)}}{\sqrt{v_{k,i}^{(l)}}}
\end{aligned}
$$&lt;hr&gt;
&lt;p&gt;以下推导长达数十页, 这里简要概括一下思路. 先考虑其中一维:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \sum_{i=0}^{n-1} \nabla f(x_k) \frac{m_{k,i}}{\sqrt{v_{k,i}}}
\end{aligned}
$$&lt;p&gt;其中 $x_k$ 实际上应该是 $x_k^{(l)}$, $\nabla f(x_k)$ 实际上应该是 $\partial_l f(x_k)$.&lt;/p&gt;
&lt;p&gt;显然一方面, 我们提过 $\frac{m_{k,i}}{\sqrt{v_{k,i}}}$ 是有界的, 由于 Lipschitz 连续, $\nabla f(x_k)$ 也是有界的, 则这一项自然也是有界的, 此界与 $k$ 无关. 因此整个左侧为 $\mathcal{O}(\eta_k) = \mathcal{O}(1/\sqrt{k})$, 这个放缩是平凡的.&lt;/p&gt;
&lt;!-- &gt; 令人费解的是, 作者在这里大费周章地讨论了 $\nabla f$ 无界的情形, 并把它作为证明讨论的核心. 然而由于 $f_i$ 是 Lipschitz 连续的, 它们的和函数 $f$ 也是 Lipschitz 连续的, 因此 $\nabla f$ 有界. 可能是我的理解有误, 但我认为作者在这里的讨论是多余的. --&gt;
&lt;p&gt;另一方面, 感性上讲, $v_{k,i} \approx C, m_{k,i} \approx \nabla f_i(x_k)$, 如果这个 $\approx$ 成立, 显然就有左边 $\ge 0$ 了, 这是我们的目标. 是时候做拆分了:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i=0}^{n-1} \nabla f(x_k) \frac{m_{k,i}}{\sqrt{v_{k,i}}} &amp;= \sum_{i=0}^{n-1} \left( \nabla f(x_k) \frac{m_{k,i}}{\sqrt{v_{k,i}}}-\nabla f(x_k) \frac{\nabla f_i(x_k)}{\sqrt{v_{k,i}}}  \right) \\
&amp;+ \sum_{i=0}^{n-1} \nabla f(x_k) \frac{\nabla f_i(x_k)}{\sqrt{v_{k,i}}}
\end{aligned}
$$&lt;p&gt;现在作者称: 两项的 $v_{k,i}$ 均可以换成 $v_{k,0}$, 且不影响量级, 证明太过繁杂这里省略. 我们转而考虑:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i=0}^{n-1} \nabla f(x_k) \frac{m_{k,i}}{\sqrt{v_{k,i}}} &amp;\approx \sum_{i=0}^{n-1} \left( \nabla f(x_k) \frac{m_{k,i}}{\sqrt{v_{k,0}}}-\nabla f(x_k) \frac{\nabla f_i(x_k)}{\sqrt{v_{k,0}}}  \right) \\
&amp;+ \sum_{i=0}^{n-1} \nabla f(x_k) \frac{\nabla f_i(x_k)}{\sqrt{v_{k,0}}} \\
\end{aligned}
$$&lt;p&gt;第二项正是我们的目标:&lt;/p&gt;
$$M_2 = \frac{\nabla f(x_k)}{\sqrt{v_{k,0}}}\sum_{i=0}^{n-1} \nabla f_i(x_k) = \frac{\| \nabla f(x_k)^2 \|}{\sqrt{v_{k,0}}} \ge 0$$&lt;p&gt;现在全力以赴地考虑第一项, 这是证明的核心, 证明也很繁杂, 涉及大量计算以及对梯度范围的讨论, 这里省略. 最后合并处理得到结论.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;强调一点, 所谓 $\beta_2$ 充分大, 是指 $\beta_2$ 需要大于一个与 $n$ 有关的常数 $\gamma(n)$, 如果 $n$ 不固定, 定理并不能证明什么. 因此, 作者与 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt; 的结论达成了和解, 它们并不矛盾. 在 &lt;a class=&#34;link cite-3&#34;&gt;[3]
    
&lt;/a&gt; 中, 作者给出了关于收敛的结论:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先, 对于每个函数, 只要满足 $\beta_1^2&lt;\beta_2$, 且 $\beta_2$ 充分大, 则当 $T \to \infty$ 时, 右侧趋于 $\sqrt{D_0}$, 这意味着收敛到一个范围内的点. 特别地, 如果 $D_0=0$, 右侧趋于 $0$, 这意味着收敛到最优点. 这与我们在实际中观察到的现象一致. 作者还给出了一块 &amp;ldquo;危险区域&amp;rdquo;, 只要 $(\beta_1, \beta_2)$ 落在这个区域内, 就会导致 Adam 不收敛. 除此之外的区域, 仍是未知的.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果超参数先于函数取值, 注意此时 $n$ 可变. 无论 $(\beta_1, \beta_2)$ 取什么值, 都存在一个函数, 使得 Adam 不收敛. 作者给出的反例是:&lt;/p&gt;
$$
    \begin{aligned}
    f_0(x) &amp;= \begin{cases}
    nx, &amp; x \ge -1 \\
    \frac{n}{2}(x+2)^2 - \frac{3n}{2}, &amp; x &lt; -1
    \end{cases} \\
    f_i(x) &amp;= \begin{cases}
    x, &amp; x \ge -1 \\
    \frac{1}{2}(x+2)^2 - \frac{3}{2}, &amp; x &lt; -1
    \end{cases} \\
    \end{aligned}
    $$&lt;p&gt;在 $n \to \infty$ 时, 不收敛的 &amp;ldquo;危险区域&amp;rdquo; 会变得越来越大, 直到盖住 $(\beta_1, \beta_2)$ 这个点. 这个反例显然要比论文 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt; 中的更加深刻.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果超参数在函数之后取值. 前面提到只要条件保证成立, 就可以保证收敛到最优局部或最优点.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然, 日常训练中的情形更符合第二种, 我们都是先定义好损失函数, 然后再选择超参数, 因此这也就解释了为什么 Adam 在绝大多数训练中表现良好, 重点在于我们选取的超参数符合定理的要求.&lt;/p&gt;
&lt;h2 id=&#34;应用-另一个算法&#34;&gt;应用: 另一个算法
&lt;/h2&gt;&lt;p&gt;遇到了一个基于 Nestorov 等价形式的算法, 和 Adam 非常相似, 于是想能不能直接推广证明, 方便起见暂时叫它 C&amp;rsquo;Adam. 更新公式如下:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;C&amp;#39;Adam&lt;/span&gt;&lt;/p&gt;
$$
\begin{aligned}
m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
\theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \odot (\beta_1 m_t + g_t)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;忽略偏差修正, 可以看到和 Adam 只有最后一行不同. 我尝试着给出一份证收敛性证明. 以下仍然简记 $x_k = x_{k,0}$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;/p&gt;
&lt;p&gt;设 $f(x) = \sum_{i=0}^{n-1} f_i$ 有界, 且任意时刻 $f_t$ 满足 $L$- Lipschitz 连续, 满足增长性条件 $\sum_{i=0}^{n-1} \| \nabla f_i(x) \|^2 \le D_1 \| \nabla f(x) \| ^2 + D_0$ . 设 $\beta_1^2 &lt; \beta_2 &lt; 1$, 且 $\beta_2 \ge \gamma(n)$, 学习率衰减 $\eta_k= \frac{\eta}{\sqrt{nk}}$, 则存在某个充分大的 $K$, 对于 $T&gt;K$ 均有:&lt;/p&gt;
$$
\min_{k \in [K,T]} \mathbb{E} \left\{ \min \left[ \sqrt{\frac{2D_1d}{D_0}} \| \nabla f(x_k) \|^2, \| \nabla f(x_k) \| \right] \right\} = \mathcal{O} \left( \frac{\log T}{\sqrt{T}} + \sqrt{D_0}\right)
$$&lt;p class=&#34;math-block-title math-prf&#34;&gt;证明&lt;/p&gt;
&lt;p&gt;从下降引理出发:&lt;/p&gt;
$$
\mathbb{E}f(x_{k+1}) - \mathbb{E}f(x_k) \le \mathbb{E} \left&lt; \nabla f(x_k), x_{k+1} - x_k \right&gt; + \frac{L}{2} \mathbb{E} \|x_{k+1} - x_k\|^2
$$&lt;p&gt;做累加, 有:&lt;/p&gt;
$$
\mathbb{E} \sum_{k=t_0}^T \left&lt; \nabla f(x_k), x_k - x_{k+1} \right&gt; \le \frac{L}{2} \mathbb{E} \sum_{k=t_0}^T \|x_{k+1} - x_k\|^2 + \mathbb{E}f(x_{t_0}) - \mathbb{E}f(x_{T+1})
$$&lt;p&gt;考虑右侧, 后两项是常数级别, 只需要考虑第一项:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \|x_{k+1} - x_k\|^2 &amp;\le \mathbb{E} \sum_{i=0}^{n-1} \|x_{k,i+1} - x_{k,i}\|^2 \le n \max_{0 \le i &lt; n} \mathbb{E} \|x_{k,i+1} - x_{k,i}\|^2 \\
&amp;\le n \mathcal{O}\left(\frac{\eta_k^2m^2_{k,i}}{v_{k,i}}\right) = \mathcal{O}\left(\frac{1}{k}\right)
\end{aligned}
$$&lt;p&gt;这里用到 $\eta_k = \mathcal{O}(1/\sqrt{k})$, 而对于 $m^2_{k,i}/{v_{k,i}}$, 论文已经证明其为有界.&lt;/p&gt;
&lt;p&gt;从而右边:&lt;/p&gt;
$$
RHS \le \sum_{k=t_0}^T \mathcal{O}\left(\frac{1}{k}\right) + C = \mathcal{O}(\log T) + \mathcal{O}(1)
$$&lt;p&gt;现在考察左侧, 右上角标 $(l)$ 表示第 $l$ 个分量:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \left&lt; \nabla f(x_k), x_k - x_{k+1} \right&gt; &amp;= \eta_k \mathbb{E} \left&lt; \nabla f(x_k), \sum_{i=0}^{n-1} \frac{\beta_1 m_{k,i}+ g_{k,i}}{\sqrt{v_{k,i}}} \right&gt; \\
&amp;= \eta_k \mathbb{E} \sum_{l=1}^d \sum_{i=0}^{n-1} \partial_l f(x_k) \left( \frac{\beta_1 m_{k,i}^{(l)} + \partial_l f_i(x_k)}{\sqrt{v_{k,i}^{(l)}}}\right)
\end{aligned}
$$&lt;p&gt;按分子上的加号拆成两部分. 对于前者, 论文中 (35) 式已经证明了:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \sum_{l=1}^d \sum_{i=0}^{n-1} \partial_l f(x_k) \frac{m_{k,i}^{(l)}}{\sqrt{v_{k,i}^{(l)}}} \ge &amp;\frac{1}{d \sqrt{10D_1d}} \mathbb{E} \min \left[ \sqrt{\frac{2D_1d}{D_0}} \| \nabla f(x_k) \|^2, \| \nabla f(x_k) \| \right] \\
&amp; - \mathcal{O}\left(\frac{1}{\sqrt{k}}\right) - \mathcal{O}\left(\sqrt{D_0}\right)
\end{aligned}
$$&lt;p&gt;对于后者, 再拆分:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E} \sum_{l=1}^d \sum_{i=0}^{n-1} \partial_l f(x_k) \frac{\partial_l f_i(x_k)}{\sqrt{v_{k,i}^{(l)}}} = \mathbb{E} \sum_{l=1}^d \sum_{i=0}^{n-1} \frac{\partial_l^2 f(x_k)}{\sqrt{v_{k,i}^{(l)}}} + \mathbb{E} \sum_{l=1}^d \sum_{i=0}^{n-1} \partial_l f(x_k) \frac{\partial_l f_i(x_k) - \partial_l f(x_k)}{\sqrt{v_{k,i}^{(l)}}}
\end{aligned}
$$&lt;p&gt;前一项显然非负, 后一项即论文中的 $(a_2)$项, 其在引理 G.5 已经证明当 $\beta_2 \to 1$ 时, 此项趋于 $0$.&lt;/p&gt;
&lt;p&gt;我们综合关于左侧的讨论, 即有:&lt;/p&gt;
$$
LHS \ge \beta_1 \sum_{k=t_0}^T \eta_k \left( \frac{1}{d \sqrt{10D_1d}} \mathbb{E} \min \left[ \sqrt{\frac{2D_1d}{D_0}} \| \nabla f(x_k) \|^2, \| \nabla f(x_k) \| \right] - \mathcal{O}\left(\frac{1}{\sqrt{k}}\right) - \mathcal{O}\left(\sqrt{D_0}\right)  \right)
$$&lt;p&gt;又 $\eta_k = \mathcal{O}(1/\sqrt{k})$, 则 $\sum_{k=t_0}^T \eta_k = \mathcal{O}(\sqrt{T})$, 忽略所有低阶项, 联合左右两端, 我们得到:&lt;/p&gt;
$$
\mathcal{O} \left(\sqrt{T}\right) \left( \frac{1}{d \sqrt{10D_1d}} \mathbb{E} \min \left[ \sqrt{\frac{2D_1d}{D_0}} \| \nabla f(x_k) \|^2, \| \nabla f(x_k) \| \right] - \mathcal{O} \left(\sqrt{D_0}\right) \right) \le \mathcal{O}(\log T)
$$&lt;p&gt;由此移项即得证.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;p&gt;Adam 优化器的收敛性问题, 经过多篇论文的讨论, 目前已经有了比较清晰的结论. 论文 &lt;a class=&#34;link cite-1&#34;&gt;[1]
    
&lt;/a&gt; 中的定理是有漏洞的, 但在实际中, Adam 的表现依然良好. 尽管存在着不收敛的情况, 论文 &lt;a class=&#34;link cite-3&#34;&gt;[3]
    
&lt;/a&gt; 已经充分表明, 在日常使用中, 只要按照正常习惯选取合理的超参数, 就可以保证收敛, Adam 的实用性至此得到了验证.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
