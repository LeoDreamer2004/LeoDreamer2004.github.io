[{"content":"奇异值分解 用 $R(A)$ 表达 $\\text{Im}(A)$, $N(A)$ 表达 $\\text{Ker}(A)$. 则 $\\text{dim} R(A) = \\text{rank}(A)$, $\\text{dim} R(A) + \\text{dim} N(A) = n$.\n定理奇异值分解\n对任意矩阵 $A$, 存在正交矩阵 $U$ 和 $V$, 以及对角矩阵 $\\Sigma$ 使得:\n$$ A = U \\Sigma V^T $$证明\n$A^TA$ 是对称的, $\\text{rank}(A^TA) = r$, 则特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\lambda_r \u003e 0 = \\lambda_{r+1} = \\lambda_{r+2} = \\cdots \\lambda_n$, 可正交对角化:\n$$A^TA = V \\Lambda V^T$$把 $V$ 分成两部分 $V=[V_1, V_2]$, 其中 $V_1 = [v_1, \\cdots, v_r]$, $V_2 = [v_{r+1}, \\cdots, v_n]$. 显见 $v_{r+1}, \\cdots, v_n$ 恰好构成 $N(A^TA)$ 的标准正交基.\n从 $V_1 = [v_1, \\cdots, v_r]$ 出发考虑 $U_1 = [u_1, \\cdots, u_r]$:\n$$ u_i = \\frac{1}{\\sqrt{\\lambda_i}} A v_i $$则容易验证 $u_i$ 是 $R(A)$ 的标准正交基. $R(A)$ 的正交补是 $N(A^T)$, 考虑其一组正交基 $U_2 = [u_{r+1}, \\cdots, u_n]$, 则 $U = [U_1, U_2]$ 是正交矩阵. 记:\n$$ \\Sigma_1 = \\text{diag}(\\sqrt{\\lambda_1}, \\cdots, \\sqrt{\\lambda_r}) \\\\ \\Sigma = \\begin{bmatrix} \\Sigma_1 \u0026 0 \\\\ 0 \u0026 0 \\end{bmatrix} $$则可以得出 $U_1\\Sigma_1 = AV_1$. 则:\n$$ U \\Sigma V^T = [U_1, U_2] \\begin{bmatrix} \\Sigma_1 \u0026 0 \\\\ 0 \u0026 0 \\end{bmatrix} \\begin{bmatrix} V_1^T \\\\ V_2^T \\end{bmatrix} = U_1 \\Sigma_1 V_1^T = AV_1V_1^T = A $$ 可以看出右奇异向量 $V$ 的列向量是 $A^TA$ 的特征向量, 左奇异向量 $U$ 的列向量是 $AA^T$ 的特征向量.\n定义\n对一个非零的 $m \\times n$ 实矩阵 $A \\in \\mathbb{R}^{m \\times n}$, 可将其表示为满足如下特性的三个实矩阵乘积形式的因子分解运算:\n$$A = U \\Sigma V^T$$其中:\n$U$ 是 $m$ 阶正交矩阵, $U^T U = I$; $V$ 是 $n$ 阶正交矩阵, $V^T V = I$; $\\Sigma$ 是由降序排列的非负的对角线元素组成的 $m \\times n$ 矩形对角矩阵: $$ \\Sigma = \\text{diag}(\\sigma_1, \\cdots, \\sigma_p)$$ 这里 $\\sigma_1 \\geq \\cdots \\geq \\sigma_p \\geq 0$, 且 $p = \\min(m, n)$.\n$U \\Sigma V^T$ 称为 $A$ 的 奇异值分解, $\\sigma_i$ 称为 $A$ 的 奇异值, $U$ 和 $V$ 的列向量分别称为 $A$ 的 左,右奇异向量.\n特别地, 当 $\\text{rank}(A)=r$ 时, $\\Sigma$ 的前 $r$ 个对角线元素 $\\sigma_1, \\cdots, \\sigma_r$ 是正的, 我们称 $U_r\\Sigma_r V_r^T$ 为 $A$ 的 紧奇异值分解; 对于任意 $0 \\lt k \\lt r$, $U_k\\Sigma_k V_k^T$ 称为 $A$ 的 截断奇异值分解.\n奇异值分解与矩阵近似 定义\n设 $A \\in \\mathbb{R}^{m \\times n}$ 且 $A = [a_{ij}]_{m \\times n}$, $A$ 的 Frobenius 范数 $\\|A\\|_F$ 定义如下:\n$$\\|A\\|_F = \\left[\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}^2\\right]^{\\frac{1}{2}}$$ 定理\n若 $Q$ 是 $m$ 阶正交矩阵, 则 $\\|QA\\|_F = \\|A\\|_F$.\n证明\n设 $A = [a_1, \\cdots, a_n]$, 则:\n$$ \\begin{aligned} \\|QA\\|_F^2 \u0026= \\| [Qa_1, \\cdots, Qa_n]\\|_F^2 = \\sum_{i=1}^{n} \\|Qa_i\\|^2 \\\\ \u0026= \\sum_{i=1}^{n} (Qa_i)^T (Qa_i) = \\sum_{i=1}^{n} a_i^T Q^T Q a_i \\\\ \u0026= \\sum_{i=1}^{n} a_i^T a_i = \\sum_{i=1}^{n} \\|a\\|^2 = \\|A\\|_F^2 \\end{aligned} $$ 定理\n设 $A \\in \\mathbb{R}^{m \\times n}$, $\\text{rank}(A) = r$, $A = U \\Sigma V^T$ 是 $A$ 的奇异值分解, 并设 $\\mathcal{M}$ 是 $\\mathbb{R}^{m \\times n}$ 中所有秩不超过 $k$ 的矩阵的集合, $0 \\lt k \\lt r$.\n若 $A'=U\\Sigma'V^T$, 其中 $\\Sigma'_{m \\times n} = \\begin{bmatrix} \\Sigma_k \u0026 0 \\\\ 0 \u0026 0 \\end{bmatrix}$, 这里 $\\Sigma_k = \\text{diag}(\\sigma_1, \\cdots, \\sigma_k)$, 则:\n$$ \\|A-A'\\|_F = \\sqrt{\\sum_{l=k+1}^{n} \\sigma_l^2} = \\min_{S\\in \\mathcal{M}} \\|A-S\\|_F $$即截断奇异值分解 $A' = U\\Sigma'V^T$ 是 $A$ 在 $\\mathcal{M}$ 中的最优近似.\n证明\n一个显然的结论是, 由上一个定理, 设 $A=U\\Sigma V^T$, 则:\n$$ \\|A\\|_F = \\|U\\Sigma V^T\\|_F = \\|\\Sigma\\|_F = \\sqrt{\\sum_{l=1}^{n} \\sigma_l^2} $$设 $X$ 是 $A$ 的最优近似, 则:\n$$ \\|A-X\\|_F = \\|A-A'\\|_F = \\sqrt{\\sum_{l=k+1}^{n} \\sigma_l^2} $$下面只需要证明:\n$$\\|A-X\\|_F \\ge \\sqrt{\\sum_{l=k+1}^{n} \\sigma_l^2}$$再设 $X$ 的奇异值分解为 $X = Q\\Omega P^T$, 其中:\n$$ \\Omega = \\begin{bmatrix} \\Omega_1 \u0026 0 \\\\ 0 \u0026 0 \\end{bmatrix} $$这里 $\\Omega_1 = \\text{diag}(\\omega_1, \\cdots, \\omega_k)$.\n令 $B=Q^TAP$, 则 $A=QBP^T$, 按照 $\\Omega$ 的分块方法对 $B$ 进行分块:\n$$ B = \\begin{bmatrix} B_{11} \u0026 B_{12} \\\\ B_{21} \u0026 B_{22} \\end{bmatrix} $$则:\n$$ \\begin{aligned} \\|A-X\\|_F^2 \u0026= \\|Q(B-\\Omega)P^T\\|_F^2 = \\|B-\\Omega\\|_F^2 \\\\ \u0026= \\|B_{11}-\\Omega_1\\|_F^2 + \\|B_{12}\\|_F^2 + \\|B_{21}\\|_F^2 + \\|B_{22}\\|_F^2 \\end{aligned} $$既然 $X$ 使得 $\\|A-X\\|_F^2$ 最小, 我们考虑取 $Y=Q\\Omega'P^T \\in \\mathcal{M}$, 其中 $\\Omega' = \\begin{bmatrix} B_{11} \u0026 B_{12} \\\\ 0 \u0026 0 \\end{bmatrix}$, 则由最小性可得:\n$$ \\|A-X\\|_F^2 \\le \\|A-Y\\|_F^2 = \\|B_{21}\\|_F^2 + \\|B_{22}\\|_F^2 $$由此立得 $B_{12} = 0$, 且 $B_{11} = \\Omega_1$, 同理 $B_{21}=0$. 从而我们得到:\n$$ \\|A-X\\|_F = \\|B_{22}\\|_F $$设 $B_{22}$ 的奇异值分解是 $B_{22} = U_1 \\Lambda V_1^T$, 则:\n$$ \\|A-X\\|_F = \\|B_{22}\\|_F = \\|\\Lambda\\|_F $$注意到:\n$$ Q^T A P = B = \\begin{bmatrix} \\Omega_1 \u0026 0 \\\\ 0 \u0026 B_{22} \\end{bmatrix} $$那么右下角也可以对角化, 准确来说设:\n$$ U_2 = \\begin{bmatrix} I_k \u0026 0 \\\\ 0 \u0026 U_1 \\end{bmatrix}, \\quad V_2 = \\begin{bmatrix} I_k \u0026 0 \\\\ 0 \u0026 V_1 \\end{bmatrix} $$显见:\n$$ U_2^T Q^T A P V_2 = U_2^T B V_2 = \\begin{bmatrix} \\Omega_1 \u0026 0 \\\\ 0 \u0026 \\Lambda \\end{bmatrix} $$即:\n$$ A = QU_2 \\begin{bmatrix} \\Omega_1 \u0026 0 \\\\ 0 \u0026 \\Lambda \\end{bmatrix} \\left(PV_2\\right)^T $$这意味着 $\\Lambda$ 的对角线元素是 $A$ 的奇异值, 故有\n$$ \\|A-X\\|_F = \\|\\Lambda\\|_F \\ge \\sqrt{\\sum_{l=k+1}^n \\sigma_l^2} $$因此 $\\|A-X\\|_F = \\sqrt{\\sum_{l=k+1}^n \\sigma_l^2} = \\|A-A'\\|_F$.\n实际上:\n$$A = U\\Sigma V^T = \\sum_{i=1}^n \\sigma_i u_i v_i^T$$这也称为 $A$ 的 外积展开式, 显然截断奇异值分解 $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$.\n主成分分析 如果数据的一些特征之间存在相关性，处理起来不太方便； 如果数据维数过高，影响算法性能. 我们希望能构造一组新的相互不相关的特征来表示数据：\n通常用原来特征的线性组合来构造新特征. 希望特征变换的过程中损失的信息尽可能少. 构造出的新特征个数比原来的特征数少很多，达到降维的目的. 总体主成分分析 定义\n设 $x = (x_1,x_2,\\cdots, x_m)^T$ 是 $m$ 维随机向量, $\\alpha \\in \\mathbb{R}^m$ 且 $\\alpha^T \\alpha = 1$, 则称:\n$$ y=\\alpha^T x $$为 标准线性组合.\n定义\n设 $\\mathbf{x}=(x_1,x_2,\\cdots,x_m)^T$ 是均值为 $\\mu$, 协方差矩阵为 $\\Sigma$ 的 $m$ 维随机向量, $A$ 是半正定矩阵 $\\Sigma$ 的对角化正交矩阵, 即 $A^T \\Sigma A = \\Lambda$. 则如下线性变换被称为 主成分变换:\n$$\\mathbf{y}=A^T(\\mathbf{x}-\\mu).$$并称 $\\mathbf{y}$ 的第 $i$ 个分量:\n$$y_i=\\alpha_i^T(\\mathbf{x}-\\mu)$$为 $\\mathbf{x}$ 的第 $i$ 主成分，这里 $\\alpha_i$ 为 $A$ 的第 $i$ 个列向量.\n定理\n设 $\\mathbf{x} \\sim (\\mu, \\Sigma)$, 则 $\\mathbf{y} = A^T (\\mathbf{x} - \\mu)$ 满足:\n$E[\\mathbf{y}] = 0$. $\\text{Var}(y_i) = \\lambda_i, i = 1, 2, \\cdots, m$. $\\text{Cov}(y_i, y_j) = 0, i \\neq j, i, j = 1, 2, \\cdots, m$. 证明略, 用协方差矩阵的性质即可.\n定理\n不存在方差比 $\\lambda_1$ 更大的标准线性组合 $y = \\alpha^T \\mathbf{x}$.\n证明\n考虑标准线性组合 $y=\\alpha^T\\mathbf{x}$, 其中 $\\alpha \\in \\mathbf{R}^m$ 且 $\\alpha^T\\alpha=1$. 由于 $\\alpha_1,\\alpha_2,\\cdots,\\alpha_m$ 正好构成了 $\\mathbf{R}^m$ 的一组标准正交基, 则存在 $c_1,c_2,\\cdots,c_m\\in\\mathbb{R}$ 使得\n$$\\alpha=\\sum_{i=1}^mc_i\\alpha_i$$对此线性组合来说，\n$$\\begin{aligned} \\text{Var}(y)=\\alpha^T\\Sigma\\alpha\u0026=\\left[\\sum_{i=1}^mc_i\\alpha_i^T\\right]\\Sigma\\left[\\sum_{i=1}^mc_i\\alpha_i\\right]\\\\ \u0026=\\sum_{i=1}^mc_i^2\\lambda_i\\alpha_i^T\\alpha_i=\\sum_{i=1}^mc_i^2\\lambda_i.\\end{aligned}$$另一方面结合:\n$$1=\\alpha^T\\alpha=\\sum_{i=1}^mc_i^2\\alpha_i^T\\alpha_i=\\sum_{i=1}^mc_i^2$$问题显然是 $c_1 = 1, c_2 = c_3 = \\cdots = c_m = 0$ 时取得最大值 $\\lambda_1$. 因此:\n$$\\max_{c_1,\\cdots,c_m}\\text{Var}(y)=\\lambda_1$$对应的标准线性组合为:\n$$y=\\alpha_1^T\\mathbf{x}$$ 定理\n如果标准线性组合 $y = \\alpha^T \\mathbf{x}$ 和 $\\mathbf{x}$ 的前 $k$ 个主成分都不相关, 则 $y$ 得到方差当 $y$ 是第 $k+1$ 个主成分时最大.\n证明\n证明：设 $y=\\alpha^T\\mathbf{x}$, 其中 $\\alpha^T\\alpha=1$. 且 $\\alpha=\\sum_i^mc_i\\alpha_i$, 对此线性组合来说:\n$$\\text{Var}(y)=\\sum_{i=1}^mc_i^2\\lambda_i$$对 $1\\leq j \\lt k$ 来说:\n$$ \\begin{aligned} \\text{Cov}(y,y_j)\u0026=\\text{Cov}(\\alpha^T\\mathbf{x},\\alpha_j^T\\mathbf{x})=\\left[\\sum_{i=1}^mc_i\\alpha_i^T\\right]\\Sigma\\alpha_j\\\\ \u0026=c_j\\lambda_j\\alpha_j^T\\alpha_j=c_j\\lambda_j=0 \\end{aligned} $$这意味着对$1\\leq j \\lt k$ 来说, $c_j^2\\lambda_j=0$. 故:\n$$\\text{Var}(y)=\\sum_{i=k+1}^mc_i^2\\lambda_i$$和前面证明类似, 我们可得:\n$$\\max_{c_1,\\cdots,c_m}\\text{Var}(y)=\\lambda_{k+1}$$对应的标准线性组合:\n$$y=\\alpha_{k+1}^T\\mathbf{x}$$正好是第 $k+1$ 主成分.\n定义\n$\\mathbf{x}$ 的第 $k$ 个主成分 $y_k$ 的 方差贡献率 定义为: $$ \\eta_k = \\frac{\\lambda_k}{\\sum_{i=1}^m \\lambda_i} $$$\\mathbf{x}$ 的前 $k$ 个主成分 $y_1, y_2, \\cdots, y_k$ 的 累计方差贡献率 定义为: $$ \\eta_{1 \\to k} = \\sum_{i=1}^k \\lambda_i= \\frac{\\sum_{i=1}^k\\lambda_k}{\\sum_{i=1}^m \\lambda_i} $$ 累计方差贡献率体现了前 $k$ 个主成分对数据的方差贡献.\n显然, $\\mathbf{y}=A^T\\mathbf{x}$ 的逆为 $\\mathbf{x}=A\\mathbf{y}$, 由此可以给出如下定义:\n定义\n定义 因子负荷量 为第 $k$ 个主成分 $y_k$ 和原始变量 $x_i$ 的相关系数:\n$$ \\begin{aligned} \\rho(y_k,x_i) \u0026= \\frac{\\text{Cov}(y_k, x_i)}{\\sqrt{\\text{Var}(y_k) \\cdot \\text{Var}(x_i)}} = \\frac{\\text{Cov}\\left(\\sum_{j=1}^m \\alpha_{ij}y_j, y_k\\right)}{\\sqrt{\\lambda_k \\sigma_{ii}}} \\\\ \u0026= \\frac{\\alpha_{ik} \\text{Var}(y_k) }{\\sqrt{\\lambda_k \\sigma_{ii}}} = \\frac{\\sqrt{\\lambda_k} \\alpha_{ik}}{\\sqrt{\\sigma_{ii}}} \\end{aligned} $$ 容易验证因子负荷量满足:\n$$ \\sum_{i=1}^m \\sigma_{ii} \\rho^2(y_k,x_i)=\\lambda_k $$ $$ \\sum_{k=1}^m \\rho^2(y_k,x_i)=1 $$ 定义\n$\\mathbf{x}$ 的前 $k$ 个主成分 $y_1, y_2, \\cdots, y_k$ 对原有变量 $x_i$ 的 贡献率 定义为: $$ \\nu_{1 \\to k} = \\sum_{j=1}^k \\rho^2(y_j, x_i) = \\sum_{j=1}^k \\frac{\\lambda_j \\alpha_{ij}^2}{\\sigma_{ii}} $$ 样本主成分分析 设 $\\mathbf{x}_1,\\mathbf{x}_2,\\cdots,\\mathbf{x}_n$ 是对 $m$ 维随机向量 $\\mathbf{x}=(x_1,x_2,\\cdots,x_m)^T$ 进行 $n$ 次独立观测的样本, 其中 $\\mathbf{x}_j=(x_{1j},x_{2j},\\cdots,x_{mj})^T$ 表示第 $j$ 个观测样本, 则观测数据矩阵:\n$$\\mathbf{X}=[\\mathbf{x}_1,\\mathbf{x}_2,\\cdots,\\mathbf{x}_n]=\\left[\\begin{array}{ccc}x_{11}\u0026\\cdots\u0026x_{1n}\\\\\\vdots\u0026\\vdots\u0026\\vdots\\\\x_{m1}\u0026\\cdots\u0026x_{mn}\\end{array}\\right]$$样本均值向量为:\n$$\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum\\limits_{j=1}^n\\mathbf{x}_j=(\\bar{x}_1,\\cdots,\\bar{x}_m)^T$$样本协方差矩阵为 $\\mathbf{S}=[s_{ij}]_{m\\times m}$, 其中:\n$$s_{ij}=\\frac{1}{n-1}\\sum\\limits_{k=1}^n(x_{ik}-\\bar{x}_i)(x_{jk}-\\bar{x}_j), i,j=1,2,\\cdots,m$$样本相关矩阵为 $\\mathbf{R}=[r_{ij}]_{m\\times m}$, 其中:\n$$r_{ij}=\\frac{s_{ij}}{\\sqrt{s_{ii}s_{jj}}}$$","date":"2025-05-13T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/pca/","title":"机器学习基础(11) —— 奇异值分解与主成分分析简介"},{"content":"C++11 特性 统一的初始化方法 1 2 3 4 int x{5}; int y{};// 默认初始化为 0（不同于 int y; 未初始化） int arr[3]{1, 2, 3}; vector\u0026lt;int\u0026gt; iv{1, 2, 3}; 这种方法可以防止缩窄类型转换.\n1 2 char c1 = 3.14e10 // ok char c2{3.14e10} // 编译错误 C++11 提供了模板类 initializer_list, 可将其用作构造函数的参数\n1 2 3 4 5 6 7 8 double SumByIntialList(std::initializer_list\u0026lt;double\u0026gt; il) { double sum = 0.0; for (auto p = il.begin(); p != il.end(); p++) { sum += *p; } return sum; } double total = SumByIntialList({ 2.5, 3.1, 4 }); MVP 问题 Most Vexing Parse(MVP) 是 C++ 中一个经典的语法歧义问题, 源于编译器将对象初始化语句错误解析为函数声明, 导致代码行为与预期不符. C++ 优先认为是函数声明而不是对象定义.\n1 2 class Timer { /* ... */ }; Timer t(); // 函数声明 auto 关键字 auto 关键字用于自动推导变量类型, 编译器根据初始化表达式的类型来确定变量的类型.\n1 2 3 4 5 6 auto x = 5; // x 的类型为 int auto p = new A() // p 的类型为 A* vector\u0026lt;int\u0026gt; v; for (auto it = v.begin(); it != v.end(); ++it) { cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // it 的类型为 vector\u0026lt;int\u0026gt;::iterator 函数的返回值类型也可以使用 auto 关键字来推导.\n1 2 3 4 5 6 7 8 9 A operator+(int n, const A \u0026amp;a) { return a; } template \u0026lt;class T1, class T2\u0026gt; auto add(T1 x, T2 y) -\u0026gt; decltype(x + y) { return x + y; } auto d = add(100, 1.5); // d 是 double d=101.5 auto k = add(100, A()); // d 是 A 类型 decltype 关键字 对于 decltype(e):\n如果 e 是一个没有带括号的标记符表达式或者类成员访问表达式, 那么的 decltype(e) 就是 e 所代表的实体的类型.如果没有这种类型或者 e 是一个被重载的函数, 则会导致编译错误. 如果 e 是一个函数调用或者一个重载操作符调用, 那么 decltype(e) 就是该函数的返回类型. 如果 e 不属于以上所述的情况, 则假设 e 的类型是 T: 当 e 是一个左值时, decltype(e) 就是 T\u0026amp;; 否则 (e 是一个右值), decltype(e) 是 T. 1 2 3 4 5 6 7 8 int i; double t; struct A { double x; }; const A* a = new A(); decltype(a) x1; // A* (规则 2) decltype(i) x2; // int (规则 1) decltype(a-\u0026gt;x) x3; // double (规则 1) decltype((a-\u0026gt;x)) x4 = t; // double\u0026amp; (规则 3) 基于范围的 for 循环 1 2 3 4 int ary[] = {1,2,3,4,5}; for (int \u0026amp;e: ary) {} vector\u0026lt;int\u0026gt; v = {1,2,3,4,5}; for (auto \u0026amp;it: v) {} lambda 表达式 格式:\n1 [capture] (parameters) -\u0026gt; return_type { body } -\u0026gt; return_type 也可以没有, 没有则编译器自动判断返回值类型.\n关于外部变量访问方式说明符:\n[] 不使用任何外部变量 [=] 以传值的形式使用所有外部变量 [\u0026amp;] 以引用形式使用所有外部变量 [x, \u0026amp;y] x 以传值形式使用, y 以引用形式使用 [=, \u0026amp;x, \u0026amp;y] x, y 以引用形式使用, 其余变量以传值形式使用 [\u0026amp;, x, y] x, y 以传值的形式使用, 其余变量以引用形式使用 1 2 int a[4]; sort(a, a + 4, [](int x, int y) -\u0026gt; bool { return x % 10 \u0026lt; y % 10; }); 1 2 3 function\u0026lt;int(int)\u0026gt; fib = [](int n) { return n \u0026lt;= 2 ? 1 : fib(n-1) + fib(n-2); }; // function\u0026lt;int(int)\u0026gt; 表示参数为一个 int, 返回值为 int 的函数 右值引用和 move 语义 一般来说, 不能取地址的表达式, 就是右值, 能取地址的 (代表一个在内存中占有确定位置的对象), 就是左值\n1 2 3 class A; A \u0026amp;r = A(); // error , A() 是无名变量, 是右值 A \u0026amp;\u0026amp;r = A(); //ok, r 是右值引用 主要目的是提高程序运行的效率, 减少需要进行深拷贝的对象进行深拷贝的次数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 String(String \u0026amp;\u0026amp;s): str(s.str) { // 将 s.str 的所有权转移到当前对象 cout \u0026lt;\u0026lt; \u0026#34;move constructor called\u0026#34; \u0026lt;\u0026lt; endl; s.str = new char[1]; s.str[0] = 0; } template\u0026lt;class T\u0026gt; void MoveSwap(T\u0026amp; a, T\u0026amp; b) { T tmp(move(a)); // std::move(a) 为右值, 这里会调用 move constructor a = move(b); // move(b) 为右值, 因此这里会调用 move assigment b = move(tmp); // move(tmp) 为右值, 因此这里会调用 move assigment } 移动规则:\n只写复制构造函数\nreturn 局部对象 -\u0026gt; 复制 return 全局对象 -\u0026gt; 复制 只写移动构造函数\nreturn 局部对象 -\u0026gt; 移动 return 全局对象 -\u0026gt; 默认复制 return move(全局对象) -\u0026gt; 移动 同时写复制构造函数和移动构造函数:\nreturn 局部对象 -\u0026gt; 移动 return 全局对象 -\u0026gt; 复制 return move(全局对象) -\u0026gt; 移动 可移动但不可复制:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 struct A{ A(const A \u0026amp;a) = delete; A(const A \u0026amp;\u0026amp; a) { cout \u0026lt;\u0026lt; \u0026#34;move\u0026#34; \u0026lt;\u0026lt; endl; } A() { }; }; A b; A func() { A a; return a; } void func2(A a) { } int main() { A a1; A a2(a1); // compile error func2(a1); // compile error func(); return 0; } 智能指针 shared_ptr 让 shared_ptr 对象托管一个 new 运算符返回的指针.\n1 2 #include \u0026lt;memory\u0026gt; // 头文件 std::shared_ptr\u0026lt;T\u0026gt; ptr(new T); 多个 shared_ptr 对象可以同时托管一个指针, 系统会维护一个托管计数. 当无 shared_ptr 托管该指针时, delete 该指针.\n1 2 3 4 5 shared_ptr\u0026lt;A\u0026gt; sp1(new A(2)); // sp1 托管 A(2) A* p = sp1.get(); // p 指向 A(2) shared_ptr\u0026lt;A\u0026gt; sp2(sp1); // sp2 也托管 A(2) shared_ptr\u0026lt;A\u0026gt; sp3 = sp1; //sp3 也托管 A(2) sp2.reset() // sp2 放弃对 A(2) 的托管, 变为 nullptr 创建 shared_ptr 对象时, 可以使用 make_shared 函数创建空对象. 添加托管时, 一定要用另一个已存在的 shared_ptr 对象来添加托管, 不能用原生指针.\n1 2 3 4 5 6 7 8 9 int main() { A* p = new A(); shared_ptr\u0026lt;A\u0026gt; ptr(p); shared_ptr\u0026lt;A\u0026gt; ptr2; ptr2.reset(p); // 并不增加 ptr 中对 p 的托管计数 cout \u0026lt;\u0026lt; \u0026#34;end\u0026#34; \u0026lt;\u0026lt; endl; return 0; // 程序会崩溃, A 对象会被 delete 两次 } 空指针 nullptr 1 2 3 4 5 6 7 8 9 10 11 12 13 int main() { int* p1 = NULL; int* p2 = nullptr; shared_ptr\u0026lt;double\u0026gt; p3 = nullptr; if (p1 == p2); // yes if (p3 == nullptr); // yes if (p3 == p2); // error if (p3 == NULL); // yes bool b = nullptr; // error bool b(nullptr); ok int i = nullptr; // error, nullptr 不能自动转换成整型 return 0; } 关于 nullptr 的定义:\n1 2 3 #define NULL ((void *)0) // C 语言 #define NULL 0 #define NULL nullptr // C++11 起 override 和 final 关键字 1 2 3 4 5 6 class Base { virtual void foo() final {} // 禁止子类重写 }; class Derived : public Base { void foo() override {} // 显式标记重写（编译时报错，因为基类已 final） }; 无序容器 (哈希表) 哈希表插入和查询的时间复杂度几乎是常数, 不过内存占用较高.\n1 2 3 4 5 #include \u0026lt;unordered_map\u0026gt; // 头文件 int main() { unordered_map\u0026lt;string, int\u0026gt; map; auto p = map.find(name); // p 是一个迭代器 } 正则表达式 1 2 3 4 5 6 #include \u0026lt;regex\u0026gt; // 头文件 int main() { regex reg(\u0026#34;b.?p.*k\u0026#34;); cout \u0026lt;\u0026lt; regex_match(\u0026#34;bopggk\u0026#34;,reg) \u0026lt;\u0026lt; endl; // 输出 1, 表示匹配成功 cout \u0026lt;\u0026lt; regex_match(\u0026#34;boopgggk\u0026#34;,reg) \u0026lt;\u0026lt; endl; // 输出 0, 匹配失败 } 多线程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #include \u0026lt;thread\u0026gt; // 头文件 struct MyThread { void operator()() { while(true) cout \u0026lt;\u0026lt; \u0026#34;IN MYTHREAD\\n\u0026#34;; } }; void my_thread(int x) { while(x) cout \u0026lt;\u0026lt; \u0026#34;in my_thread\\n\u0026#34;; } int main() { MyThread x; // 对 x 的要求: 可复制 thread th(x); // 创建线程并执行 thread th1(my_thread, 100); while(true) cout \u0026lt;\u0026lt; \u0026#34;in main\\n\u0026#34;; return 0; } C++14 特性 泛型 lambda lambda 参数支持 auto，实现泛型:\n1 2 3 auto print = [](const auto\u0026amp; x){ std::cout \u0026lt;\u0026lt; x; }; print(42); // int print(\u0026#34;hello\u0026#34;); // const char 二进制字面量和数字分隔符 提高可读性:\n1 2 int bin = 0b1100\u0026#39;1010; // 二进制表示 double pi = 3.1415\u0026#39;9265; 智能指针 unique_ptr 1 2 auto ptr = std::make_unique\u0026lt;MyClass\u0026gt;(42, \u0026#34;example\u0026#34;); auto arr = std::make_unique\u0026lt;int[]\u0026gt;(10); // 管理 10 个 int 的数组 C++17 特性 结构化绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct Point { int x; int y; }; int main() { // 结构体绑定 Point p{3, 4}; auto [a, b] = p; // 数组绑定 int arr[] = {5, 6}; auto\u0026amp; [c, d] = arr; c = 7; // arr[0] = 7 // 元组绑定 auto t = std::make_tuple(8, 9.5, \u0026#39;A\u0026#39;); auto [e, f, g] = t; // 引用语义 auto\u0026amp; [x, y] = p; x = 10; // p.x = 10 } if/switch 初始化语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if (auto it = m.find(key); it != m.end()) { // it 仅在此作用域有效 } switch (auto code = fetch_status(); code) { // code 仅在此作用域有效 case 200: std::cout \u0026lt;\u0026lt; \u0026#34;OK\u0026#34; \u0026lt;\u0026lt; endl; break; case 404: std::cout \u0026lt;\u0026lt; \u0026#34;Not Found\u0026#34; \u0026lt;\u0026lt; endl; break; default: std::cout \u0026lt;\u0026lt; \u0026#34;Unknown code: \u0026#34; \u0026lt;\u0026lt; code \u0026lt;\u0026lt; endl; } ","date":"2025-04-30T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/cpp-feature/","title":"程序设计实习(9) —— C++ 特性"},{"content":"概率近似正确 (PAC) 定义\n我们定义 泛化误差 为:\n$$ L_{\\mathcal{D},f}(h) = P_{X \\sim \\mathcal{\\mathcal{D}}}(h(X) \\neq f(X)) $$训练误差 为:\n$$ L_S(h) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I}(h(X_i) \\neq f(X_i)) $$ 定义\n如果存在 $h^*$ 使得对任意 $L_{\\mathcal{D},f}(h^*) = 0$, 则称为 $f,\\mathcal{D}$ 满足 可实现假设.\n可实现假设意味着对 $1$ 的概率, 满足 $L_S(h^*) = 0$, 且对每个经验风险最小化的假设 $h_S$ 有 $L_S(h_S) =0$.\n定理\n设 $\\mathcal{H}$ 是有限的假设空间, $\\delta \\in (0,1), \\epsilon\u003e0$, 设正整数 $m$ 满足:\n$$ m \\ge \\frac{\\log(|\\mathcal{H}|/\\delta)}{\\epsilon} $$对任意标签函数 $f$ 和任意分布 $\\mathcal{D}$, 如果可实现性假设相对于 $\\mathcal{H}, \\mathcal{\\mathcal{D}}, f$ 成立, 则在大小为 $m$ 的独立同分布样本 $S$ 的选择上有最低 $1-\\delta$ 的概率满足: 对每个经验风险最小化的假设 $h_S$ 有:\n$$ L_{\\mathcal{D},f}(h_S) \\leq \\epsilon $$证明\n令 $\\mathcal{H}_B$ 表示“坏”假设的集合, 即\n$$ \\mathcal{H}_B = \\{ h \\in \\mathcal{H} : L_{(\\mathcal{D}, f)}(h) \u003e \\epsilon \\} $$令 $S|_x = \\{ x_1, \\cdots, x_m \\}$ 表示训练集的实例, $M = \\{S|_x : \\exists h \\in \\mathcal{H}_B, L_S(h) = 0\\}$. 注意由可实现假设:\n$$ \\{S|_x : L_{(\\mathcal{D}, f)}(h_S) \u003e \\epsilon\\} \\subseteq M = \\bigcup_{h \\in \\mathcal{H}_B} \\{S|_x : L_S(h) = 0\\}. $$因此:\n$$ \\begin{aligned} \\mathcal{D}^m(\\{S|_x : L_{(\\mathcal{D}, f)}(h_S) \u003e \\epsilon\\}) \u0026\\leq \\mathcal{\\mathcal{D}}^m(M) = \\mathcal{\\mathcal{D}}^m\\left(\\bigcup_{h \\in \\mathcal{H}_B} \\{S|_x : L_S(h) = 0\\}\\right) \\\\ \u0026\\leq \\sum_{h \\in \\mathcal{H}_B} \\mathcal{\\mathcal{D}}^m(\\{S|_x : L_S(h) = 0\\}) \\\\ \u0026= \\sum_{h \\in \\mathcal{H}_B} \\prod_{i=1}^m \\mathcal{\\mathcal{D}}(\\{x_i : h(x_i) = f(x_i)\\}) \\end{aligned} $$注意到对于每个 $h \\in \\mathcal{H}_B$,\n$$ \\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\}) = 1 - L_{(\\mathcal{D}, f)}(h) \\leq 1 - \\epsilon $$代入上式, 再利用 $m$ 的定义可得:\n$$ \\mathcal{D}^m(\\{S|_x : L_{(\\mathcal{D}, f)}(h_S) \u003e \\epsilon\\}) \\leq |\\mathcal{H}_B| (1 - \\epsilon)^m \\le |\\mathcal{H}| e^{-\\epsilon m} \\le \\delta $$由此, 即 $1-\\mathcal{D}^m(\\{S|_x : L_{(\\mathcal{D}, f)}(h_S) \u003e \\epsilon\\}) \u003e 1-\\delta$, 得证.\n我们现在可以引入 PAC 可学习性的概念.\n定义\n称假设空间 $\\mathcal{H}$ 是 PAC 可学习的, 如果存在一个函数 $m_{\\mathcal{H}} : (0, 1)^2 \\to \\mathbb{N}$ 和一个学习算法, 满足以下性质: 对于任意 $\\delta, \\epsilon \\in (0, 1)$, 对于任意定义在 $\\mathcal{X}$ 上的分布 $\\mathcal{\\mathcal{D}}$, 以及对于任意标记函数 $f : \\mathcal{X} \\to \\{0, 1\\}$, 如果可实现性假设相对于 $\\mathcal{H}, \\mathcal{\\mathcal{D}}, f$ 成立, 那么当使用由 $\\mathcal{\\mathcal{D}}$ 生成的 $m \\geq m_{\\mathcal{H}}(\\epsilon, \\delta)$ 个独立同分布样本, 并用 $f$ 标记这些样本运行该算法时, 算法将返回一个假设 $h$, 使得在样本选择上以至少 $1 - \\delta$ 的概率满足\n$$ L_{(\\mathcal{\\mathcal{D}}, f)}(h) \\leq \\epsilon $$这里, $m$ 的大小称为 样本复杂度.\n由刚才的定理, 显然:\n$$ m_{\\mathcal{H}}(\\epsilon, \\delta) \\le \\left\\lceil\\frac{\\log(|\\mathcal{H}|/\\delta)}{\\epsilon}\\right\\rceil $$不可知 PAC 可学习性 实际中 PAC 可学习性的假设很强. 我们放宽可实现性假设.\nBayers 最优预测: 对于任意 $\\mathcal{X} \\times (0,1)$ 上的分布 $\\mathcal{\\mathcal{D}}$, 则最优预测是:\n$$ f_{\\mathcal{\\mathcal{D}}}(x) = \\begin{cases} 1 \u0026 \\text{if } P(y=1|x) \\ge \\frac{1}{2} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$但由于 $\\mathcal{\\mathcal{D}}$ 是未知的, 我们不能直接使用 $f_{\\mathcal{\\mathcal{D}}}$ 进行预测. 我们希望找一个预测函数使得损失不比 $f_{\\mathcal{\\mathcal{D}}}$ 大很多.\n定义\n称假设空间 $\\mathcal{H}$ 是 不可知 PAC 可学习的, 如果存在一个函数 $m_{\\mathcal{H}} : (0, 1)^2 \\to \\mathbb{N}$ 和一个学习算法, 满足以下性质: 对于任意 $\\delta, \\epsilon \\in (0, 1)$, 对于任意定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的分布 $\\mathcal{\\mathcal{D}}$, 当使用由 $\\mathcal{\\mathcal{D}}$ 生成的 $m \\geq m_{\\mathcal{H}}(\\epsilon, \\delta)$ 个独立同分布样本训练时, 算法将返回一个假设 $h$, 使得在样本选择上以至少 $1 - \\delta$ 的概率满足:\n$$ L_{\\mathcal{\\mathcal{D}}}(h) \\le \\min_{h' \\in \\mathcal{H}} L_{\\mathcal{\\mathcal{D}}}(h') + \\epsilon $$特别地, 我们称假设空间 $\\mathcal{H}$ 是关于集合 $Z$ 和损失函数 $\\ell: \\mathcal{H} \\times Z \\to \\mathbb{R}_+$ 不可知 PAC 可学习的, 如果在上述定义中 $\\mathcal{\\mathcal{D}}$ 是 $Z$ 上的分布, 且不等式中 $L_{\\mathcal{\\mathcal{D}}}(h) = \\mathbb{E}_{z \\sim \\mathcal{\\mathcal{D}}}[\\ell(h, z)]$.\n显然, 如果可实现性假设成立, 则不可知 PAC 可学习性转化为 PAC 可学习性.\n一致收敛 (UC) 定义\n训练集 $S$ 被称为关于域 $Z$, 假设空间 $\\mathcal{H}$, 损失函数 $\\ell$ 和分布 $\\mathcal{\\mathcal{D}}$ 是 $\\epsilon$-典型的, 如果\n$$ \\forall h \\in \\mathcal{H}, |L_S(h) - L_{\\mathcal{\\mathcal{D}}}(h)| \\leq \\epsilon $$ 定理\n假设训练集 $S$ 是 $\\epsilon/2$-典型的, 则对于任意 $ERM_\\mathcal{\\mathcal{H}}(S)$ 算法的输出, 即任意 $h_S \\in \\argmin_{h \\in \\mathcal{H}} L_S(h)$, 有:\n$$ L_{\\mathcal{\\mathcal{D}}}(h_S) \\leq \\min_{h \\in \\mathcal{H}} L_D(h) + \\epsilon $$证明\n利用定义可知\n$$ L_{\\mathcal{\\mathcal{D}}}(h_S) \\le L_S(h_S) + \\epsilon/2 \\le L_S(h) + \\epsilon/2 \\le L_{\\mathcal{\\mathcal{D}}}(h) + \\epsilon $$ 定义\n称假设空间 $\\mathcal{H}$ 关于域 $Z$, 损失函数 $\\ell$ 具有 一致收敛性, 如果存在一个函数 $m_{\\mathcal{H}}^{UC}: (0, 1)^2 \\to \\mathbb{N}$, 使得对于任意 $\\epsilon, \\delta \\in (0, 1)$ 和任意 $Z$ 上的分布 $\\mathcal{\\mathcal{D}}$, 如果 $S$ 是从 $\\mathcal{\\mathcal{D}}$ 中独立同分布抽取的大小为 $m \\geq m_{\\mathcal{H}}^{UC}(\\epsilon, \\delta)$ 的样本, 则以至少 $1 - \\delta$ 的概率, $S$ 是 $\\epsilon$-典型的.\n定理\n如果假设空间 $\\mathcal{H}$ 对于 $m_{\\mathcal{H}}^{UC}(\\epsilon, \\delta)$ 具有一致收敛性, 则 $\\mathcal{H}$ 是不可知 PAC 可学习的, 且样本复杂度满足:\n$$ m_{\\mathcal{H}}(\\epsilon, \\delta) \\leq m_{\\mathcal{H}}^{UC}(\\epsilon/2, \\delta) $$在这种情况下, $ERM_\\mathcal{H}(S)$ 算法是 $\\mathcal{H}$ 的不可知 PAC 学习算法.\n定理Hoeffding 不等式\n设 $\\theta_1, \\cdots, \\theta_m$ 是独立同分布随机变量, 且 $\\mathbb{E}[\\theta_i] = \\mu$, $P(\\theta_i \\in [a, b]) = 1$. 则对于任意 $\\epsilon \u003e 0$, 有:\n$$ P\\left(\\left|\\frac{1}{m} \\sum_{i=1}^m \\theta_i - \\mu\\right| \u003e \\epsilon\\right) \\leq 2 \\exp\\left(-\\frac{2m\\epsilon^2}{(b-a)^2}\\right) $$ 定理\n设 $\\mathcal{H}$ 是有限的假设空间, $Z$ 是一个域, $\\ell : \\mathcal{H} \\times Z \\to [0, 1]$ 是一个损失函数. 则 $\\mathcal{H}$ 具有一致收敛性, 且样本复杂度满足:\n$$ m_{\\mathcal{H}}^{UC}(\\epsilon, \\delta) \\leq \\left\\lceil \\frac{\\log(2|\\mathcal{H}|/\\delta)}{2\\epsilon^2} \\right\\rceil $$且此时 $\\mathcal{H}$ 是不可知 PAC 可学习的, 且样本复杂度满足:\n$$ m_{\\mathcal{H}}(\\epsilon, \\delta) \\leq m_{\\mathcal{H}}^{UC}(\\epsilon/2, \\delta) \\leq \\left\\lceil \\frac{2\\log(2|\\mathcal{H}|/\\delta)}{\\epsilon^2} \\right\\rceil $$证明\n固定 $\\epsilon, \\delta$, 我们要找 $m$ 使得对任意 $\\mathcal{\\mathcal{D}}$, 至少 $1 - \\delta$ 的概率, $S$ 是 $\\epsilon$-典型的. 即:\n$$ \\mathcal{\\mathcal{D}}^m(\\{ S: \\forall h \\in \\mathcal{H}, |L_S(h)-L_{\\mathcal{\\mathcal{D}}}(h)| \\le \\epsilon \\}) \\ge 1 - \\delta $$注意由 Hoeffding 不等式:\n$$ \\begin{aligned} \u0026\\mathcal{\\mathcal{D}}^m(\\{ S: \\forall h \\in \\mathcal{H}, |L_S(h)-L_{\\mathcal{\\mathcal{D}}}(h)| \u003e \\epsilon \\}) \\\\ \u0026 \\le \\sum_{h \\in \\mathcal{H}} \\mathcal{\\mathcal{D}}^m(\\{ S: |L_S(h)-L_{\\mathcal{\\mathcal{D}}}(h)| \u003e \\epsilon \\}) \\\\ \u0026 \\le \\sum_{h \\in \\mathcal{H}} 2 e^{-2m\\epsilon^2} = 2|\\mathcal{H}| e^{-2m\\epsilon^2} \\end{aligned} $$我们只要取:\n$$ m \\ge \\frac{\\log(2|\\mathcal{H}|/\\delta)}{2\\epsilon^2} $$即得:\n$$ \\mathcal{\\mathcal{D}}^m(\\{ S: \\forall h \\in \\mathcal{H}, |L_S(h)-L_{\\mathcal{\\mathcal{D}}}(h)| \u003e \\epsilon \\}) \\le \\delta $$从而得证.\n偏差复杂性分解 定理无免费午餐\n设 $A$ 是在域 $X$ 上的 $0-1$ 误差函数二分类学习算法, 训练集大小 $m$ 是小于 $|X|/2$ 的任意数. 则存在一个在 $X \\times \\{0, 1\\}$ 上的分布 $\\mathcal{\\mathcal{D}}$ 使得:\n存在一个函数 $f : X \\to \\{0, 1\\}$ 使得 $L_{\\mathcal{\\mathcal{D}}}(f) = 0$. 选择 $S \\sim \\mathcal{\\mathcal{D}}^m$ 时, 有至少 $1/7$ 的概率满足 $L_{\\mathcal{\\mathcal{D}}}(A(S)) \\geq 1/8$. 以此我们可以得到如下推论:\n定理\n设 $\\mathcal{X}$ 是一个无限域, $\\mathcal{H}$ 是从 $\\mathcal{X}$ 到 $\\{0, 1\\}$ 的所有函数的集合. 则 $\\mathcal{H}$ 不是 PAC 可学习的.\n证明\n假设 $\\mathcal{H}$ 是 PAC 可学习的, 选 $\\epsilon \u003c 1/8, \\delta \u003c 1/7$, 则存在一个算法 $A$ 和一个整数 $m=m_{\\mathcal{H}}(\\epsilon, \\delta)$, 对任意 $\\mathcal{X} \\times \\{0, 1\\}$ 上的分布 $\\mathcal{D}$, 如果对于某个函数 $f: \\mathcal{X} \\to \\{0, 1\\}$, $L_{\\mathcal{\\mathcal{D}}}(f) = 0$, 则当 $A$ 在 $S \\sim \\mathcal{\\mathcal{D}}^m$ 上运行时, 有至少 $1 - \\delta$ 的概率满足: $L_{\\mathcal{\\mathcal{D}}}(A(S)) \\leq \\epsilon$.\n但根据无免费午餐定理, 由于 $|X|\u003e2m$, 对于算法 $A$, 存在一个分布 $\\mathcal{\\mathcal{D}}$ 使得有至少 $1/7\u003e\\delta$ 的概率满足 $L_{\\mathcal{\\mathcal{D}}}(A(S)) \\geq 1/8\u003e\\epsilon$, 矛盾.\n误差分解:\n$$ L_{\\mathcal{\\mathcal{D}}}(h_S) = \\min_{h \\in \\mathcal{H}} L_{\\mathcal{\\mathcal{D}}}(h) + (L_{\\mathcal{\\mathcal{D}}}(h_S) - \\min_{h \\in \\mathcal{H}} L_{\\mathcal{\\mathcal{D}}}(h)) $$第一项称为 近似误差; 第二项称为 估计误差 $\\epsilon_{\\text{est}}$: 最小化风险和经验风险之间的差距.\n在有限假设情形下, $\\epsilon_{\\text{est}}$ 通常随 $|H|$ 增加, 随 $m$ 减小. 当 $\\mathcal{H}$ 很小时, 估计误差很小, 但近似误差可能很大, 是欠拟合; 当 $\\mathcal{H}$ 很大时, 近似误差很小, 但估计误差可能很大, 是过拟合.\n","date":"2025-04-29T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/pac-uc/","title":"机器学习基础(10) —— PAC 和 UC 可学习性"},{"content":"Markov 链 Markov 链是刻画随机变量序列的概率分布的模型.\n定义\n设 $\\{X_t\\mid t=1,2,\\cdots\\}$ 是随机序列, 若 $X_t$ 都在 $S$ 中取值, 则称 $S$ 是 $\\{X_t\\}$ 的状态空间, $S$中的元素称为 状态.\n如果对任何正整数 $t\\geq 2$ 和 $S$ 中的状态 $s_i,s_j,s_{i_1},s_{i_2},\\cdots,s_{i_{t-1}}$, 随机序列 $\\{X_t\\}$ 满足\n$$ P(X_{t+1}=s_j\\mid X_t=s_i,X_{t-1}=s_{i_{t-1}},\\cdots,X_1=s_{i_1}) \\\\ = P(X_{t+1}=s_j\\mid X_t=s_i) = P(X_2=s_j\\mid X_1=s_i) $$则称$\\{X_t\\}$为时齐的 Markov 链.\n我们称\n$$a_{ij} = P(X_2 = s_j | X_1 = s_i), s_i, s_j \\in S$$为 Markov 链 $\\{X_t\\}$ 的 转移概率. 称矩阵 $A = [a_{ij}]$ 为 Markov 链 $\\{X_t\\}$ 的 一步转移概率矩阵, 简称为 转移矩阵.\nMarkov 链的初始状态 $X_1$ 的分布称为 初始分布, 记为 $\\pi = (\\pi_1,\\pi_2,\\cdots,\\pi_N)$, 其中 $\\pi_i = P(X_1 = s_i)$.\n设 $|S| = N$, 则转移矩阵为 $N \\times N$ 矩阵, 且 $\\sum_{j=1}^N a_{ij} = 1$.\nMarkov 链的性质直观上可以理解为, 在时刻 $t$ 的状态只与时刻 $t-1$ 的状态有关, 与之前的状态无关. 也就是说, Markov 链具有 无记忆性.\n隐 Markov 模型 实际中, 我们往往无法直接观察到 Markov 链的状态, 而只能观察到与状态相关的观测值.\n隐 Markov 模型 (HMM) 刻画了首先由一个马尔可夫链随机生成不可观测的状态随机序列 $\\{X_t\\}$, 再由每个状态 $X_t$ 生成一个观测 $O_t$ 而生成观测随机序列 $\\{O_t\\}$ 的过程.\n设 观测概率 矩阵 $B = [b_{ij}]$, 其中 $b_{ij} = P(O_t = o_j | X_t = s_i)$.\n算法HMM\n输入: 隐 Markov 模型 $M = (A, B, \\pi)$, 其中 $A$ 是转移概率矩阵, $B$ 是观测概率矩阵, $\\pi$ 是初始分布.\n输出: 长度为 $T$ 的观测序列.\n令 $t=1$, 随机选择初始状态 $X_1$ 使得 $P(X_1 = s_i) = \\pi_i$. 根据状态 $X_t$ 和观测概率矩阵 $B$, 随机生成观测 $O_t$ 使得 $P(O_t = o_j | X_t = s_i) = b_{ij}$. 根据状态 $X_t$ 和转移概率矩阵 $A$, 随机选择下一个状态 $X_{t+1}$ 使得 $P(X_{t+1} = s_j | X_t = s_i) = a_{ij}$. 令 $t = t + 1$, 如果 $t \\leq T$, 则返回第 2 步, 否则停止. 返回观测序列 $\\mathbf{O} = (O_1, O_2, \\cdots, O_T)$. 概率计算方法 Markov 的第一个核心问题是概率计算问题: 给定 Markov 模型 $\\lambda = (A,B,\\pi)$, 计算 $p(\\mathbf{O} | \\lambda)$, 其中 $O=(O_1,O_2, \\cdots, O_T)$, 即计算给定模型时得到观测序列的概率.\n前向算法 我们定义前向概率:\n$$\\alpha_t(i) = p(O_1, O_2, \\cdots, O_t, X_t = s_i | \\lambda)$$显见 $\\alpha_T(i) = p(\\mathbf{O}, X_T = s_i | \\lambda)$, 因此 $p(\\mathbf{O} | \\lambda) = \\sum_{i=1}^N \\alpha_T(i)$. 对于首项:\n$$ \\begin{aligned} \\alpha_1(i) \u0026= p(O_1, X_1 = s_i | \\lambda) \\\\ \u0026= p(X_1 = s_i | \\lambda) p(O_1 | X_1 = s_i, \\lambda) \\\\ \u0026= \\pi_i b_i(O_1) \\end{aligned} $$推导递推式:\n$$ \\begin{aligned} \\alpha_{t+1}(i) \u0026= p(O_1, O_2, \\cdots, O_t, O_{t+1}, X_{t+1} = s_i | \\lambda) \\\\ \u0026= \\sum_{j=1}^N p(O_1, O_2, \\cdots, O_t, X_t = s_j, X_{t+1} = s_i | \\lambda) \\\\ \u0026= \\sum_{j=1}^N \\alpha_t(j) p(O_{t+1}|X_{t+1}=s_i, \\lambda) p(X_{t+1}=s_i|X_t=s_j, \\lambda) \\\\ \u0026= \\sum_{j=1}^N \\alpha_t(j)b_i(O_{t+1}) a_{ji} = \\left(\\sum_{j=1}^N a_{ji}\\alpha_t(j)\\right)b_i(O_{t+1}) \\\\ \\end{aligned} $$前向算法\n$$\\alpha_1(i) = \\pi_i b_i(O_1)$$$$\\alpha_{t+1}(i) = \\left(\\sum_{j=1}^N a_{ji}\\alpha_t(j)\\right)b_i(O_{t+1})$$$$p(\\mathbf{O} | \\lambda) = \\sum_{i=1}^N \\alpha_T(i)$$ 后向算法 我们定义后向概率:\n$$\\beta_t(i) = p(O_{t+1}, O_{t+2}, \\cdots, O_T | X_t = s_i, \\lambda)$$约定 $\\beta_T(i) = 1$. 仿照前向算法的思路推导即可.\n后向算法\n$$\\beta_T(i) = 1$$$$\\beta_t(i) = \\sum_{j=1}^N a_{ij} b_j(O_{t+1}) \\beta_{t+1}(j)$$$$p(\\mathbf{O} | \\lambda) = \\sum_{i=1}^N \\pi_i b_i(O_1) \\beta_1(i)$$ Viterbi 算法 Markov 的第二个核心问题是解码问题: 给定 Markov 模型 $\\lambda = (A,B,\\pi)$ 和观测序列 $O$, 计算最可能的状态序列 $X = \\{X_1, X_2, \\cdots, X_T\\}$. 即找:\n$$X^* = \\argmax_X p(X | \\mathbf{O}, \\lambda)$$也可以定义为:\n$$X^* = \\argmax_X p(X, O| \\lambda)$$Viterbi 算法是求解该问题的动态规划算法. 考虑时刻 $T$ 状态为 $s_i$ 的所有单个路径 $(X_1,X_2,\\cdots X_{T-1},X_T = s_i)$ 的概率最大值为\n$$ \\delta_{T}(i) = \\max_{X_{1},X_{2},\\cdots,X_{T-1}} P(X_{1},X_{2},\\cdots,X_{T-1},O_{1},O_{2},\\cdots,O_{T},X_{T}=s_{i}|\\lambda) $$对于最优路径 $X^*$, 即有:\n$$ P(X^*|\\mathbf{O},\\lambda) = \\max_{1 \\le i \\le N} \\delta_{T}(i), X_T^* = \\argmax_{1 \\le i \\le N} \\delta_{T}(i) $$特别地, $\\delta_1(i)=\\pi_i b_i(O_1)$. 既然要动态规划, 递推公式如下:\n$$ \\delta_t(i) = \\max_{1 \\le j \\le N} \\left( \\delta_{t-1}(j) a_{ji} \\right) b_i(O_t) $$动态规划还要记住路径, 用 $\\Psi_t(s_i)$ 记录时刻 $t$ 状态为 $s_i$ 的概率最大的路径的前一个状态, 即:\n$$ \\Psi_t(s_i) = \\argmax_{1 \\le j \\le N} \\left( \\delta_{t-1}(j) a_{ji} \\right) $$ 算法Viterbi\n输入: $\\lambda = (A,B,\\pi)$, 观测序列 $\\mathbf{O} = (O_1,O_2,\\cdots,O_T)$\n输出: 最优状态序列 $X^* = (X_1^*, X_2^*, \\cdots, X_T^*)$\n初始化 $\\delta_1(i) = \\pi_i b_i(O_1)$, $\\Psi_1(s_i) = 0$. 对于 $t=2,3,\\cdots,T$: $$ \\begin{aligned} \\delta_t(i) \u0026= \\max_{1 \\le j \\le N} \\left( \\delta_{t-1}(j) a_{ji} \\right) b_i(O_t) \\\\ \\Psi_t(s_i) \u0026= \\argmax_{1 \\le j \\le N} \\left( \\delta_{t-1}(j) a_{ji} \\right) \\end{aligned} $$ 选择最优路径: $$ \\begin{aligned} P^* \u0026= \\max_{1 \\le i \\le N} \\delta_T(i) \\\\ X_T^* \u0026= \\argmax_{1 \\le i \\le N} \\delta_T(i) \\end{aligned} $$ 从时间 $T$ 追溯历史: $$X_{t-1}^* = \\Psi_t(X_t^*)$$ 返回最优路径 $X^* = (X_1^*, X_2^*, \\cdots, X_T^*)$. Baum-Welch 算法 Markov 的第三个核心问题是学习问题: 给定观测序列 $O$ 和隐 Markov 模型 $\\lambda = (A,B,\\pi)$, 计算最优的模型参数使得似然 $p(O|\\lambda)$ 最大.\n如果 Markov 链是可观测的, 则可以直接用极大似然估计来估计参数. 如果隐藏, 可以用 EM 算法来估计参数. 我们依然沿用 EM 算法的思路:\n$$ Q(\\theta|\\theta^{(t)}) = \\sum_{Z} LL(\\theta|D,Z) p(Z|D,\\theta^{(t)}) $$用 $\\bar{\\lambda}$ 表示当前的参数, 则在 M 步中的 $Q$ 函数为:\n$$ \\begin{aligned} Q(\\lambda|\\bar{\\lambda}) \u0026= \\sum_{X} p(X|\\mathbf{O},\\bar{\\lambda}) \\log p(\\mathbf{O},X|\\lambda) \\\\ \u0026= \\frac{1}{p(O|\\bar{\\lambda})} \\sum_{X} p(\\mathbf{O},X|\\bar{\\lambda})\\log p(\\mathbf{O},X|\\lambda) \\end{aligned} $$忽略前面的常数项, Baum-Welch 直接定义 $Q$ 函数为:\n$$ Q(\\lambda|\\bar{\\lambda}) = \\sum_{X} p(X|\\mathbf{O},\\bar{\\lambda})\\log p(\\mathbf{O},X|\\lambda) $$如果我们记相应的隐状态序列为 $X = (X_1=s_{i_1}, X_2=s_{i_2}, \\cdots, X_T=s_{i_T})$, 则有:\n$$ P(\\mathbf{O},X|\\lambda) = \\pi_{i_1} b_{i_1}(O_1) \\prod_{t=1}^{T-1} a_{i_t i_{t+1}} b_{i_{t+1}}(O_{t+1}) $$代入有\n$$ \\begin{aligned} Q(\\lambda,\\bar{\\lambda}) \u0026 =\\sum_{X}p(\\mathbf{O},X|\\bar{\\lambda})\\log\\left[\\pi_{i_{1}}b_{i_{1}}(O_{1})\\prod_{t=1}^{T-1}a_{i_{t}i_{t+1}}b_{i_{t+1}}(O_{t+1})\\right] \\\\ \u0026=\\sum_{X}p(\\mathbf{O},X|\\bar{\\lambda})\\log\\pi_{i_{1}}+\\sum_{X}p(\\mathbf{O},X|\\bar{\\lambda})\\left[\\sum_{t=1}^{T-1}\\log a_{i_{t}i_{t+1}}\\right] +\\sum_X p(\\mathbf{O},X|\\bar{\\lambda})\\left[\\sum_{t=1}^T\\log b_{i_t}(O_t)\\right]. \\end{aligned} $$三个部分分别设为 $Q_1, Q_2, Q_3$.\n计算 Q1 $$ \\begin{aligned} Q_1 \u0026= \\sum_{i=1}^N \\sum_{X_1=s_i, X_2, \\cdots, X_T} p(\\mathbf{O},X|\\bar{\\lambda}) \\log \\pi_i \\\\ \u0026= \\sum_{i=1}^N p(\\mathbf{O},X_1=s_i|\\bar{\\lambda}) \\log \\pi_i \\\\ \\end{aligned} $$$\\pi_i$ 要满足 $\\sum_{i=1}^N \\pi_i = 1$, 因此可以用 Lagrange 乘子法来求解. 得到:\n$$ \\pi_i = \\frac{p(\\mathbf{O},X_1=s_i|\\bar{\\lambda})}{p(O|\\bar{\\lambda})} = p(X_1=s_i|\\mathbf{O},\\bar{\\lambda}) $$计算 Q2 类似 $Q_1$ 的处理手法:\n$$ Q_2 = \\sum_{i,j=1}^{N} \\sum_{t=1}^{T-1} p(\\mathbf{O}, X_t=s_i, X_{t+1}=s_j|\\bar{\\lambda}) \\log a_{ij} $$附加条件 $\\sum_{j=1}^N a_{ij} = 1$, Lagrange 乘子法求解, 得到:\n$$ a_{ij} = \\frac{\\sum_{t=1}^{T-1}P(X_t=s_i,X_{t+1}=s_j|\\mathbf{O},\\bar{\\lambda})}{\\sum_{t=1}^{T-1}P(X_t=s_i|\\mathbf{O},\\bar{\\lambda})} $$这里分子分母也同时除了 $p(O|\\bar{\\lambda})$.\n计算 Q3 仍然类似处理:\n$$ Q_{3}=\\sum_{j=1}^{N}\\sum_{t=1}^{T}P(\\mathbf{O},X_{t}=s_{j}|\\bar{\\lambda})\\log b_{j}(O_{t}) $$附加条件 $\\sum_{k=1}^M b_j(k) = 1$. 注意 $b_{j}(O_{t})$ 和 $b_{j}(t)$ 并不见得相同, 我们需要简单改写一下:\n$$ \\log b_j(O_t) = \\sum_{k=1}^M I(O_t=\\nu_k) \\log b_j(k) $$此时再用 Lagrange 乘子法求解, 得到:\n$$ b_{j}(k) = \\frac{\\sum_{t=1}^{T}P(X_{t}=s_{j}|\\mathbf{O},\\bar{\\lambda})I(O_{t}=\\nu_{k})}{\\sum_{t=1}^{T}P(X_{t}=s_{j}|\\mathbf{O},\\bar{\\lambda})} $$这里分子分母也同时除了 $p(O|\\bar{\\lambda})$.\n于是核心转化为了计算:\n$$ \\begin{aligned} \\gamma_t(i|\\lambda) \u0026= p(X_t=s_i|\\mathbf{O},\\bar{\\lambda})\\\\ \\xi(i,j|\\lambda)\u0026=p(X_t=s_i,X_{t+1}=s_j|\\mathbf{O},\\bar{\\lambda}) \\end{aligned} $$利用 Bayes 公式, 结合前向后向算法可得结果.\n算法Baum-Welch\n输入: 观测序列 $\\mathbf{O} = (O_1,O_2,\\cdots,O_T)$.\n输出: 隐 Markov 模型 $\\lambda = (A,B,\\pi)$.\n初始化 $\\lambda = (A,B,\\pi)$. 按概率计算方法计算前向概率 $\\alpha_t(i)$ 和后向概率 $\\beta_t(i)$. 计算以下参数: $$ \\begin{aligned} \\gamma_t(i|\\lambda) \u0026= \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j)\\beta_t(j)} \\\\ \\xi(i,j|\\lambda) \u0026= \\frac{\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_{t+1}(j)}{\\sum_{j=1}^N \\sum_{k=1}^N \\alpha_t(k)a_{kj}b_j(O_{t+1})\\beta_{t+1}(k)} \\\\ \\pi_i \u0026= \\gamma_1(i|\\lambda) \\\\ a_{ij} \u0026= \\frac{\\sum_{t=1}^{T-1} \\xi(i,j|\\lambda)}{\\sum_{t=1}^{T-1} \\gamma_t(i|\\lambda)} \\\\ b_{j}(k) \u0026= \\frac{\\sum_{t=1}^{T} \\gamma_t(j|\\lambda)I(O_t=\\nu_k)}{\\sum_{t=1}^{T} \\gamma_t(j|\\lambda)} \\end{aligned} $$ 得到新的参数 $\\lambda = (A,B,\\pi)$. 重复步骤 2-4 直到收敛. 返回隐 Markov 模型 $\\lambda = (A,B,\\pi)$. ","date":"2025-04-22T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/markov/","title":"机器学习基础(9) —— 隐 Markov 模型"},{"content":" 容器: 可以容纳各种数据类型的通用结构, 是类模板. 迭代器: 用于遍历容器的对象 算法: 用于操作容器的函数模板 容器 顺序容器: 元素不是排序的, 插入和删除元素的复杂度都是线性级别.\nvector: 动态数组, 元素在内存中是连续的, 随机存取任何元素都能在常数时间内完成, 在尾部插入和删除元素也能在常数时间内完成. deque: 双端队列, 元素在内存中是连续的, 在头尾都能高效地插入和删除元素, 但是随机存取元素的效率比 vector 差. list: 双向链表, 元素在内存中是不连续的, 可以在任意位置以常数时间插入和删除元素, 不支持随机存取. 关联容器: 元素是排序的, 插入任何元素都按照排序规则来确定位置, 插入和搜索的复杂度都是对数级别.\nset/multiset: 集合, set 元素是唯一的, 不允许重复; multiset 元素可以重复. map/multimap: 映射, map 的键是唯一的, 不允许重复; multimap 的键可以重复. 适配器 是指将不适用的序列式容器转换为适用的序列式容器, 即通过封装某个序列式容器, 使其具有另一种序列式容器的特性.\nstack: 栈, 只能在栈顶插入和删除元素, 先进后出. queue: 队列, 只能在队尾插入元素, 在队头删除元素, 先进先出. priority_queue: 优先队列, 最高优先级元素先出列. 对于顺序容器和关联容器, 都有:\nbegin end: 返回指向容器第一个元素的迭代器和指向容器最后一个元素的迭代器. rbegin rend: 返回指向容器最后一个元素的迭代器和指向容器第一个元素的迭代器. erase: 删除指定位置的元素. clear: 删除所有元素. 迭代器 用于指向顺序容器和关联容器中的元素, 迭代器用法和指针类似, 有 const 和非 const 两种.\n1 2 3 4 5 vector\u0026lt;int\u0026lt;v\u0026gt;\u0026gt;; vector\u0026lt;int\u0026gt;::const_iterator it; for (it = v.begin(); it != v.end(); ++it) { cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } 如果 p, p1 是双向迭代器, 则可以做:\n1 2 3 4 ++p, p++ // 指向下一个元素 --p, p-- // 指向上一个元素 p = p1 // 赋值 p == p1 // 判断是否相等 如果 p, p1 是随机访问迭代器, 则可以做:\n1 2 3 4 5 p += i // 向后移动 i 个元素 p -= i // 向前移动 i 个元素 p[i] // 第 i 个元素的引用 p \u0026lt; p1 // 判断大小 p - p1 // 计算距离 双向迭代器不支持比大小和 [] 运算符.\n容器 支持的迭代器 vector 随机访问 deque 随机访问 list 双向 set/multiset 双向 map/multimap 双向 stack 不支持 queue 不支持 priority_queue 不支持 算法 算法就是一个个函数模板, 大多数在\u0026lt;algorithm\u0026gt; 中定义.\n示例: find()\n1 2 template\u0026lt;class InIt, class T\u0026gt; InIt find(InIt first, InIt last, const T\u0026amp; val); first 和 last 是迭代器, val 是要查找的值, 返回一个迭代器指向找到的元素, 如果没有找到则返回 last.\n关联容器内部的元素是从小到大排序的, 称为 有序区间算法. 例如 binary_search; 有些算法会对区间进行从小到大排序, 称为 排序算法, 例如 sort.\n有时, \u0026ldquo;x 和 y 相等\u0026rdquo; 等价于 \u0026ldquo;x==y 为真\u0026rdquo;, 例如 find; 有时等价于 \u0026ldquo;x 小于 y 和 y 小于 x 同时为假\u0026rdquo;, 例如 binary_search.\n所有适用于 vector 的操作都适用于 deque. deque还有 push_front(将元素插入到前面) 和 pop_front (删除最前 面的元素) 操作, 复杂度是 O(1).\n对于 list, 除了具有所有顺序容器都有的成员函数以外, 还支持 8 个成员函数:\npush_front: 在头部插入元素 pop_front: 删除头部元素 sort: 排序 (list 不支持 STL 的 sort 函数) remove: 删除指定元素 unique: 删除相邻重复元素, 如果要求所有重复元素, 可以先排序, 然后调用 unique 函数. merge: 合并两个有序的 list, 操作后第二个容器变为空. reverse: 反转 splice: 在指定位置前面插入另一链表中的一个或多个元素, 并在另一链表中删除被插入的元素. 函数对象 是对象, 但是可以像函数一样使用, 即重载 operator(). STL 中有 equal_to, less, greater 等函数对象, 用于比较两个值的大小关系.\n1 2 3 4 5 6 template\u0026lt;class T\u0026gt; struct greater : public binary_function\u0026lt;T, T, bool\u0026gt; { bool operator()(const T\u0026amp; x, const T\u0026amp; y) const { return x \u0026gt; y; } }; list 有两个 sort 函数, 不带参数 sort 函数将 list 中的元素按 \u0026lt; 规定的比较方法升序排列. 还有带参数的版本:\n1 2 template \u0026lt;class T2\u0026gt; void sort(T2 op); 可以用 op 比较, 为 op(x, y) 为 true 则 x 在 y 前面.\n1 2 list\u0026lt;int\u0026gt; lst; lst.sort(greater\u0026lt;int\u0026gt;()); // greater\u0026lt;int\u0026gt;() 是个对象, 本句进行降序排序 比较规则:\n1 2 3 4 5 struct MyStruct { bool operator()(const T \u0026amp; a1,const T \u0026amp; a2) const { // 若 a1 应该在 a2 前面, 则返回 true; 否则返回 false. } } 比较规则返回 true, 意味着 a1 必须在 a2 前面. 返回 false, 意味着 a1 并非必须在 a2 前面. 排序规则的写法, 不能造成比较 a1,a2 返回 true, 比较 a2,a1 也返回 true, 否则会出问题, 比如 sort 会 runtime error. 比较 a1,a2 返回 false, 比较 a2,a1 也返回 false, 则没有问题. 集合和映射 set 和 multiset 内部元素有序排列, 新元素插入的位置取决于它的值, 查找速度快. 除了容器都有的函数之外, 还支持:\nfind: 查找等于某个值的元素 (x小于y和y小于x同时不成立即为相等) lower_bound: 查找某个下界 upper_bound: 查找某个上界 equal_range: 同时查找上界和下界 count: 计算等于某个值的元素个数 (x小于y和y小于x同时不成立即为相等) insert: 用以插入一个元素或一个区间 集合 1 2 3 4 template\u0026lt;class Key, class Pred = less\u0026lt;Key\u0026gt;, class A = allocator\u0026lt;Key\u0026gt;\u0026gt; class multiset { // ... }; Pred 类型的变量决定了 multiset 中的元素“一个比另一个小”是怎么定义的. multiset 运行过程中, 比较两个元素 x,y 的大小的做法, 就是生成一个 Pred 类型的变量, 假定为 op, 若表达式 op(x,y) 返回值为 true, 则 x 比 y 小. 缺省类型为 less.\n插入元素时, multiset 会将被插入元素和已有元素进行比较. 由于less 模板是用 \u0026lt; 进行比较的, 所以,这都要求对象能用 \u0026lt; 比较, 即适当重载了 \u0026lt;. 当然也可以自定义 Pred 类型.\n注意: multiset 集合元素不可修改 (返回的迭代器是 const 的), 只能添加和删除元素.\n1 2 3 4 5 template\u0026lt;class Key, class Pred = less\u0026lt;Key\u0026gt;, class A = allocator\u0026lt;Key\u0026gt; \u0026gt; class set { // ... } set 与 multiset 的区别在于, set 中的元素是唯一的, 不允许重复. 对于 insert, multiset 会返回一个迭代器指向新插入的元素, set 则会返回插入是否成功的布尔值.\n映射 1 2 3 4 5 template\u0026lt;class Key, class T, class Pred = less\u0026lt;Key\u0026gt;, class A = allocator\u0026lt;T\u0026gt;\u0026gt; class multimap { typedef pair\u0026lt;const Key, T\u0026gt; value_type; // ... }; multimap 中的元素由 \u0026lt;k, v\u0026gt; 组成, 每个元素是一个 pair 对象, 关键字就是 first 成员变量, 其类型是 Key. multimap 中允许多个元素的关键字相同. 元素按照 first 成员变量从小到大排列, 缺省情况下用 less\u0026lt;Key\u0026gt; 定义关键字的 \u0026lt; 关系. 拥有等价键的键值对的顺序就是插入顺序, 且不会更改. (C++11 起)\n1 2 3 4 5 template\u0026lt;class Key, class T, class Pred = less\u0026lt;Key\u0026gt;, class A = allocator\u0026lt;T\u0026gt;\u0026gt; class map { typedef pair\u0026lt;const Key, T\u0026gt; value_type; // ... }; map 中的元素都是 pair 模板类对象. 关键字 (first 成员变量) 各不相同. 元素按照关键字从小到大排列, 缺省情况下用 less\u0026lt;Key\u0026gt;. map 则要求关键字唯一, 不允许重复. map 还重载了 [] 运算符, 用于访问元素. map 的 [] 运算符返回一个引用, 如果关键字不存在, 则会插入一个新的元素, 用无参构造初始化.\n适配器 stack 是后进先出的的数据结构, 只能插入/删除/访问栈顶的元素. 可用 vector, list, deque 来实现. 缺省情况下, 用 deque 实现.\n1 2 template\u0026lt;class T, class Cont = deque\u0026lt;T\u0026gt;\u0026gt; class stack; queue 是先进先出的数据结构, 即 push 在队尾, pop 在队头. 可用 vector, list, deque 来实现. 缺省情况下, 用 deque 实现.\n1 2 template\u0026lt;class T, class Cont = deque\u0026lt;T\u0026gt;\u0026gt; class queue; priority_queue 是优先队列, 最高优先级元素先出列. 用堆排序技术实现, 即执行 pop 操作时, 删除的是最大的元素; 执行 top 操作时, 返回的是最大元素的常引用, 默认元素比较器是 less. 可用 vector, deque 来实现. 缺省情况下, 用 vector 实现.\n1 2 template \u0026lt;class T, class Container = vector\u0026lt;T\u0026gt;, class Compare = less\u0026lt;T\u0026gt; \u0026gt; class priority_queue; STL 算法 大多重载的算法都是有两个版本的, 其中一个是用 == 判断元素是否相等, 或用 \u0026lt; 来比较大小; 而另一个版本多出来一个类型参数 Pred, 以及函数形参 Pred op, 通过表达式 op(x,y) 的返回值是 true 还是 false来判断 x 是否“等于” y, 或者 x 是否“小于” y. 例如:\n1 2 iterate min_element(iterate first, iterate last); iterate min_element(iterate first, iterate last, Pred op); 不变序列算法 不修改容器或对象的算法, 适用于所有容器, 时间复杂度都是 $O(n)$.\nmin: 求两个对象中较小的 (可自定义比较器) max: 求两个对象中较大的 (可自定义比较器) min_element: 求区间中的最小值 (可自定义比较器) max_element: 求区间中的最大值 (可自定义比较器) for_each: 对区间中的每个元素都做某种操作 count: 计算区间中等于某值的元素个数 count_if: 计算区间中符合某种条件的元素个数 find: 在区间中查找等于某值的元素 find_if: 在区间中查找符合某条件的元素 find_end: 在区间中查找另一个区间最后一次出现的位置 (可自定义比较器) find_first_of: 在区间中查找第一个出现在另一个区间中的元素 (可自定义比较器) adjacent_find: 在区间中寻找第一次出现连续两个相等元素的位置 (可自定义比较器) search: 在区间中查找另一个区间第一次出现的位置 (可自定义比较器) search_n: 在区间中查找第一次出现等于某值的连续 n 个元素 (可自定义比较器) equal: 判断两区间是否相等 (可自定义比较器) mismatch: 逐个比较两个区间的元素, 返回第一次发生不相等的两个元素的位置 (可自定义比较器) lexicographical_compare: 按字典序比较两个区间的大小 (可自定义比较器) 变值算法 此类算法会修改源区间或目标区间元素的值. 值被修改的那个区间不可以是属于关联容器的.\nfor_each: 对区间中的每个元素都做某种操作 copy: 复制一个区间到别处 copy_backward: 复制一个区间到别处, 但目标区从后往前被修改的 transform: 将一个区间的元素变形后拷贝到另一个区间 swap_ranges: 交换两个区间内容 fill: 用某个值填充区间 fill_n: 用某个值替换区间中的 n 个元素 generate: 用某个操作的结果填充区间 generate_n: 用某个操作的结果替换区间中的 n 个元素 replace: 将区间中的某个值替换为另一个值 replace_if: 将区间中符合某种条件的值替换成另一个值 replace_copy: 将一个区间拷贝到另一个区间, 拷贝时某个值要换成新值拷过去 replace_copy_if: 将一个区间拷贝到另一个区间, 拷贝时符合某条件的值要换成新值拷过去 删除算法 删除算法会删除一个容器里的某些元素. 这里所说的“删除”, 并不会使容器里的元素减少, 其工作过程是：将所有应该被删除的元素看做空位子, 然后用留下的元素从后往前移, 依次去填空位子\nremove: 删除区间中等于某个值的元素 remove_if: 删除区间中满足某种条件的元素 remove_copy: 拷贝区间到另一个区间. 等于某个值的元素不拷贝 remove_copy_if: 拷贝区间到另一个区间. 符合某种条件的元素不拷贝 unique: 删除区间中连续相等的元素, 只留下一个 (可自定义比较器) unique_copy: 拷贝区间到另一个区间. 连续相等的元素, 只拷贝第一个到目标区间 (可自定义比较器) 变序算法 变序算法改变容器中元素的顺序, 但是不改变元素的值. 变序算法不适用于关联容器. 此类算法复杂度都是 $O(n)$ 的.\nreverse: 颠倒区间的前后次序 reverse_copy: 把一个区间颠倒后的结果拷贝到另一个区间, 源区间不变 rotate: 将区间进行循环左移 rotate_copy: 将区间以首尾相接的形式进行旋转后的结果拷贝到另一个区间, 源区间不变 next_permutation: 将区间改为下一个排列 (可自定义比较器) prev_permutation: 将区间改为上一个排列 (可自定义比较器) random_shuffle: 随机打乱区间内元素的顺序 partition: 把区间内满足某个条件的元素移到前面, 不满足该条件的移到后面 stable_patitio: 把区间内满足某个条件的元素移到前面, 不满足该条件的移到后面. 而且对这两部分元素, 分别保持它们原来的先后次序不变 排序算法 排序算法比前面的变序算法复杂度更高, 一般是 $O(n \\log n)$. 排序算法需要随机访问迭代器的支持, 因而不适用于关联容器和 list.\nsort: 将区间从小到大排序 (可自定义比较器) stable_sort: 将区间从小到大排序, 并保持相等元素间的相对次序 (可自定义比较器) partial_sort: 对区间部分排序, 直到最小的 n 个元素就位 (可自定义比较器) partial_sort_copy: 将区间前 n 个元素的排序结果拷贝到别处. 源区间不变 (可自定义比较器) nth_element: 对区间部分排序, 使得第 n 小的元素（n 从 0 开始算）就位, 而且比它小的都在它前面, 比它大的都在它后面 (可自定义比较器) make_heap: 使区间成为一个“堆” (可自定义比较器) push_heap: 将元素加入一个是“堆”区间 (可自定义比较器) pop_heap: 从“堆”区间删除堆顶元素 (可自定义比较器) sort_heap: 将一个“堆”区间进行排序, 排序结束后, 该区间就是普通的有序区间, 不再是“堆”了 (可自定义比较器) sort 实际上是快速排序, 平均性能最优. stable_sort 是归并排序, 能保证最差情况下的性能.\n有序区间算法 有序区间算法要求所操作的区间是已经从小到大排好序的, 而且需要随机访问迭代器的支持. 所以有序区间算法不能用于关联容器和 list.\nbinary_search: 判断区间中是否包含某个元素. includes: 判断是否一个区间中的每个元素, 都在另一个区间中. lower_bound: 查找第一个不小于某值的元素的位置. upper_bound: 查找第一个大于某值的元素的位置. equal_range: 同时获取 lower_bound 和 upper_bound. merge: 合并两个有序区间到第三个区间. set_union: 将两个有序区间的并拷贝到第三个区间 set_intersection: 将两个有序区间的交拷贝到第三个区间 set_difference: 将两个有序区间的差拷贝到第三个区间 set_symmetric_difference: 将两个有序区间的对称差拷贝到第三个区间 inplace_merge: 将两个连续的有序区间原地合并为一个有序区间 附注: STL 还提供了位图 bitset, 用于压缩存储.\n","date":"2025-04-16T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/stl/","title":"程序设计实习(8) —— 标准模板库"},{"content":"string 类是模板类:\n1 typedef basic_string\u0026lt;char\u0026gt; string 成员函数 构造函数:\n1 2 3 4 5 6 string s1(\u0026#34;Hello\u0026#34;); string month = \u0026#34;March\u0026#34;; string s2(8,\u0026#39;x\u0026#39;) string error1 = \u0026#39;c\u0026#39;; // 错 string error2(\u0026#39;u\u0026#39;); // 错 string error4(8); // 错 支持流插入/提取运算符, 支持 getline 函数.\n访问字符:\n1 2 s1[0] = \u0026#39;h\u0026#39;; s1.at(0) = \u0026#39;h\u0026#39;; at 函数会检查下标是否越界, 如果越界会抛出异常.\n复制:\n1 2 string s3 = s1; // 复制 s3.assign(s1); // 复制 比较: 用 ==, !=, \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;= 运算符.\n查找:\n1 2 3 s1.find(\u0026#39;l\u0026#39;); // 返回第一个 \u0026#39;l\u0026#39; 的下标 s1.find(\u0026#39;l\u0026#39;, 3); // 从下标 3 开始查找 s1.rfind(\u0026#39;l\u0026#39;); // 返回最后一个 \u0026#39;l\u0026#39; 的下标 如果没有找到, 返回 string::npos (即 -1).\n拼接:\n1 2 s1 += \u0026#34; world\u0026#34;; // 拼接 s1.append(\u0026#34; world\u0026#34;); // 拼接 删除:\n1 s1.erase(0, 2); // 删除下标 0 到 2 的字符 替换:\n1 s1.replace(2, 3, \u0026#34;haha\u0026#34;) // 从下标 2 开始的 3 个字符替换为 \u0026#34;haha\u0026#34; 插入:\n1 s1.inqsert(2, \u0026#34;abc\u0026#34;); // 在下标 2 处插入 \u0026#34;abc\u0026#34; 子串:\n1 s1.substr(2, 3); // 从下标 2 开始的 3 个字符 流处理: 类似 istream 和 osteram 进行标准流输入输出, 我们用 istringstream 和 ostringstream 进行字符串上的输入输出, 也称为内存输入输出.\n1 2 3 4 5 string input = \u0026#34;12 Hello\u0026#34;; istringstream iss(input); int a; string s; iss \u0026gt;\u0026gt; a \u0026gt;\u0026gt; s; // 读取整数和字符串 ","date":"2025-04-15T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/string/","title":"程序设计实习(7) —— string 类"},{"content":"函数模板 1 2 3 4 template \u0026lt;class T\u0026gt; int foo(T a) { // 函数体 } 函数模板也可以有不止一个类型参数.\n1 2 3 4 5 template \u0026lt;class T1, class T2\u0026gt; T2 print(T1 arg1, T2 arg2) { cout \u0026lt;\u0026lt; arg1 \u0026lt;\u0026lt; \u0026#34; \u0026#34;\u0026lt;\u0026lt; arg2\u0026lt;\u0026lt;endl; return arg2; } 也可以不通过参数实例化函数模板:\n1 2 3 4 5 6 7 8 template \u0026lt;class T\u0026gt; T Inc(T n) { return 1 + n; } int main() { cout \u0026lt;\u0026lt; Inc\u0026lt;double\u0026gt;(4) / 2; // 输出 2.5 return 0; } 函数模板可以重载，只要它们的形参表或类型参数表不同即可.\n1 2 3 4 5 6 7 8 template\u0026lt;class T1, class T2\u0026gt; void print(T1 arg1, T2 arg2); template\u0026lt;class T\u0026gt; void print(T arg1, T arg2); template\u0026lt;class T,class T2\u0026gt; void print(T arg1, T arg2); 注意: 匹配模板函数时不可以进行自动类型转换.\n1 2 3 4 5 6 template\u0026lt;class T\u0026gt; T myFunction(T arg1, T arg2); myFunction(5, 7); // ok: replace T with int myFunction(5.8, 8.4); // ok: replace T with double myFunction(5, 8.4); // error, no matching function 类模板 在定义类的时候，加上一个/多个类型参数. 在使用类模板时, 指定类型参数应该 如何替换成具体类型，编译器据此生成相应的模板类.\n1 2 3 4 template \u0026lt;class T\u0026gt; class MyClass { T data; } 编译器由类模板生成类的过程叫类模板的实例化. 由类模板实例化得到的类叫模板类. 同一个类模板的两个模板类是不兼容的.\n1 2 3 pair\u0026lt;string, int\u0026gt; *p; pair\u0026lt;string, double\u0026gt; a; p = \u0026amp;a; // wrong 类型参数表可以出现非类型参数.\n类模板与派生 类模板从类模板派生 类模板从模板类派生 类模板从普通类派生 普通类从模板类派生 类模板与友元 函数、类、类的成员函数作为类模板的友元 函数模板作为类模板的友元 函数模板作为类的友元 类模板作为类模板的友元 普通类从模板类派 需要注意, 如果手动实现函数的特化, 则不会成为友元.\n1 2 3 4 5 6 7 8 9 class A { int v; public: A(int n):v(n) { } template \u0026lt;class T\u0026gt; friend void Print(const T \u0026amp; p); }; template \u0026lt;class T\u0026gt; void Print(const T \u0026amp; p); void Print(int p); // 不是友元 类模板与静态成员 类模板中可以定义静态成员，那么从该类模板实例化得到的每个模板类，都有自己的类模 板静态数据成员，该模板类的所有对象，共享一个静态数据成员\n","date":"2025-04-09T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/template/","title":"程序设计实习(6) —— 模板"},{"content":"输入输出流 istream 是用于输入的流类, ostream 是用于输出的流类. ifstream 继承 自 istream, 用于从文件中读取数据; ofstream 继承自 ostream, 用于向文件中写入数据. fstream 继承自 iostream, 既可以用于输入也可以用于输出.\n输入流对象: cin 与标准输入设备相连, 对应于标准输入流, 用于从键盘读取数据, 也可以被重定向为从文件中读取数据. 输出流对象: cout 与标准输出设备相连, 对应于标准输出流, 用于向屏幕输出数据, 也可以被重定向为向文件写入数据. cerr 与标准错误输出设备相连. clog 与标准错误输出设备相连, 但是缓冲的, 用于输出调试信息. 持续读入直到结束:\n1 2 3 int x; while (cin \u0026gt;\u0026gt; x) { } 背后的原理是 istream 有一个 operator bool() 来判断.\n读取输入流:\n1 2 istream \u0026amp;getline(char *buf, int bufSize); istream \u0026amp;getline(char *buf, int bufSize, char delim); 从输入流中读取 bufSize - 1 个字符 (要给 '\\0' 留一个) 到缓冲区buf, 或读到碰到 \\n 或 delim 字符为止 (哪个先到算哪个).\n默认分隔符是换行符 \\n, 也可以指定其他分隔符. 会自动在 buf 中读入数据的结尾添加'\\0', '\\n' 或 delim 都不会被读入buf, 但会被从输入流中取走.\n需要注意, 如果 buf 不足以容纳读入的字符, 就导致读入出错, 其结果就是虽然本次读入已经完成, 但是之后的读入就都会失败了.\n重定向:\n1 2 freopen(\u0026#34;test.txt\u0026#34;, \u0026#34;w\u0026#34;, stdout); // 重定向标准输出到文件 freopen(\u0026#34;test.txt\u0026#34;, \u0026#34;r\u0026#34;, stdin); // 重定向标准输入到文件 流操纵算子 流操纵算子是一些函数, 需要 #include \u0026lt;iomanip\u0026gt;, 用于控制输入输出流的格式. 例如:\n整数流基数: dec (十进制), hex (十六进制), oct (八进制), setbase (设置基数);\n浮点数的精度: setprecision (设置精度) (precision 是成员函数);\n非定点模式下, setprecision 设置有效数字的位数; 定点模式下 (setiosflags(ios::fixed)), setprecision 设置小数点后面的有效位数; 域宽: setw (设置域宽) (width 是成员函数); 不够宽会用空格填充, 且宽度设置是一次性的.\n1 2 3 4 5 6 7 8 9 10 int main() { int w = 4; char string[10]; cin.width(5); while (cin \u0026gt;\u0026gt; string) { cout.width(w++); cout \u0026lt;\u0026lt; string \u0026lt;\u0026lt; endl; cin.width(5); } } 输入:\n1 1234567890 输出：\n1 2 3 1234 5678 90 用户也可以自定义流操纵算子, 例如:\n1 2 3 4 ostream \u0026amp;tab(ostream \u0026amp;output){ return output \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39;; } cout \u0026lt;\u0026lt; \u0026#34;aa\u0026#34; \u0026lt;\u0026lt; tab \u0026lt;\u0026lt; \u0026#34;bb\u0026#34; \u0026lt;\u0026lt; endl; 原理是 iostream 类中有一个 operator\u0026lt;\u0026lt; 函数\n1 ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;(*p)(ostream \u0026amp;)); 可以接收一个函数指针作为参数, 且函数内部把 this 对象传入这个函数, 这个函数返回一个 ostream 对象的引用.\n文件操作 1 2 #include \u0026lt;fstream\u0026gt; // 包含头文件 ofstream outFile(\u0026#34;clients.dat\u0026#34;, ios::out | ios::binary); // 创建 给出打开方式: ios::out 输出到文件，删除原有内容; ios::app 输出到文件，保留原有内容，总是在尾部添加.\n对于输入文件, 有一个读指针; 对于输出文件, 有一个写指针; 对于输入输出文件, 有一个读写指针; 标识文件操作的当前位置, 该指针在哪里, 读写操作就在哪里进行:\n1 2 3 4 5 6 7 ofstream fout(\u0026#34;a1.out\u0026#34;, ios::app); // 以添加方式打开 long location = fout.tellp(); // 取得写指针的位置 location = 10; fout.seekp(location); // 将写指针移动到第 10 个字节处 fout.seekp(location, ios::beg); // 从头数 location fout.seekp(location, ios::cur); // 从当前位置数 location fout.seekp(location, ios::end); // 从尾部数 location ","date":"2025-04-08T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/file-io/","title":"程序设计实习(5) —— 输入输出和文件操作"},{"content":"基于原型的聚类方法 与监督学习不同, 无监督学习基于数据集 $D=\\{x_i\\}_{i=1}^N$, 没有标签 $y_i$. 基于原型的方法通常假设数据内在的分布结构可以通过一组原型刻画, 先对原型初始化, 然后按照相应策略和准则进行迭代更新.\nK-means 聚类 算法K-means 聚类\n输入: 数据集 $D=\\{x_i\\}_{i=1}^N$, 聚类簇个数 $K$.\n输出: 簇划分 $\\mathcal{C}=\\{C_l\\}_{l=1}^K$.\n选择 $K$ 个样本点作为初始簇心 $\\mu_l$. 初始化 $C_l = \\emptyset$. 对每个 $x_i$, 求 $x_i$ 的簇标记 $\\lambda_i = \\argmin_j \\|x_i - \\mu_j\\|^2$, 即找到距离最近的簇心, 并将 $x_i$ 加入到 $C_{\\lambda_i}$. 对每个簇 $C_l$, 更新簇心 $\\mu_l = \\frac{1}{|C_l|} \\sum_{x_i \\in C_l} x_i$. 如果簇心不再变化, 则停止迭代, 否则返回第 2 步. 返回 $\\mathcal{C} = \\{C_l\\}_{l=1}^K$. 一般会基于不同的核心多次运行 K-means. 均值运算对于噪声和离群点非常敏感.\n还有一些变体, K-中心点方法通过挑选簇内相对处于最中心位置的一个实际样本点而非样本均值向量来作为簇心.\n用 $O_l$ 表示簇 $C_l$ 的簇心样本点, 用 $\\text{dist}(x_i, O_l)$ 表示样本点 $x_i$ 和 $O_l$ 的相异程度度量, 则 K-中心点方法相当于通过最小化绝对误差\n$$E = \\sum_{l=1}^{K} \\sum_{x \\in C_l} \\text{dist}(x, O_l)$$围绕中心点的划分算法 (PAM) 是一种典型的 K-中心点方法.\n算法PAM\n输入: 数据集 $D=\\{x_i\\}_{i=1}^N$, 聚类簇个数 $K$.\n输出: 簇划分 $\\mathcal{C}=\\{C_l\\}_{l=1}^K$.\n首先对每个簇的中心点进行随机初始化，并将非中心点的样本划分到簇心与其最相似的簇中，形成样本集的初始划分. 然后采用贪心策略，迭代更新划分，直到没有变化为止. 对当前的一个中心点 $o_l$, 随机选择一个非中心点样本 $x_i$, 评估以 $x_i$ 替代 $o_l$ 作为簇心能否得到更好的划分. 如果这种替代能得到更好的划分，则以 $x_i$ 作为簇 $C_l$ 的新中心点, 然后对当前的非中心点样本进行重新划分; 尝试这样所有可能的替换, 直到簇划分不再发生变化为止. PAM 算法使用中心点作为簇的原型表示，可以避免均值向量作为原型时易受离群点影响的问题.\nGauss 混合模型 定义\nGauss 混合模型 是指具有如下概率分布密度函数的模型:\n$$p(x|\\theta) = \\sum_{k=1}^K \\alpha_i p(x | \\mu_i, \\Sigma_i)$$其中:\n$\\alpha_i$ 是混合系数, 满足 $\\sum_{i=1}^K \\alpha_i = 1$; $p(x | \\mu_i, \\Sigma_i)$ 是 Gauss 分布, 其均值为 $\\mu_i$, 协方差矩阵为 $\\Sigma_i$, 即 $$p(x|\\mu_i, \\Sigma_i) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_i|}} \\exp\\left(-\\frac{1}{2}(x - \\mu_i)^T \\Sigma_i^{-1} (x - \\mu_i)\\right)$$ 给定样本集 $D=\\{x_i\\}_{i=1}^N$, 基于 Gauss 混合模型的聚类算法假定样本 $x_j$ 依据 Gauss 混合分布生成, 即先以概率 $\\alpha_i$ 选择一个高斯分布 $p(x | \\mu_i, \\Sigma_i)$, 然后从该高斯分布中生成样本 $x_j$.\n对 $x_j$, 设 $z_j$ 表示生成 $x_j$ 的分模型, 即 $p(z_j = i) = \\alpha_i$. 后验概率最大化\n$$ \\lambda_j = \\argmax_i p(z_j = i | x_j) $$由 Bayes 公式, 忽略相同的分母, 则\n$$ \\begin{aligned} \\lambda_j \u0026= \\argmax_i p(x_j | z_j = i) p(z_j = i) \\\\ \u0026= \\argmax_i p(x_j | \\mu_i, \\Sigma_i) \\alpha_i \\end{aligned} $$考虑对数似然函数\n$$ LL(\\theta | D) = \\sum_{j=1}^N \\log p(x_j | \\theta) = \\sum_{j=1}^N \\log \\left( \\sum_{i=1}^K \\alpha_i p(x_j | \\mu_i, \\Sigma_i) \\right) $$并不是很好求解. 我们引入隐变量 $z_{ji}$ 表示 $x_j$ 由第 $i$ 个高斯分布生成, 即\n$$ z_{ji} = \\begin{cases} 1, \u0026 \\text{if } z_j = i \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$则这样的对数似然函数可以写成\n$$ \\begin{aligned} LL(\\theta D|Z)\u0026=\\sum_{j=1}^N \\sum_{i=1}^K z_{ji} \\log \\left( \\alpha_i p(x_j | \\mu_i, \\Sigma_i) \\right) \\\\ \u0026=\\sum_{i=1}^K \\left( \\sum_{j=1}^N z_{ji} \\right) \\log \\alpha_i + \\sum_{i=1}^K \\sum_{j=1}^N z_{ji} \\log p(x_j | \\mu_i, \\Sigma_i) \\end{aligned} $$常采用 EM 算法迭代求解.\n算法EM\nE步, 求期望: 基于当前参数 $\\theta^{(t)}$, 计算对数似然函数关于 $Z$ 的期望: $$ Q \\left(\\theta | \\theta^{(t)}\\right) = \\mathbb{E}_{Z} \\left[ LL(\\theta | D, Z) | D, \\theta^{(t)} \\right] = \\sum_Z LL(\\theta | D, Z) p(Z | D, \\theta^{(t)}) $$ M步, 最大化: 通过最大化 $Q\\left(\\theta | \\theta^{(t)}\\right)$ 来更新参数 $\\theta$: $$ \\theta^{(t+1)} = \\argmax_{\\theta} Q\\left(\\theta | \\theta^{(t)}\\right) $$ 迭代直到收敛. 用 EM 算法估计参数:\n$$ LL(\\theta|D,Z)=\\sum_{i=1}^k\\left\\{\\left(\\sum_{j=1}^Nz_{ji}\\right)\\log\\alpha_i+\\sum_{j=1}^Nz_{ji}\\log p(x_j|\\mu_i,\\sigma_i^2)\\right\\} $$令 $n_i=\\sum_{j=1}^Nz_{ji}$, 则\n$$ \\begin{aligned} \u0026 LL(\\theta|D,Z)=\\sum_{i=1}^k\\left\\{n_i\\log\\alpha_i+\\sum_{j=1}^Nz_{ji}\\log p(x_j|\\mu_i,\\sigma_i^2)\\right\\} \\\\ \u0026 =\\sum_{i=1}^{k}\\left\\{n_{i}\\log\\alpha_{i}+\\sum_{j=1}^{N}z_{ji}\\left[\\log\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log\\sigma_{i}-\\frac{1}{2\\sigma_{i}^{2}}(x_{j}-\\mu_{i})^{2}\\right]\\right\\} \\end{aligned} $$我们考虑 $z_{ji}$ 期望:\n$$ \\begin{aligned} \\gamma_{ji}^{(t)} \u0026= E_{Z} \\left[ z_{ji} | D, \\theta^{(t)} \\right] = p\\left(z_{ji} = 1 | D, \\theta^{(t)}\\right) \\\\ \u0026= p\\left(z_j = i | D, \\theta^{(t)}\\right) = \\frac{\\alpha_i^{(t)} p\\left(x_j | \\mu_i^{(t)}, {\\sigma_i^2}^{(t)}\\right)}{\\sum_{l=1}^k \\alpha_i^{(t)} p\\left(x_j | \\mu_l^{(t)}, {\\sigma_l^2}^{(t)}\\right)} \\end{aligned} $$则对 $E$ 步, 有:\n$$ \\begin{aligned} Q\\left(\\theta |\\theta^{(t)}\\right) \u0026= E_Z \\left[ LL(\\theta | D, Z) | D, \\theta^{(t)} \\right] \\\\ \u0026= \\sum_{i=1}^k \\left\\{ \\sum_{j=1}^N \\gamma_{ji}^{(t)} \\log \\alpha_i + \\sum_{j=1}^N \\gamma_{ji}^{(t)} \\left[\\log\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log\\sigma_{i}-\\frac{1}{2\\sigma_{i}^{2}}(x_{j}-\\mu_{i})^{2}\\right]\\right\\} \\\\ \\end{aligned} $$既然要极大化 $Q\\left(\\theta |\\theta^{(t)}\\right)$, 那么我们可以对 $\\mu_i$, $\\sigma_i^2$ 分别求偏导数, 令其为 $0$. 分别得到:\n$$ \\begin{aligned} \\mu_i^{(t+1)} \u0026= \\frac{\\sum_{j=1}^N \\gamma_{ji}^{(t)} x_j}{\\sum_{j=1}^N \\gamma_{ji}^{(t)}} \\\\ {\\sigma_i^2}^{(t+1)} \u0026= \\frac{\\sum_{j=1}^N \\gamma_{ji}^{(t)} \\left(x_j - \\mu_i^{(t+1)}\\right)^2}{\\sum_{j=1}^N \\gamma_{ji}^{(t)}} \\end{aligned} $$注意 $\\alpha_i$ 还有约束 $\\sum_{i=1}^k \\alpha_i = 1$, 为此用 Lagrange 对偶, 令\n$$ L(\\theta, \\beta) = Q\\left(\\theta |\\theta^{(t)}\\right) + \\beta \\left(1 - \\sum_{i=1}^k \\alpha_i\\right) $$对 $\\alpha_i$ 求偏导数, 令其为 $0$, 可得:\n$$ n_i^{(t)} = \\beta \\alpha_i $$两边求和, 随后可以得出 $\\alpha$:\n$$ N = \\sum_{i=1}^k n_i^{(t)} = \\beta \\sum_{i=1}^k \\alpha_i = \\beta $$$$ \\alpha_i^{(t+1)} = \\frac{n_i^{(t)}}{\\beta} = \\frac{\\sum_{j=1}^N \\gamma_{ji}^{(t)}}{N} $$把这些综合起来, 就得到基于 Gauss 混合模型的 EM 算法 (GMM):\n算法GMM\n输入: 数据集 $D=\\{x_i\\}_{i=1}^N$, 聚类簇个数 $K$.\n输出: 簇划分 $\\mathcal{C}=\\{C_l\\}_{l=1}^K$.\n初始化参数 $\\theta =\\{\\alpha_i, \\mu_i, \\Sigma_i\\}_{i=1}^K, C_l = \\emptyset$. E步: 计算后验概率: $$ \\gamma_{ji} = p(z_j = i | x_j, \\theta) = \\frac{\\alpha_i p(x_j | \\mu_i, \\Sigma_i)}{\\sum_{l=1}^K \\alpha_l p(x_j | \\mu_l, \\Sigma_l)} $$ M步: 更新参数: $$ \\begin{aligned} \\mu_i \u0026= \\frac{\\sum_{j=1}^N \\gamma_{ji} x_j}{\\sum_{j=1}^N \\gamma_{ji}} \\\\ \\Sigma_i \u0026= \\frac{\\sum_{j=1}^N \\gamma_{ji} (x_j - \\mu_i)(x_j - \\mu_i)^T}{\\sum_{j=1}^N \\gamma_{ji}} \\\\ \\alpha_i \u0026= \\frac{\\sum_{j=1}^N \\gamma_{ji}}{N} \\end{aligned} $$ 重复步骤 2 和 3, 直到收敛. 对于每个 $x_j$, 求 $x_j$ 的簇标记: $$ \\lambda_j = \\argmax_i \\alpha_i p(x_j | \\mu_i, \\Sigma_i) $$ 并将 $x_j$ 加入到 $C_{\\lambda_j}$. 返回 $\\mathcal{C} = \\{C_l\\}_{l=1}^K$. 层次聚类算法 允许在聚类过程中对已有的簇进行合并或分裂, 通过对样本集不同层次的划分形成树状结构.\nAGNES 算法是自底向上的层次聚类算法, 其基本思想是从每个样本点开始, 逐步合并最相近的簇. 关于衡量簇之间的距离, 可以有很多定义, 例如最小距离, 最大距离, 平均距离, 质心距离, 中心距离等. 如果一个聚类算法分别选用最小距离/最大距离/平均距离作为两个簇的距离, 则相应的算法分别被称为单连接算法/全连接算法/均连接算法.\nAGNES 算法采用距离 (相异性) 矩阵来保存当前簇之间的距离:\n$$M(i,j)=d(C_i,C_j),\\quad i,j=1,2,\\cdots,N$$随着每次距离最近的两个簇的合并, 对距离矩阵也作相应的修正. 不妨设当前距离最近的两个聚类簇为 $C_i^*$ 和 $C_j^*$ 且 $i^*","date":"2025-04-08T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/clustering-intro/","title":"机器学习基础(8) —— 聚类简介"},{"content":"介绍 在论文 [1] Unknown-material 中, 作者首次介绍了 Adam 优化器. 此算法一经出现立刻爆火, 现在在深度学习当中已经成为一种最常用的优化算法.\n算法Adam\nAdam 的更新公式如下:\n$$ \\begin{aligned} m_t \u0026 = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \u0026 = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\ \\hat{m}_t \u0026 = \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v}_t \u0026 = \\frac{v_t}{1 - \\beta_2^t} \\\\ \\theta_t \u0026 = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t \\end{aligned} $$其中 $\\odot$ 表示逐元素相乘. 超参数通常取 $\\beta_1=0.9, \\beta_2=0.999, \\epsilon=10^{-8}$.\n可以认为 Adam 本身直接由 SGD 而来. 在此基础上 Adam 引入了几个重要技术:\n移动平均 (Moving Average): $m_t$ 不是通过对梯度直接求和, 而是按照 $\\beta_1$ 和 $\\beta_2$ 的比例进行移动平均, 保证了梯度的稳定性. 自适应学习率 (Adaptive Learning Rate): $v_t$ 通过对梯度的二阶矩进行估计, 使得学习率可以自适应地调整. 偏差修正 (Bias Correction): 由于在训练开始移动平均几乎为 0, 对其引入偏差修正可以加快初始化时刻的收敛速度. 当然还有加上衰减的 AdamW, 以及其他的变种, 以适应 Transformer 等模型的训练.\n收敛 \u0026hellip; 吗? 论文 [1] 中提到我们可以引入误差量来衡量收敛性:\n定义\n称 累积误差 为\n$$R(T) = \\sum_{t=1}^T (f_t(\\theta_t) - f_t(\\theta^*))$$其中 $\\theta^*$ 是最优解, 即 $\\theta^* = \\argmin_{\\theta} \\sum_{t=1}^T f_t(\\theta)$.\n可以认为, 当 $R(T)/T \\to 0$ 时算法收敛. 作者在文献中对 Adam 的收敛性给了自己证明, 里面的细节太多, 这里只给粗略过程.\n鉴于偏差修正只在初期有较大影响, 之后对于收敛性的讨论, 以下证明对其不予考虑 (原论文有), 此外忽略微小项 $\\epsilon$.\n在原始的 Adam 中 $\\beta_1, \\eta$ 都是常数, 实际上此时难以证明. 因此原文中, 对参数做了随时间动态调整:\n$$\\eta_t = \\frac{\\eta}{\\sqrt{t}}, \\beta_{1,t}=\\beta_1 \\lambda^{t-1}, \\lambda \\in (0,1)$$注意: 以下定理的证明有争议!\n定理\n假设 $f_t$ 梯度有界, $\\theta_t$ 之间的距离有界, 即 $\\| g_t \\|_{\\infty} \\le G, \\|\\theta_i-\\theta_j\\|_{\\infty} \\le D$, 且 $\\beta_1^4 \u003c \\beta_2$, Adam 中超参数 $\\eta, \\beta_1$ 遵从如上动态调整, 则 $R(T) \\le \\mathcal{O}(\\sqrt{T})$, 因而 Adam 收敛.\n证明\n首先, 可以证明:\n$$ f_t(\\theta_t) - f_t(\\theta^*) \\le g_t^T(\\theta_t - \\theta^*) = \\sum_{i=1}^d g_{t,i}(\\theta_{t,i} - \\theta^*_i) $$$d$ 个分量求和并不会影响量级, 从而我们只需要关心第 $i$ 个分量, 因而下面我们不妨设 $\\theta_t, g_t, m_t, v_t$ 等都是一维的. (或者可以用 $\\theta_t = \\theta_{t,i}$ 来表示), 那么我们只要证明:\n$$ \\sum_{t=1}^{T} g_t(\\theta_t - \\theta^*) \\le \\mathcal{O}(\\sqrt{T}) $$既然要估计 $\\theta_t - \\theta^*$, 由学习率公式, 我们可以得到:\n$$(\\theta_{t+1} - \\theta^*) = (\\theta_t - \\theta^*) - \\eta_t \\frac{m_t}{\\sqrt{v_t}}$$取平方, 有:\n$$(\\theta_{t+1} - \\theta^*)^2 = (\\theta_t - \\theta^*)^2 - 2\\eta_t \\frac{m_t}{\\sqrt{v_t}}(\\theta_t - \\theta^*) + \\eta_t^2 \\frac{m_t^2}{v_t}$$要把 $m_t$ 换成 $g_t$, 由 $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ 代入得到:\n$$(\\theta_{t+1} - \\theta^*)^2 = (\\theta_t - \\theta^*)^2 - 2\\eta_t \\frac{\\beta_{1,t} m_{t-1} + (1 - \\beta_{1,t}) g_t}{\\sqrt{v_t}}(\\theta_t - \\theta^*) + \\eta_t^2 \\frac{m_t^2}{v_t} $$把需要处理的量放在左边:\n$$ (1-\\beta_{1,t})g_t(\\theta_t - \\theta^*) = \\frac{\\sqrt{v_t}\\left((\\theta_t - \\theta^*)^2-(\\theta_{t+1} - \\theta^*)^2\\right)}{2\\eta_t} - \\beta_{1,t} m_{t-1}(\\theta_t - \\theta^*) + \\frac{\\eta_t m_t^2}{2\\sqrt{v_t}} $$这左边可以由 $1 \\ge 1-\\beta_{1,t} \\ge 1-\\beta_{1,1}$ 直接看作 $g_t(\\theta_t - \\theta^*)$ 量级, 右边现在已经分成三个部分了, 只需要累和来看每个部分的量级.\n一个显然的结论是, 在移动平均下, 易见 $m_t \\le G (m_0 \\le G), v_t \\le G^2 (v_0 \\le G^2)$.\n第一项: 忽略常数 $2$, 暂记 $\\gamma_t = \\frac{\\sqrt{v_t}}{\\eta_t} = \\mathcal{O}(\\sqrt{T})$, 只要考虑:\n$$ M_1=\\sum_{t=1}^{T} \\gamma_t \\left((\\theta_t - \\theta^*)^2-(\\theta_{t+1} - \\theta^*)^2\\right) $$利用 Abel 求和法则, 可以得到:\n$$ M_1 =\\gamma_1(\\theta_1 - \\theta^*)^2 - \\gamma_{t+1}(\\theta_{T+1} - \\theta^*)^2 + \\sum_{t=1}^{T} (\\gamma_{t+1} - \\gamma_t)(\\theta_t - \\theta^*)^2 $$一般来说 $\\gamma_t = \\mathcal{O}(\\sqrt{T})$ 应该是单调不减的, 在 $\\gamma_t \\le \\gamma_{t+1}$ 的情况下, 可以得到:\n$$ \\begin{aligned} M_1 \u0026\\le \\gamma_1(\\theta_1 - \\theta^*)^2 + \\sum_{t=1}^{T} (\\gamma_{t+1} - \\gamma_t)D^2 \\\\ \u0026= C + (\\gamma_{t+1} - \\gamma_1)D^2 = \\mathcal{O}(\\sqrt{T}) \\end{aligned} $$ 问题就出在这个 \u0026ldquo;一般来说\u0026rdquo; 上, 因为尽管引入参数衰减, 实际上 Adam 并不能保证 $\\gamma_t$ 是单调不减的. 后面会提到这里的争议.\n第二项: 直接放缩即可:\n$$ \\begin{aligned} M_2 \u0026=\\sum_{t=1}^{T} \\beta_{1,t}m_{t-1}(\\theta_t - \\theta^*) \\le \\sum_{t=1}^{T} \\beta_{1,t} |m_{t-1}| D \\\\ \u0026\\le GD \\sum_{t=1}^{T} \\beta_{1,t} = G D \\beta_1 \\frac{1-\\lambda^T}{1-\\lambda} = \\mathcal{O}(1) \\end{aligned} $$第三项: $v_t$ 未必有下界, 有点麻烦! 直接写通式:\n$$ \\begin{aligned} m_t \u0026= \\sum_{s=1}^t (1-\\beta_{1,s})\\left(\\prod_{r=s+1}^{t}\\beta_{1,r}\\right)g_s \\le \\sum_{s=1}^t \\beta_1^{t-s}g_s\\\\ v_t \u0026= (1-\\beta_2)\\sum_{s=1}^t \\beta_2^{t-s}g_s^2 \\end{aligned} $$既然要控制 $\\frac{m_t^2}{\\sqrt{v_t}}$, 结合 $\\beta_1^4 \u003c \\beta_2$, 那么考虑 Young 不等式:\n$$ \\begin{aligned} m_t^4 \u0026\\le \\left(\\sum_{s=1}^t \\beta_2^{t-s}g_s^2 \\right) \\left(\\sum_{s=1}^t \\frac{\\beta_1^{\\frac{4}{3}(t-s)}}{\\beta_2^{\\frac{1}{3}(t-s)}}g_s^{\\frac{2}{3}} \\right)^{3} \\\\ \u0026\\le v_t^2 G^2 \\left(\\frac{1-\\mu^t}{1-\\mu}\\right)^3 = v_t^2 \\mathcal{O}(1) \\end{aligned} $$这里 $\\mu = \\beta_1^{\\frac{4}{3}} / \\beta_2^{\\frac{1}{3}} \u003c 1$. 因此:\n$$ M_3 =\\sum_{t=1}^{T} \\eta_t^2 \\frac{m_t^2}{\\sqrt{v_t}} \\le C \\sum_{t=1}^{T} \\eta_t^2 = C \\sum_{t=1}^{T} \\mathcal{O}\\left(\\frac{1}{t}\\right) = \\mathcal{O}(\\ln T) $$自此, 三项综合可以得到:\n$$ R(T) \\le \\mathcal{O}(\\sqrt{T}) + \\mathcal{O}(1) + \\mathcal{O}(\\ln T) = \\mathcal{O}(\\sqrt{T}) $$ Objection! 论文 [1] 的这个漏洞显然为人诟病. 于是论文 [2] Unknown-material 中, 作者指出 Adam 并不总是收敛的. 论文中给出了一个反例:\n我们取 $\\beta_1=0, \\beta_2=\\frac{1}{1+C^2}$. 设考虑在时间 $t$ 观测到的函数 $f_t$ 为:\n$$ f_t(x) = \\begin{cases} Cx \u0026 t \\equiv 1 \\pmod {3} \\\\ -x \u0026 \\text{Otherwise} \\\\ \\end{cases}, \\quad x \\in [-1,1] $$其中 $C\u003e2$ 是一个常数. 从宏观尺度上, $f=\\frac{1}{3}(C-2)x$, 最低点在 $x=-1$ 处. 显然 $f$ 和其他超参数满足定理条件, 然而经过 (冗长枯燥的) 计算可以得知, 由于每三次迭代中 $f$ 就会有两次向错误的方向更新, 加上移动平均导致的历史遗忘, 会导致无法正确收敛. 具体过程可以参考原文附录.\n为了解决这个问题, 作者提出为了确保 $\\gamma$ 是单调不减的, 可以在更新时让 $v_{t+1}$ 与 $v_t$ 取一个最大值作为更新值. 由此, 引出 Amsgrad 算法:\n算法Amsgrad\nAmsgrad 的更新公式如下:\n$$ \\begin{aligned} m_t \u0026 = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \u0026 = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\ \\hat{v}_t \u0026= \\max(v_t, \\hat{v}_{t-1}) \\\\ \\theta_t \u0026 = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot m_t \\end{aligned} $$此处省略了偏差修正.\n论文 [2] 采取实验证明, Amsgrad 具有更好的收敛性.\n辩护与和解 既然 Adam 可能不收敛, 那为什么在实际中表现良好呢? 一个直观的批驳是, 论文 [2] 中的反例取的参数相当极端: $\\beta_1, \\beta_2$ 都很小. 在实际中, 我们通常取 $\\beta_1=0.9, \\beta_2=0.999$, 这便是论文 [3] Unknown-material 的切入点. 作者认为, 只要 $\\beta_1^2 \u003c \\beta_2$, 且 $\\beta_2$ 充分大, 就可以保证收敛.\n和 [2] 不同, 论文 [3] 更注重实际. 从训练角度上, 一般是分成若干个 Epoch, 每个 Epoch 内处理的都是相同的 $n$ 个函数. 作者对 $f$ 以及其内的 $n$ 个分函数提出了如下要求:\n定义\n定义集合 $\\mathcal{F}$ 为满足如下条件的函数 $f: \\mathbb{R}^d \\mapsto \\mathbb{R}$ 集合:\n$f$ 有界, 即 $$f \\le f^\\ast$$ 所有 $n$ 个时间刻的 $f_t$ 是 Lipschitz 连续的, 即 $$\\|f_t(x) - f_t(y)\\| \\le L \\|x - y\\|$$ 所有 $n$ 个时间刻的 $f_t$ 满足 $$\\sum_{i=0}^{n-1} \\| \\nabla f_i(x) \\|^2 \\le D_1 \\| \\nabla f(x) \\|^2 + D_0 $$ 作者称第三个要求实际上是非常容易的, 事实上确实如此.\n我们记第 $k$ 个 Epoch 开始时的变量为 $x_k$, 运行到第 $0 \\le i \u003c n$ 个函数时为 $x_{k,i}$, 即 $x_k=x_{k,0}=x_{k-1,n}$. 现在, 作者称梯度的上界有保证:\n定理\n设 $f(x) \\in \\mathcal{F}$, 遵守 $\\mathcal{F}$ 内定义中的记号. 设 $\\beta_1^2 \u003c \\beta_2 \u003c 1$, 且 $\\beta_2 \\ge \\gamma(n)$, 学习率衰减 $\\eta_k= \\frac{\\eta}{\\sqrt{nk}}$, 则存在某个充分大的 $K$, 对于 $T\u003eK$ 均有:\n$$ \\min_{k \\in [K,T]} \\mathbb{E} \\left\\{ \\min \\left[ \\sqrt{\\frac{2D_1d}{D_0}} \\| \\nabla f(x_k) \\|^2, \\| \\nabla f(x_k) \\| \\right] \\right\\} = \\mathcal{O} \\left( \\frac{\\log T}{\\sqrt{T}} + \\sqrt{D_0}\\right) $$证明\n我们先澄清, 由 $R(T)$ 中的证明, $m_t$ 和 $v_t$ 有上界, 且 $\\frac{m_t}{\\sqrt{v_t}}$ 也有上界 (证明依然类似, 把 Young 特化成 Cauchy 即可, 此时条件变为 $\\beta_1^2 \\le \\beta_2$).\n从下降引理出发:\n$$ \\mathbb{E}f(x_{k+1}) - \\mathbb{E}f(x_k) \\le \\mathbb{E} \\left\u003c \\nabla f(x_k), x_{k+1} - x_k \\right\u003e + \\frac{L}{2} \\mathbb{E} \\|x_{k+1} - x_k\\|^2 $$做累加, 有:\n$$ \\mathbb{E} \\sum_{k=t_0}^T \\left\u003c \\nabla f(x_k), x_k - x_{k+1} \\right\u003e \\le \\frac{L}{2} \\mathbb{E} \\sum_{k=t_0}^T \\|x_{k+1} - x_k\\|^2 + \\mathbb{E}f(x_{t_0}) - \\mathbb{E}f(x_{T+1}) $$首先考虑右侧的上界, 这个相对容易, 注意到由于 $m_k$, $v_k$ 都是常量级:\n$$ \\begin{aligned} \\mathbb{E} \\|x_{k+1} - x_k\\|^2 \u0026\\le \\mathbb{E} \\sum_{i=0}^{n-1} \\|x_{k,i+1} - x_{k,i}\\|^2 \\le n \\max_{0 \\le i \u003c n} \\mathbb{E} \\|x_{k,i+1} - x_{k,i}\\|^2 \\\\ \u0026= n\\mathcal{O}\\left(\\frac{\\eta_k^2m^2_{k,i}}{v_{k,i}}\\right) = \\mathcal{O}\\left(\\frac{1}{k}\\right) \\end{aligned} $$因此:\n$$ \\frac{L}{2} \\mathbb{E} \\sum_{k=t_0}^T \\|x_{k+1} - x_k\\|^2 \\le \\sum_{k=t_0}^T \\mathcal{O}\\left(\\frac{1}{k}\\right) = \\mathcal{O}(\\log T) $$关于左侧的下界, 按每一维展开:\n$$ \\begin{aligned} \\mathbb{E} \\left\u003c \\nabla f(x_k), x_k - x_{k+1} \\right\u003e \u0026= \\eta_k \\mathbb{E} \\left\u003c \\nabla f(x_k), \\sum_{i=0}^{n-1} \\frac{m_{k,i}}{\\sqrt{v_{k,i}}} \\right\u003e \\\\ \u0026= \\eta_k \\mathbb{E} \\sum_{l=1}^d \\sum_{i=0}^{n-1} \\partial_l f(x_k) \\frac{m_{k,i}^{(l)}}{\\sqrt{v_{k,i}^{(l)}}} \\end{aligned} $$ 以下推导长达数十页, 这里简要概括一下思路. 先考虑其中一维:\n$$ \\begin{aligned} \\mathbb{E} \\sum_{i=0}^{n-1} \\nabla f(x_k) \\frac{m_{k,i}}{\\sqrt{v_{k,i}}} \\end{aligned} $$其中 $x_k$ 实际上应该是 $x_k^{(l)}$, $\\nabla f(x_k)$ 实际上应该是 $\\partial_l f(x_k)$.\n显然一方面, 我们提过 $\\frac{m_{k,i}}{\\sqrt{v_{k,i}}}$ 是有界的, 由于 Lipschitz 连续, $\\nabla f(x_k)$ 也是有界的, 则这一项自然也是有界的, 此界与 $k$ 无关. 因此整个左侧为 $\\mathcal{O}(\\eta_k) = \\mathcal{O}(1/\\sqrt{k})$, 这个放缩是平凡的.\n另一方面, 感性上讲, $v_{k,i} \\approx C, m_{k,i} \\approx \\nabla f_i(x_k)$, 如果这个 $\\approx$ 成立, 显然就有左边 $\\ge 0$ 了, 这是我们的目标. 是时候做拆分了:\n$$ \\begin{aligned} \\sum_{i=0}^{n-1} \\nabla f(x_k) \\frac{m_{k,i}}{\\sqrt{v_{k,i}}} \u0026= \\sum_{i=0}^{n-1} \\left( \\nabla f(x_k) \\frac{m_{k,i}}{\\sqrt{v_{k,i}}}-\\nabla f(x_k) \\frac{\\nabla f_i(x_k)}{\\sqrt{v_{k,i}}} \\right) \\\\ \u0026+ \\sum_{i=0}^{n-1} \\nabla f(x_k) \\frac{\\nabla f_i(x_k)}{\\sqrt{v_{k,i}}} \\end{aligned} $$现在作者称: 两项的 $v_{k,i}$ 均可以换成 $v_{k,0}$, 且不影响量级, 证明太过繁杂这里省略. 我们转而考虑:\n$$ \\begin{aligned} \\sum_{i=0}^{n-1} \\nabla f(x_k) \\frac{m_{k,i}}{\\sqrt{v_{k,i}}} \u0026\\approx \\sum_{i=0}^{n-1} \\left( \\nabla f(x_k) \\frac{m_{k,i}}{\\sqrt{v_{k,0}}}-\\nabla f(x_k) \\frac{\\nabla f_i(x_k)}{\\sqrt{v_{k,0}}} \\right) \\\\ \u0026+ \\sum_{i=0}^{n-1} \\nabla f(x_k) \\frac{\\nabla f_i(x_k)}{\\sqrt{v_{k,0}}} \\\\ \\end{aligned} $$第二项正是我们的目标:\n$$M_2 = \\frac{\\nabla f(x_k)}{\\sqrt{v_{k,0}}}\\sum_{i=0}^{n-1} \\nabla f_i(x_k) = \\frac{\\| \\nabla f(x_k)^2 \\|}{\\sqrt{v_{k,0}}} \\ge 0$$现在全力以赴地考虑第一项, 这是证明的核心, 证明也很繁杂, 涉及大量计算以及对梯度范围的讨论, 这里省略. 最后合并处理得到结论.\n强调一点, 所谓 $\\beta_2$ 充分大, 是指 $\\beta_2$ 需要大于一个与 $n$ 有关的常数 $\\gamma(n)$, 如果 $n$ 不固定, 定理并不能证明什么. 因此, 作者与 [2] 的结论达成了和解, 它们并不矛盾. 在 [3] 中, 作者给出了关于收敛的结论:\n首先, 对于每个函数, 只要满足 $\\beta_1^2\u003c\\beta_2$, 且 $\\beta_2$ 充分大, 则当 $T \\to \\infty$ 时, 右侧趋于 $\\sqrt{D_0}$, 这意味着收敛到一个范围内的点. 特别地, 如果 $D_0=0$, 右侧趋于 $0$, 这意味着收敛到最优点. 这与我们在实际中观察到的现象一致. 作者还给出了一块 \u0026ldquo;危险区域\u0026rdquo;, 只要 $(\\beta_1, \\beta_2)$ 落在这个区域内, 就会导致 Adam 不收敛. 除此之外的区域, 仍是未知的.\n如果超参数先于函数取值, 注意此时 $n$ 可变. 无论 $(\\beta_1, \\beta_2)$ 取什么值, 都存在一个函数, 使得 Adam 不收敛. 作者给出的反例是:\n$$ \\begin{aligned} f_0(x) \u0026= \\begin{cases} nx, \u0026 x \\ge -1 \\\\ \\frac{n}{2}(x+2)^2 - \\frac{3n}{2}, \u0026 x \u003c -1 \\end{cases} \\\\ f_i(x) \u0026= \\begin{cases} x, \u0026 x \\ge -1 \\\\ \\frac{1}{2}(x+2)^2 - \\frac{3}{2}, \u0026 x \u003c -1 \\end{cases} \\\\ \\end{aligned} $$在 $n \\to \\infty$ 时, 不收敛的 \u0026ldquo;危险区域\u0026rdquo; 会变得越来越大, 直到盖住 $(\\beta_1, \\beta_2)$ 这个点. 这个反例显然要比论文 [2] 中的更加深刻.\n如果超参数在函数之后取值. 前面提到只要条件保证成立, 就可以保证收敛到最优局部或最优点.\n显然, 日常训练中的情形更符合第二种, 我们都是先定义好损失函数, 然后再选择超参数, 因此这也就解释了为什么 Adam 在绝大多数训练中表现良好, 重点在于我们选取的超参数符合定理的要求.\n应用: 另一个算法 遇到了一个基于 Nestorov 等价形式的算法, 和 Adam 非常相似, 于是想能不能直接推广证明, 方便起见暂时叫它 C\u0026rsquo;Adam. 更新公式如下:\n算法C\u0026#39;Adam\n$$ \\begin{aligned} m_t \u0026= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\ v_t \u0026= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\ \\theta_{t+1} \u0026= \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\odot (\\beta_1 m_t + g_t) \\end{aligned} $$ 忽略偏差修正, 可以看到和 Adam 只有最后一行不同. 我尝试着给出一份证收敛性证明. 以下仍然简记 $x_k = x_{k,0}$.\n定理\n设 $f(x) = \\sum_{i=0}^{n-1} f_i$ 有界, 且任意时刻 $f_t$ 满足 $L$- Lipschitz 连续, 满足增长性条件 $\\sum_{i=0}^{n-1} \\| \\nabla f_i(x) \\|^2 \\le D_1 \\| \\nabla f(x) \\| ^2 + D_0$ . 设 $\\beta_1^2 \u003c \\beta_2 \u003c 1$, 且 $\\beta_2 \\ge \\gamma(n)$, 学习率衰减 $\\eta_k= \\frac{\\eta}{\\sqrt{nk}}$, 则存在某个充分大的 $K$, 对于 $T\u003eK$ 均有:\n$$ \\min_{k \\in [K,T]} \\mathbb{E} \\left\\{ \\min \\left[ \\sqrt{\\frac{2D_1d}{D_0}} \\| \\nabla f(x_k) \\|^2, \\| \\nabla f(x_k) \\| \\right] \\right\\} = \\mathcal{O} \\left( \\frac{\\log T}{\\sqrt{T}} + \\sqrt{D_0}\\right) $$证明\n从下降引理出发:\n$$ \\mathbb{E}f(x_{k+1}) - \\mathbb{E}f(x_k) \\le \\mathbb{E} \\left\u003c \\nabla f(x_k), x_{k+1} - x_k \\right\u003e + \\frac{L}{2} \\mathbb{E} \\|x_{k+1} - x_k\\|^2 $$做累加, 有:\n$$ \\mathbb{E} \\sum_{k=t_0}^T \\left\u003c \\nabla f(x_k), x_k - x_{k+1} \\right\u003e \\le \\frac{L}{2} \\mathbb{E} \\sum_{k=t_0}^T \\|x_{k+1} - x_k\\|^2 + \\mathbb{E}f(x_{t_0}) - \\mathbb{E}f(x_{T+1}) $$考虑右侧, 后两项是常数级别, 只需要考虑第一项:\n$$ \\begin{aligned} \\mathbb{E} \\|x_{k+1} - x_k\\|^2 \u0026\\le \\mathbb{E} \\sum_{i=0}^{n-1} \\|x_{k,i+1} - x_{k,i}\\|^2 \\le n \\max_{0 \\le i \u003c n} \\mathbb{E} \\|x_{k,i+1} - x_{k,i}\\|^2 \\\\ \u0026\\le n \\mathcal{O}\\left(\\frac{\\eta_k^2m^2_{k,i}}{v_{k,i}}\\right) = \\mathcal{O}\\left(\\frac{1}{k}\\right) \\end{aligned} $$这里用到 $\\eta_k = \\mathcal{O}(1/\\sqrt{k})$, 而对于 $m^2_{k,i}/{v_{k,i}}$, 论文已经证明其为有界.\n从而右边:\n$$ RHS \\le \\sum_{k=t_0}^T \\mathcal{O}\\left(\\frac{1}{k}\\right) + C = \\mathcal{O}(\\log T) + \\mathcal{O}(1) $$现在考察左侧, 右上角标 $(l)$ 表示第 $l$ 个分量:\n$$ \\begin{aligned} \\mathbb{E} \\left\u003c \\nabla f(x_k), x_k - x_{k+1} \\right\u003e \u0026= \\eta_k \\mathbb{E} \\left\u003c \\nabla f(x_k), \\sum_{i=0}^{n-1} \\frac{\\beta_1 m_{k,i}+ g_{k,i}}{\\sqrt{v_{k,i}}} \\right\u003e \\\\ \u0026= \\eta_k \\mathbb{E} \\sum_{l=1}^d \\sum_{i=0}^{n-1} \\partial_l f(x_k) \\left( \\frac{\\beta_1 m_{k,i}^{(l)} + \\partial_l f_i(x_k)}{\\sqrt{v_{k,i}^{(l)}}}\\right) \\end{aligned} $$按分子上的加号拆成两部分. 对于前者, 论文中 (35) 式已经证明了:\n$$ \\begin{aligned} \\mathbb{E} \\sum_{l=1}^d \\sum_{i=0}^{n-1} \\partial_l f(x_k) \\frac{m_{k,i}^{(l)}}{\\sqrt{v_{k,i}^{(l)}}} \\ge \u0026\\frac{1}{d \\sqrt{10D_1d}} \\mathbb{E} \\min \\left[ \\sqrt{\\frac{2D_1d}{D_0}} \\| \\nabla f(x_k) \\|^2, \\| \\nabla f(x_k) \\| \\right] \\\\ \u0026 - \\mathcal{O}\\left(\\frac{1}{\\sqrt{k}}\\right) - \\mathcal{O}\\left(\\sqrt{D_0}\\right) \\end{aligned} $$对于后者, 再拆分:\n$$ \\begin{aligned} \\mathbb{E} \\sum_{l=1}^d \\sum_{i=0}^{n-1} \\partial_l f(x_k) \\frac{\\partial_l f_i(x_k)}{\\sqrt{v_{k,i}^{(l)}}} = \\mathbb{E} \\sum_{l=1}^d \\sum_{i=0}^{n-1} \\frac{\\partial_l^2 f(x_k)}{\\sqrt{v_{k,i}^{(l)}}} + \\mathbb{E} \\sum_{l=1}^d \\sum_{i=0}^{n-1} \\partial_l f(x_k) \\frac{\\partial_l f_i(x_k) - \\partial_l f(x_k)}{\\sqrt{v_{k,i}^{(l)}}} \\end{aligned} $$前一项显然非负, 后一项即论文中的 $(a_2)$项, 其在引理 G.5 已经证明当 $\\beta_2 \\to 1$ 时, 此项趋于 $0$.\n我们综合关于左侧的讨论, 即有:\n$$ LHS \\ge \\beta_1 \\sum_{k=t_0}^T \\eta_k \\left( \\frac{1}{d \\sqrt{10D_1d}} \\mathbb{E} \\min \\left[ \\sqrt{\\frac{2D_1d}{D_0}} \\| \\nabla f(x_k) \\|^2, \\| \\nabla f(x_k) \\| \\right] - \\mathcal{O}\\left(\\frac{1}{\\sqrt{k}}\\right) - \\mathcal{O}\\left(\\sqrt{D_0}\\right) \\right) $$又 $\\eta_k = \\mathcal{O}(1/\\sqrt{k})$, 则 $\\sum_{k=t_0}^T \\eta_k = \\mathcal{O}(\\sqrt{T})$, 忽略所有低阶项, 联合左右两端, 我们得到:\n$$ \\mathcal{O} \\left(\\sqrt{T}\\right) \\left( \\frac{1}{d \\sqrt{10D_1d}} \\mathbb{E} \\min \\left[ \\sqrt{\\frac{2D_1d}{D_0}} \\| \\nabla f(x_k) \\|^2, \\| \\nabla f(x_k) \\| \\right] - \\mathcal{O} \\left(\\sqrt{D_0}\\right) \\right) \\le \\mathcal{O}(\\log T) $$由此移项即得证.\n总结 Adam 优化器的收敛性问题, 经过多篇论文的讨论, 目前已经有了比较清晰的结论. 论文 [1] 中的定理是有漏洞的, 但在实际中, Adam 的表现依然良好. 尽管存在着不收敛的情况, 论文 [3] 已经充分表明, 在日常使用中, 只要按照正常习惯选取合理的超参数, 就可以保证收敛, Adam 的实用性至此得到了验证.\n","date":"2025-03-30T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/paper-reading/adam-convergence/","title":"论文阅读 - Adam 的收敛性分析"},{"content":"集成学习概述 定义\n集成学习 是指通过将相对比较容易构建但泛化性能一般的多个学习器进行结合, 以获得比单一学习器更好的泛化性能的一种机器学习方法.\n根据个体分类器是否由同一学习算法, 集成学习可以分为 同质集成 和 异质集成 两大类; 根据个体分类器的依赖关系, 可以将学习方法分为 序列化方法(串行集成) 和 并行化方法(并行集成) 两种.\n提升方法 PAC 框架下, 概念类强可学习和其弱可学习等价, 但弱可学习实现更容易, 提升方法就是指将弱学习算法提升为强学习算法的方法.\nAdaBoost 算法 算法AdaBoost\n输入: 给定训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i \\in \\mathcal{X}, y_i \\in \\mathcal{Y}=\\{-1,+1\\}, i=1,2,\\cdots,N$; 弱学习算法 $\\mathcal{L}$ 以及基本分类器个数 $T$;\n输出: 最终分类器 $f(x)$\n准备一个权重向量 $W_t=(w_{i,t})_{i=1}^N$, 表示第 $t$ 轮训练数据的权重分布, 初始时 $w_{i,1}=\\frac{1}{N}$; 在第 $t$ 轮学习中, 应用算法 $\\mathcal{L}$ 基于训练数据集 $D$ 和权重向量 $W_t$ 学得具有最小训练误差的基本分类器 $f_t(x)$, 即 $$f_t = \\argmin_{f} \\sum_{i=1}^N w_{i,t} \\mathbb{I}(f(x_i) \\neq y_i)$$ 计算 $f_t(x)$ 的误差率 $$e_t = \\sum_{i=1}^N w_{i,t} \\mathbb{I}(f_t(x_i) \\neq y_i)$$ 计算 $f_t(x)$ 的权值 $$\\alpha_t = \\frac{1}{2} \\ln \\frac{1-e_t}{e_t}$$ 按照投票权值更新训练数据集的权重分布 $$w_{i,t+1} = \\frac{w_{i,t}}{Z_t} \\exp(-\\alpha_t y_i f_t(x_i))$$ 其中 $Z_t$ 是规范化因子, 使得 $w_{i,t+1}$ 成为一个概率分布. 经过 $T$ 轮迭代后, 得到最终分类器 $$ \\begin{aligned} f(x) \u0026= \\text{sign}(G(x)) \\\\ G(x) \u0026= \\sum_{t=1}^T \\alpha_t f_t(x) \\end{aligned} $$ $G(x)$ 的符号决定了 $x$ 的类别, $|G(x)|$ 表示分类的确信度. 注意:\n$e_t$ 越小, $\\alpha_t$ 越大, 表示 $f_t(x)$ 的权重越大; $\\alpha_t$ 不仅仅平衡了 $f_t(x)$ 的权重, 还调节了样本分布的权重: $$w_{i,t+1}=\\begin{cases} \\frac{w_{i,t}}{Z_t} \\exp(\\alpha_t), \u0026 y_i=f_t(x_i) \\\\ \\frac{w_{i,t}}{Z_t} \\exp(-\\alpha_t), \u0026 y_i \\neq f_t(x_i) \\end{cases} $$ 对于那些错误样本, 下次迭代时的权重会增大, 以便让弱分类器更关注这些样本. 关于为什么要这样赋值 $\\alpha_t$, 由表达式可以得到 $$\\exp(\\alpha_t)e_t = \\exp(-\\alpha_t)(1-e_t)$$ 这表明分配给错误样本的权重之和与正确样本的权重之和相等. 计算可知 $$Z_t = \\sum_{i=1}^N w_{i,t} \\exp(-\\alpha_t y_i f_t(x_i)) = 2 \\sqrt{e_t(1-e_t)}$$ 权值调整累计过程 $$ \\begin{aligned} w_{i,t+1}\u0026=w_{i,t} \\frac{\\exp(-\\alpha_t y_i f_t(x_i))}{Z_t} \\\\ \u0026=w_{i,1} \\frac{\\exp \\left(-y_i \\sum_{s=1}^t \\alpha_s f_s(x_i)\\right)}{\\prod_{s=1}^t Z_s} \\\\ \\end{aligned} $$ 取 $t=T$, 由 $w_{i,1}=\\frac{1}{N}$ 可得 $$ w_{i,T} = \\frac{1}{N} \\frac{\\exp (-y_i G(x_i))}{\\prod_{s=1}^T Z_s} $$ AdaBoost 误差分析 集成分类器的训练误差为\n$$ \\begin{aligned} \\hat{R}(f) \u0026= \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(f(x_i) \\neq y_i) =\\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(y_iG(x_i) \\le 0) \\\\ \u0026\\le \\frac{1}{N} \\sum_{i=1}^N \\exp(-y_iG(x_i)) = \\frac{1}{N} \\sum_{i=1}^N \\left( N \\prod_{t=1}^T Z_tw_{i,T+1} \\right) \\\\ \u0026=\\prod_{t=1}^T Z_t = \\prod_{t=1}^T \\left(2\\sqrt{e_t(1-e_t)}\\right) \\le \\exp\\left(-2 \\sum_{t=1}^T \\left(\\frac{1}{2}-e_t\\right)^2\\right)\\\\ \\end{aligned} $$这说明 AdaBoost 的训练误差是负指数量级的. 而且不需要提前知道 $e_t$ 的值, 只需要知道 $e_t$ 的上界即可, 这也是 AdaBoost 的 Adaptive 性质所在.\n加法模型 AdaBoost 可以看作是 $\\{\\alpha_t, f_t(x)\\}$ 的加法模型, 如果我们采用指数损失函数 $\\exp(-y_if(x))$, 令\n$$ G_t(\\alpha, f) = \\frac{1}{N} \\sum_{i=1}^N \\exp(-y_iG(x_i) + \\alpha f(x_i)) $$则可以证明第 $t$ 轮迭代得到的 $(\\alpha_t,f_t)$ 是最小化 $G_t(\\alpha, f)$ 的解, 过程略.\n算法提升树模型\n输入: 给定训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i \\in \\mathcal{X}, y_i \\in \\mathcal{Y}=\\{-1,+1\\}, i=1,2,\\cdots,N$; 弱学习算法(一般是分类或回归树) $\\mathcal{T}$, 损失函数 $L(y,f(x))$;\n输出: 最终分类器 $f(x)$\n第 $m$ 个基学习器 $T(x; \\Theta_m)$ 由经验风险最小化得到, 即\n$$\\Theta_m = \\argmin_{\\Theta} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \\Theta))$$ 如果是平方损失, 提升树算法实际上就是在拟合当前模型的残差.\nBagging 方法 对训练样本进行重采样, 利用不同的样本数据来学习不同的基学习器, 通过降低方差来提高集成学习器的泛化性能. Bagging (Bootstrap Aggregating) 就是一种典型的并行集成学习方法.\n算法Bagging\n输入: 给定训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i \\in \\mathcal{X}, y_i \\in \\mathcal{Y}, i=1,2,\\cdots,N$; 弱学习算法 $\\mathcal{L}$ 以及基分类器个数 $T$;\n输出: 最终分类器 $f(x)$\n对 $t=1,2,\\ldots,T$, 从$D$利用自助采样法随机抽取 $N$ 个样本得到 $D_t$. 从 $D_t$ 依学习算法 $\\mathcal{L}$ 学得基分类器 $f_t(x)$.\n返回集成分类器\n$$f(x)=\\argmax_{y\\in\\mathcal{Y}} \\sum_{t=1}^T I(f_t(x)=y)$$ 对样本 $x_i$ 来说，我们采用 $x_i$ 未参与训练的基学习器在 $x_i$ 上的预测的简单投票结果作为 $x_i$ 的包外预测\n$$f_{\\text{oob}}(x_i) = \\argmax_{y \\in Y} \\sum_{t=1}^{T} \\mathbb{I} (f_t(x_i) = y) \\mathbb{I} (x_i \\notin D_t)$$相应的 Bagging 算法泛化误差的包外估计 (out-of-bag estimate) 为\n$$\\epsilon^{\\text{oob}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(f_{\\text{oob}}(x_i) \\neq y_i)$$随机森林 所谓随机森林, 就是在 Bagging 方法中以决策树算法作为基学习器, 并引入随机属性选择, 即在选择划分特征时, 先从当前节点的所有 $d$ 个特征中随机选择 $k$ 个特征, 再从这 $k$ 个特征中选择最优划分特征. 随机选取的目的是为了增加基学习器之间的差异性, 使得集成学习器的泛化性能更好.\n当 $k=d$ 时, 随机森林退化为 Bagging 方法, 一般取 $k=\\log_2 d$.\n","date":"2025-03-28T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/integrated-learning/","title":"机器学习基础(7) —— 集成学习"},{"content":"特征的线性组合 之前的二分类问题中, 如果把 $z=w^Tx+b$ 看做是衍生的新特征, 实际上感知机的模型就是 $y=\\text{sign}(z)$. 二项逻辑斯蒂回归模型中, $P(y=1 \\mid x) = \\sigma(z)=\\frac{1}{1+e^{-z}}$. 相当于引入了一个 sigmoid 函数进行非线性变换.\n因此, 神经网络应运而生, 主要想法:\n通过各维特征线性组合得到新特征 基于衍生特征通过非线性变换得到新特征 再对新特征进行线性组合和非线性变换, 逐层叠加 通过嵌套逼近复杂函数 多层前馈神经网络 设当前的 (衍生) 特征向量是\n$$ z = \\left(z^{(1)},z^{(2)},\\cdots,z^{(m)}\\right)^T $$进行线性组合\n$$ v \\cdot z - \\theta = \\sum_{i=1}^m v_i z^{(i)} - \\theta $$再通过非线性变换 (考虑到数学性质, 通常是 sigmoid 函数):\n$$ t = g(v \\cdot z - \\theta) $$ 定义\n多层前馈神经网络 是常见的神经网络模型:\n逐层排列神经元, 仅限于相邻层之间的完全连接; 接受外部输入信号的神经元在同一层, 称为 输入层; 最后一层神经元输出网络的结果, 称为 输出层; 输入层和输出层之间的神经元称为 隐藏层; 输入层直接接受激活函数, 输出层和隐藏层都对接受到的信号做激活函数变换. 所谓 感知机, 就是没有隐藏层的前馈神经网络.\n前面学到的感知机学习能力有限, 例如它无法解决异或问题. 但是, 只要再加一层隐藏层, 就可以解决.\n考虑一个单隐层的神经网络:\n输入层有 $n$ 个神经元来接受输入信号; 输出层有 $k$ 个神经元来输出结果, 且第 $l$ 个神经元的阈值是 $\\theta_l$; 隐藏层有 $m$ 个神经元, 第 $t$ 个神经元的阈值是 $\\gamma_t$. 输入层到隐藏层的权重是 $w_{jt}$, 隐藏层到输出层的权重是 $v_{tl}$. 因而, 隐藏层的输出是\n$$ z^{(t)}(x)=\\sigma \\left(\\sum_{j=1}^n w_{jt} x^{(j)} - \\gamma_t \\right) $$输出层的输出是\n$$ y^{(l)}(x) = \\sigma \\left( \\sum_{t=1}^m v_{tl} z^{(t)} - \\theta_l \\right) $$参数集为 $\\Theta = \\{w_{jt},v_{tl},\\gamma_t,\\theta_l\\}$\n误差反向传播算法 我们采用平方误差作为预测损失函数, 则\n$$ R(\\Theta) = \\sum_{i=1}^N R_i(\\Theta) = \\sum_{i=1}^N \\| y_i - \\hat{y}_i \\| ^2 = \\sum_{i=1}^N \\sum_{l=1}^k (y_i^{(l)} - \\hat{y}_i^{(l)})^2 $$依然采用经验风险最小化策略, 通过梯度下降法来求解参数集 $\\Theta$. 求偏导可得:\n$$ \\begin{aligned} \\frac{\\partial R_i(\\Theta)}{\\partial v_{tl}} \u0026= \\delta_i^{(l)}z^{(t)}(x_i) \\\\ \\frac{\\partial R_i(\\Theta)}{\\partial \\theta_l} \u0026= -\\delta_i^{(l)} \\\\ \\frac{\\partial R_i(\\Theta)}{\\partial w_{jt}} \u0026= s_i^{(t)} x_i^{(j)} \\\\ \\frac{\\partial R_i(\\Theta)}{\\partial \\gamma_t} \u0026= -s_i^{(t)} \\end{aligned} $$其中\n$$ \\begin{aligned} \\delta_i^{(l)}\u0026=-2(y_i^{(l)}-\\hat{y}_i^{(l)})\\hat{y}_i^{(l)}(1-\\hat{y}_i^{(l)}) \\\\ s_i^{(t)} \u0026= z^{(t)}(x_i)(1-z^{(t)}(x_i))\\sum_{l=1}^k v_{tl}\\delta_i^{(l)} \\end{aligned} $$给定学习率 $\\eta$, 按照 $\\alpha = \\alpha - \\eta \\frac{\\partial R_i(\\Theta)}{\\partial \\alpha}$ 进行迭代更新.\n采用正则化策略来缓解过拟合问题:\n$$ \\hat{\\Theta} = \\argmin_{\\Theta} (R(\\Theta) + \\lambda J(\\Theta)) $$其中 $J(\\Theta)$ 是正则化项, 通常是参数的 $L_2$ 范数, 所有参数的平方和.\n关于激活函数, 除了 sigmoid 函数, 还有 tanh 函数, ReLU 函数等. 前两者函数性质连续, 但是在部分情况可能导数接近 $0$, 从而导致梯度消失问题. 相对之下, ReLU 函数梯度计算简单. 还有带泄漏的 ReLU 函数:\n$$ f(x) = \\begin{cases} x \u0026 x\u003e0 \\\\ \\lambda x \u0026 x \\leq 0 \\end{cases} $$等等.\n","date":"2025-03-25T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/nn-beginner/","title":"机器学习基础(6) —— 神经网络学习初步"},{"content":"对偶理论 对于一般的约束优化问题:\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 c_i(x) \\leq 0, \\quad i \\in \\mathcal{I} \\\\ \u0026 c_i(x) = 0, \\quad i \\in \\mathcal{E} \\end{aligned} $$Lagrange 函数为:\n$$ L(x, \\lambda, \\nu) = f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) + \\sum_{i \\in \\mathcal{E}} \\nu_i c_i(x) $$其中 $\\lambda_i \\ge 0$.\n定义\nLagrange 对偶函数 $g: \\mathbb{R}_+^m \\times \\mathbb{R}^p \\to [-\\infty, +\\infty)$ 定义为:\n$$ g(\\lambda, \\nu) = \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu) $$ 定理弱对偶原理\n若 $\\lambda \\ge 0$, 则 $g(\\lambda, \\nu) \\le p^\\ast$.\n证明\n对 $x_0 \\in \\mathcal{X}$, 有:\n$$ g(\\lambda, \\nu) = \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu) \\le L(x_0, \\lambda, \\nu) \\le f(x_0) $$对 $x_0$ 取 $\\inf$ 得到\n$$ g(\\lambda, \\nu) \\le \\inf_{x \\in \\mathcal{X}} f(x) = p^\\ast $$ 定义\nLagrange 对偶问题 形式如下:\n$$ \\max_{\\lambda \\ge 0, \\nu} g(\\lambda, \\nu)= \\max_{\\lambda \\ge 0, \\nu} \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu) $$称 $\\lambda, \\nu$ 为对偶变量, 设最优值为 $q^\\ast$. $q^\\ast$ 是 $p^\\ast$ 的最优下界, 称 $p^\\ast-q^\\ast$ 为 对偶间隙.\n示例: 线性规划问题 $$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 x \\ge 0 \\end{aligned} $$Lagrange 函数为:\n$$ \\begin{aligned} L(x, \\lambda, \\nu) \u0026= c^T x + \\lambda^T (Ax - b) - \\nu^T x \\\\ \u0026= -b^T \\lambda + (c + A^T \\lambda - \\nu)^T x \\end{aligned} $$对偶函数:\n$$ g(\\lambda, \\nu) = \\inf_{x \\ge 0} L(x, \\lambda, \\nu) = \\begin{cases} -b^T \\lambda, \u0026 c + A^T \\lambda - \\nu = 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 -b^T \\lambda \\\\ \\text{s.t.} \\quad \u0026 c + A^T \\lambda - \\nu = 0 \\\\ \u0026 \\nu \\ge 0 \\end{aligned} $$可以计算证明, 线性规划问题和对偶问题互为对偶.\n示例: 范数最小化问题 $$ \\begin{aligned} \\min \\quad \u0026 \\|x\\| \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$对偶函数:\n$$ g(\\nu) = \\inf_x(\\|X\\| - \\nu^T (Ax - b)) = \\begin{cases} b^T\\nu, \u0026 \\|A^T \\nu\\|_* \\le 1 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} $$其中 $\\|v\\|_*=\\sup_{\\|x\\|\\le 1} x^T v$ 为 $v$ 的对偶范数 (关于为什么自证). 因此对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 b^T \\nu \\\\ \\text{s.t.} \\quad \u0026 \\|A^T \\nu\\|_* \\le 1 \\end{aligned} $$示例: 最大割问题 上回说到, 最大割问题可以写成:\n$$ \\begin{aligned} \\max \\quad \u0026 x^T W x \\\\ \\text{s.t.} \\quad \u0026 x_i^2 = 1 \\end{aligned} $$首先加负号变成 $\\min$, Lagrange 函数为:\n$$ \\begin{aligned} L(x, y) \u0026= -x^T W x + \\sum_{i=1}^n y_i (x_i^2 - 1) \\\\ \u0026= x^T(\\text{diag}(y) - W) x - \\mathbf{1}^T y \\end{aligned} $$对偶函数:\n$$ g(y) = \\inf_x L(x, y) = \\begin{cases} -\\mathbf{1}^T y, \u0026 \\text{diag}(y) - W \\succeq 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 -\\mathbf{1}^T y \\\\ \\text{s.t.} \\quad \u0026 \\text{diag}(y) - W \\succeq 0 \\end{aligned} $$再来考虑这个对偶问题的对偶问题.\n对偶函数:\n$$ \\begin{aligned} g(X)\u0026=\\inf_y(\\mathbf{1}^Ty) - \\left\u003c\\text{diag}(y) - W, X\\right\u003e \\\\ \u0026= \\inf \\left(\\sum_{i=1}^n (1-X_{ii})y_i + \\left\u003c W, X \\right\u003e \\right) \\\\ \u0026= \\begin{cases} \\left\u003c W, X \\right\u003e, \u0026 X_{ii} = 1, i = 1, \\ldots, n \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 \\left\u003c W, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 X_{ii} = 1, i = 1, \\ldots, n \\\\ \u0026 X \\succeq 0 \\end{aligned} $$示例: 共轭函数 $$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 Ax \\le b \\\\ \u0026 Cx = d \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\lambda, \\nu) \u0026= \\inf_x(f(x) + \\lambda^T (Ax - b) + \\nu^T (Cx - d)) \\\\ \u0026=\\inf_x(f(x) + (A^T \\lambda + C^T \\nu)^T x - b^T \\lambda - d^T \\nu) \\\\ \u0026=-f^\\ast(-A^T \\lambda - C^T \\nu) - b^T \\lambda - d^T \\nu \\end{aligned} $$其中 $f^\\ast(v) = \\sup_x(x^T v - f(x))$ 为 $f$ 的共轭函数.\n对偶性 定义\n弱对偶性: $d^\\ast \\le p^\\ast$. 对一般约束优化问题成立. 强对偶性: $d^\\ast = p^\\ast$, 且若一个线性规划问题有最优解, 则其对偶问题有最优解, 且最优值相等. 一般不成立, 但通常对凸优化问题成立. 称保证凸问题强对偶性成立的条件为 约束品性.\n考虑下面这个不满足强对偶性的例子.\n$$ \\begin{aligned} \\min \\quad \u0026 x_0-x_1 \\\\ \\text{s.t.} \\quad \u0026 x_0 \\ge \\sqrt{x_1^2+1} \\end{aligned} $$显然, $x_0-x_1 \u003e 0$, 但当 $x_0 \\to \\infty$ 时, $x_0-x_1 \\to 0$, 因此 $p^\\ast = 0$, 但是不可达. 而对偶问题是:\n$$ \\begin{aligned} \\max \\quad \u0026 \\lambda \\\\ \\text{s.t.} \\quad \u0026 1 \\ge \\sqrt{1 + \\lambda^2} \\end{aligned} $$ 因此 $\\lambda \\le 0$, $d^\\ast = 0$. 虽然 $d^\\ast = p^\\ast$, 但原问题是没有最优解的.\n当然, 也有使得 $p^\\ast \\ne d^\\ast$ 的例子.\n改写问题形式 当对偶问题难以推导或没有价值时, 可以尝试改写原问题的形式.\n引入新变量与等式约束; 将显式约束隐式化或将隐式约束显式化; 改变目标函数或者约束函数的形式. 例如, 用 $\\phi(f(x))$ 取代 $f(x)$, 其中 $\\phi$ 是凸的增函数. 先来看几个引入等式约束的例子.\n示例: 函数值最小化问题 $$ \\min f(Ax+b) $$直接做对偶是常数, 没有意义. 可以改写为:\n$$ \\begin{aligned} \\min \\quad \u0026 f(y) \\\\ \\text{s.t.} \\quad \u0026 y = Ax + b \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 b^T \\nu-f^\\ast(\\nu) \\\\ \\text{s.t.} \\quad \u0026 A^T \\nu = 0 \\end{aligned} $$示例: 范数逼近问题 $$ \\min \\|Ax-b\\|_2 $$改写成:\n$$ \\begin{aligned} \\min \\quad \u0026 \\|y\\| \\\\ \\text{s.t.} \\quad \u0026 y = Ax - b \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\nu) \u0026= \\inf_{x,y}(\\|y\\| + \\nu^Ty - \\nu^TAx + \\nu^Tb) \\\\ \u0026= \\begin{cases} \\nu^T b + \\inf_y(\\|y\\| + \\nu^Ty), \u0026 A^T \\nu = 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\\\ \u0026= \\begin{cases} \\nu^T b, \u0026 A^T \\nu = 0, \\|\\nu\\|_* \\le 1 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 \\nu^T b \\\\ \\text{s.t.} \\quad \u0026 A^T \\nu = 0 \\\\ \u0026 \\|\\nu\\|_* \\le 1 \\end{aligned} $$示例: L1 正则化问题 $$ \\min_{x\\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax-b\\|^2 + \\mu \\|x\\|_1 $$改写成:\n$$ \\begin{aligned} \\min \\quad \u0026 \\frac{1}{2}\\|r\\|^2 + \\mu \\|x\\|_1 \\\\ \\text{s.t.} \\quad \u0026 r = Ax - b \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\nu) \u0026= \\inf_{x,r}\\left(\\frac{1}{2}\\|r\\|_2^2 + \\mu \\|x\\|_1 + \\lambda^T(r-Ax+b)\\right) \\\\ \u0026= \\inf_{x,r}\\left(\\frac{1}{2}\\|r\\|^2 + \\lambda^Tr + \\mu \\|x\\|_1 - (A^T\\lambda)^Tx + b^T\\lambda\\right) \\\\ \u0026= \\begin{cases} b^T \\lambda - \\frac{1}{2} \\| \\lambda \\|^2, \u0026 \\| A^T \\lambda \\|_{\\infty} \\le \\mu \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 b^T \\lambda - \\frac{1}{2} \\| \\lambda \\|^2 \\\\ \\text{s.t.} \\quad \u0026 \\| A^T \\lambda \\|_{\\infty} \\le \\mu \\end{aligned} $$现在考虑显式和隐式约束转化的例子.\n示例: 带边界约束的线性规划问题 $$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 -\\mathbf{1} \\le x \\le \\mathbf{1} \\end{aligned} $$我们把边界要求隐藏在目标函数中:\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) =\\begin{cases} c^T x, \u0026 -\\mathbf{1} \\le x \\le \\mathbf{1} \\\\ +\\infty, \u0026 \\text{otherwise} \\end{cases} \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\nu) \u0026= \\inf_{-\\mathbf{1} \\le x \\le \\mathbf{1}}(c^Tx+\\nu^T(Ax-b)) \\\\ \u0026= -b^T \\nu - \\|A^T \\nu + c\\|_1 \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 -b^T \\nu - \\|A^T \\nu + c\\|_1 \\\\ \\text{s.t.} \\quad \u0026 \\nu \\ge 0 \\end{aligned} $$示例: 广义不等式约束优化问题 $$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 c_i(x) \\preceq_{K_i} 0, \\quad i \\in \\mathcal{I} \\\\ \u0026 c_i(x) = 0, \\quad i \\in \\mathcal{E} \\end{aligned} $$其中对于 $i \\in \\mathcal{I}$, $c_i: \\mathbb{R}^n \\to \\mathbb{R}^{k_i}$ 是向量值函数, $K_i$ 是适当锥. 对于 $i \\in \\mathcal{E}$, $c_i: \\mathbb{R}^n \\to \\mathbb{R}$ 是标量函数. 实际上这种情况下, 其 Lagrange 函数为:\n$$ L(x, \\lambda, \\nu) = f(x) + \\sum_{i \\in \\mathcal{I}} \\left\u003c\\lambda_i, c_i(x)\\right\u003e + \\sum_{i \\in \\mathcal{E}} \\nu_i c_i(x), \\quad \\lambda_i \\in K_i^\\ast $$这里 $K_i^\\ast$ 是 $K_i$ 的对偶锥. 此时也依然满足 $L(x, \\lambda, \\nu) \\le f(x)$. 对偶问题依然是 $\\max_{\\lambda \\in K_i^\\ast, \\nu} g(\\lambda, \\nu)$.\n示例: 半定规划问题 $$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c C, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 \\left\u003c A_i, X \\right\u003e = b_i, \\quad i = 1, \\ldots, m \\\\ \u0026 X \\succeq 0 \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\lambda, \\nu) \u0026= \\inf_X \\left( \\left\u003c C, X \\right\u003e + \\sum_{i=1}^m \\lambda_i (\\left\u003c A_i, X \\right\u003e - b_i) - \\left\u003c\\nu,X\\right\u003e \\right) \\\\ \u0026= \\begin{cases} b^T\\lambda, \u0026 \\sum_{i=1}^m \\lambda_i A_i - C +\\nu \\succeq 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\max \\quad b^T\\lambda \\\\ \\text{s.t.} \\quad \\sum_{i=1}^m \\lambda_i A_i - C +\\nu \\succeq 0 $$可以证明, 半定规划问题和对偶问题互为对偶.\n带约束凸优化问题的最优性理论 综合来看, 前面的问题其实都可以写成如下的形式:\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 c_i(x) \\le 0, i=1, \\ldots, m \\end{aligned} $$其中 $f(x)$ 为适当的凸函数, $c_i(x)$ 也是凸函数且 $\\text{dom} c_i = \\mathbb{R}^n$.\n定义\n给定集合 $\\mathcal{D}$, 设其仿射包为 $\\text{affine} \\mathcal{D}$, 则其 相对内点集 定义为:\n$$ \\text{relint} \\mathcal{D} = \\{x \\in \\mathcal{D} \\mid \\exists r \u003e 0, B(x,r) \\cap \\text{affine} \\mathcal{D} \\subset \\mathcal{D}\\} $$ 定义\n若对于凸优化问题\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 c_i(x) \\le 0, i=1, \\ldots, m \\end{aligned} $$存在 $x \\in \\text{relint} \\mathcal{D}$, 使得\n$$c_i(x) \u003c 0, i=1, \\ldots, m$$则称对于该问题 Slater 约束品性 成立. 该品性也称为 Slater 条件.\n注意: 如果某个不等式约束是仿射函数时, Slater 可以对这个不等式约束放宽到 $c_i(x) \\le 0$.\n定理\n若 Slater 约束品性成立, 则强对偶性成立.\n证明\n这里假设 $\\mathcal{D}$ 内部非空, $A$ 是行满秩的 (否则可以去掉冗余约束), $p^\\ast$ 是有限的. 要证明当 $d^\\ast \u003e -\\infty$ 时, 存在对偶可行解 $(\\lambda^\\ast, \\nu^\\ast)$ 使得 $g(\\lambda^\\ast, \\nu^\\ast) = d^\\ast=p^\\ast$.\n定义集合\n$$ \\begin{aligned} \\mathbb{A}\u0026= \\{(u,v,t) \\mid \\exists x \\in \\mathcal{D}, c_i(x) \\le u_i (i=1, \\ldots, m), Ax-b=v, f(x) \\le t\\} \\\\ \\mathbb{B}\u0026= \\{(0,0,s) \\mid s \\le p^\\ast\\} \\end{aligned} $$若 $(0,0,t) \\in \\mathbb{A} \\cap \\mathbb{B}$, 则 $f(x) \\le t\u003c p^\\ast$ 矛盾. 因而两集合不交. 由于两者都是凸集, 由超平面分离定理, 存在 $(\\lambda, \\nu, \\mu) \\ne 0$ 和 $\\alpha$ 使得\n$$ \\begin{aligned} \\lambda^Tu + \\nu^Tv + \\mu t \u0026\\ge \\alpha, \\quad \\forall (u,v,t) \\in \\mathbb{A} \\\\ \\lambda^Tu + \\nu^Tv + \\mu t \u0026\\le \\alpha, \\quad \\forall (u,v,t) \\in \\mathbb{B} \\end{aligned} $$显见, $\\lambda \\ge 0, \\mu \\ge 0$, 否则可以让 $\\mu_i, t \\to +\\infty$ 使得集合 $\\mathbb{A}$ 上不等式左侧无下界. 由 $\\mathbb{B}$ 的不等式, 立得 $\\mu p^\\ast \\le \\alpha$.\n对于 $(u,v,t)=(c_i(x), Ax-b, f(x))$, 代入有\n$$ \\sum_{i=1}^m \\lambda_i c_i(x) + \\nu^T(Ax-b) + \\mu f(x) \\ge \\alpha \\ge \\mu p^\\ast $$若 $\\mu \u003e 0$, 则上式恰好对应 Lagrange 函数:\n$$ L(x, \\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}) \\ge p^\\ast $$故 $g(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}) \\ge p^\\ast$, 再结合弱对偶性, $g(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}) = p^\\ast$. 则此情况下强对偶性成立, 且最优解可达.\n若 $\\mu=0$, 可以得到对于所有 $x \\in \\mathcal{D}$, 有\n$$ \\sum_{i=1}^m \\lambda_i c_i(x) + \\nu^T(Ax-b) \\ge 0 $$令 $x_S$ 为满足 Slater 条件的点, $Ax_S=b, c_i(x_S) \u003c 0$, 但 $\\lambda \\ge 0$ 则必须 $\\lambda = 0$. 因此\n$$ \\nu^T(Ax-b) \\ge 0, \\quad \\forall x \\in \\mathcal{D} $$$x_S$ 恰好是谷底的 $0$, 四周全部都 $\\ge 0$ 不太可能. 更具体地, 由于 $(\\lambda, \\nu, \\mu) \\ne 0$, 则 $\\nu \\ne 0$. $A$ 是行满秩的, 则 $A^T \\nu \\ne 0$, 因为 $x_S \\in \\text{int}\\mathcal{D}$, 则存在微扰 $e$ 使得\n$$\\widetilde{x} = x_S + e \\in \\mathcal{D}, \\quad v^TAe \u003c 0$$但是\n$$ v^TAe = v^TA(\\widetilde{x}-x_S) = v^T(A\\widetilde{x} - b) \\ge 0$$矛盾. 因此 $\\mu \u003e 0$, 强对偶性成立.\n现在假设 Slater 成立, $x^\\ast, \\lambda^\\ast$ 是原问题和对偶问题的最优解. 由强对偶性,\n$$ \\begin{aligned} f(x^\\ast) = g(\\lambda^\\ast) \u0026= \\inf_{x} f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) + \\sum_{i \\in \\mathcal{E}} \\lambda_i c_i(x) \\\\ \u0026\\le f(x^\\ast) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x^\\ast) + \\sum_{i \\in \\mathcal{E}} \\lambda_i c_i(x^\\ast) \\\\ \u0026\\le f(x^\\ast) \\end{aligned} $$因此等号要成立, 即 $\\lambda_i c_i(x^\\ast) = 0, i \\in \\mathcal{I}$, 这称为互补条件 (complementary slackness).\n定理凸优化问题的一阶充要条件\n对于凸优化问题, 用 $a_i$ 表示矩阵 $A^T$ 的第 $i$ 列, 如 果 Slater 条件成立, 那么$x^\\ast, \\lambda^\\ast$ 分别是原始, 对偶全局最优解当且仅当\n稳定性条件: $ 0 = \\nabla f(x^\\ast) + \\sum_{i \\in \\mathcal{I}} \\lambda_i^\\ast \\nabla c_i(x^\\ast) + \\sum_{i \\in \\mathcal{E}} \\lambda_i^\\ast a_i$ 原始可行性条件: $Ax^\\ast = b, i \\in \\mathcal{E}; c_i(x^\\ast) \\le 0, i \\in \\mathcal{I}$ 对偶可行性条件: $\\lambda_i^\\ast \\ge 0, i \\in \\mathcal{I}$ 互补松弛条件: $\\lambda_i^\\ast c_i(x^\\ast) = 0, i \\in \\mathcal{I}$ 证明\n必要性已知, 只证明充分性.\n考虑 Lagrange 函数\n$$ L(x, \\lambda) = f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) + \\sum_{i \\in \\mathcal{E}} \\lambda_i (a_i^T x - b_i) $$固定 $\\lambda = \\bar{\\lambda}$, 易见 $L$ 是关于 $x$ 的凸函数. 由凸函数全局最优点的一阶充要性可知, 此时 $\\bar{x}$ 就是全局的极小点. 由 Lagrange 对偶函数的定义, 有\n$$ L(\\bar{x}, \\bar{\\lambda}) = \\inf_x L(x, \\bar{\\lambda}) = g(\\bar{\\lambda}) $$又由原始可行性条件和互补松弛条件, 有\n$$ L(\\bar{x}, \\bar{\\lambda}) = f(\\bar{x}) $$由弱对偶性\n$$ L(\\bar{x}, \\bar{\\lambda}) = f(\\bar{x}) \\ge p^\\ast\\ge d^\\ast \\ge g(\\bar{\\lambda}) = L(\\bar{x}, \\bar{\\lambda}) $$因此等号成立, $p^\\ast = d^\\ast$, 强对偶性成立, $\\bar{x}, \\bar{\\lambda}$ 是全局最优解.\n例子: 仿射空间的投影问题 $$ \\begin{aligned} \\min \\quad \u0026 \\|x - y\\|^2 \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$Lagrange 函数为\n$$ L(x, \\lambda) = \\|x - y\\|^2 + \\lambda^T(Ax - b) $$Slater 条件成立, 由一阶充要条件, 有\n$$ \\begin{aligned} x^\\ast - y + A^T \\lambda^\\ast \u0026= 0 \\\\ Ax^\\ast \u0026= b \\end{aligned} $$解之\n$$ \\begin{aligned} \\lambda^\\ast \u0026= (AA^T)^{-1}(Ay-b) \\\\ x^\\ast \u0026= y - A^T(AA^T)^{-1}(Ay-b) \\end{aligned} $$显然, $x^\\ast$ 是 $y$ 在 $Ax=b$ 上的投影.\n例子: 基追踪问题 $$ \\begin{aligned} \\min \\quad \u0026 \\|x\\|_1 \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$这个函数实际是不光滑的. 把 $x$ 写作 $x^+-x^-$, 再令 $y=[x^+, x^-]$, 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\mathbf{1}^T y \\\\ \\text{s.t.} \\quad \u0026 [A, -A] y = b \\\\ \u0026 y \\ge 0 \\end{aligned} $$按照 KKT 条件, 有\n$$ \\begin{aligned} \\mathbf{1} + [A, -A]^T \\lambda^\\ast - \\nu^\\ast \u0026= 0 \\\\ [A, -A] y^\\ast \u0026= b \\\\ y^\\ast \u0026\\ge 0 \\\\ \\nu^\\ast \u0026\\ge 0 \\\\ \\nu^\\ast \\odot y^\\ast \u0026= 0 \\end{aligned} $$直接推导也得到相应结果, 二者实际上是等价的. (利用 $x^\\ast=y_i^\\ast-y_{i+n}^\\ast$, 代入验证最优点处方向导数为 $0$ 即可)\n最优化问题解的存在性 定理推广的 Weierstrass 定理\n如果函数 $f: \\mathcal{X} \\to (-\\infty, +\\infty]$ 适当且闭, 且以下条件中至少一个成立:\n$\\text{dom} f = \\{ x\\in \\mathcal{X}: f(x) \u003c +\\infty \\} $ 是有界的; 存在一个常数 $\\bar{\\g `2amma}$ 使得下水平集 $C_{\\bar{\\gamma}} = \\{ x \\in \\mathcal{X}: f(x) \\le \\bar{\\gamma} \\}$ 是非空且有界的; $f$ 是强制的, 即对于任一满足 $\\| x^k \\| \\to +\\infty$ 的点列 $\\{ x^k \\} \\subset \\mathcal{X}$, 都有 $\\lim_{k \\to \\infty} f(x^k) = +\\infty$ 则函数 $f$ 的最小值点集 $\\{x \\in \\mathcal{X} \\mid f(x) \\le f(y), \\forall y \\in \\mathcal{X}\\}$ 非空且紧.\n证明\n第二个: 又有\n","date":"2025-03-23T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/opt-theory/","title":"最优化方法(5) —— 最优性理论"},{"content":"特征的分类能力评估 定义\n给定数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i=\\left(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(m)}\\right) \\in \\mathcal{X}$ 是第 $i$ 个样本的特征向量, $y_i \\in \\mathcal{Y}=\\{c_1,c_2,\\cdots,c_K\\}$ 是第 $i$ 个样本的标签. 假设数据集 $D$ 根据特征分成了 $K$ 个子集 $D_1,D_2,\\cdots,D_K$, 定义 经验熵 为\n$$ H(D) = -\\sum_{k=1}^K \\frac{|D_k|}{|D|} \\log_2 \\frac{|D_k|}{|D|} $$现在给定某维特征 $A$ 和其取值集合 $\\{a_1,a_2,\\cdots,a_m\\}$, 根据 $A$ 的取值将数据集 $D$ 分成了 $m$ 个子集 $D_1^A,D_2^A,\\cdots,D_m^A$, 并进一步考虑 $D_i^A$ 中的标签分布, 定义 条件经验熵 为\n$$ H(D|A) = \\sum_{i=1}^m \\frac{|D_i^A|}{|D|} H(D_i^A) $$ 如果条件经验熵和经验熵之差越大, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.\n定义\n属性 $A$ 对数据集 $D$ 的 信息增益 $g(D,A)$ 定义为\n$$ g(D,A) = H(D) - H(D|A) $$ 考虑到信息增益的计算会偏向于选择取值较多的特征, 为了避免这种情况, 引入信息增益率来评估特征的分类能力.\n定义\n特征 $A$ 的 分裂信息 $IV(A)$ 定义为\n$$ IV(A) = -\\sum_{i=1}^m \\frac{|D_i^A|}{|D|} \\log_2 \\frac{|D_i^A|}{|D|} $$特征 $A$ 的 信息增益率 $g_R(D,A)$ 定义为\n$$ g_R(D,A) = \\frac{g(D,A)}{IV(A)} $$ 分裂信息其实就是按照 $A$ 取值作划分的经验熵.\n除了信息增益和信息增益率, 还有 Gini 指数可以用来评估特征的分类能力.\n定义\n数据集 $D$ 的 Gini 指数 $\\text{Gini}(D)$ 定义为\n$$ \\text{Gini}(D) = 1 - \\sum_{k=1}^K \\left(\\frac{|D_k|}{|D|}\\right)^2 $$特征 $A$ 的 Gini 指数 $\\text{Gini}(D,A)$ 定义为\n$$ \\text{Gini}(D,A) = \\sum_{i=1}^m \\frac{|D_i^A|}{|D|} \\text{Gini}(D_i^A) $$如果按照特征 $A$ 是否取值为 $a_i$ 对数据集 $D$ 进行划分 $D=D_i^A \\cup (D-D_i^A)$, 则 $A=a_i$ 的 Gini 指数 $\\text{Gini}_d(D,A=a_i)$ 定义为\n$$ \\text{Gini}_d(D,A=a_i) = \\frac{|D_i^A|}{|D|} \\text{Gini}(D_i^A) + \\frac{|D-D_i^A|}{|D|} \\text{Gini}(D-D_i^A) $$ Gini 指数可以看作任取两个样本, 它们的标签不一致的概率. 如果 Gini 指数越小, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.\n决策树模型 算法生成决策树\n输入: 训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 特征集 $\\mathcal{A}=\\{A_1,A_2,\\cdots,A_m\\}$, 最优特征选择函数 $F$.\n输出: 决策树 $T$.\n若数据集 $D$ 中所有样本的标签都是 $c_k$, 则生成一个类标记为 $c_k$ 的叶结点, 返回 $T$; 若 $A=\\emptyset$, 且 $D$ 非空, 则生成一个单节点树, 并以 $D$ 中样本数最多的类标记作为该节点的类标记, 返回 $T$; 计算 $A^\\ast=F(D,\\mathcal{A})$; 对 $A^\\ast$ 的每一个取值 $a_i$, 构造一个对应于 $D_i$ 的子节点; 若 $D_i=\\emptyset$, 则将子节点标记为叶结点, 类标记为 $D$ 中样本数最多的类标记; 否则, 将 $D_i$ 中样本数最多的类标记作为该节点的类标记 对每个 $D_i$ 对应的非叶子节点, 以 $D_i$ 为训练集, 以 $\\mathcal{A}-\\{A^\\ast\\}$ 为特征集, 递归调用 1-6 步, 构建决策树 $T$. 如果以信息增益为特征选择函数, 即 $A^\\ast = \\arg\\max_{A \\in \\mathcal{A}} g(D,A)$, 则算法对应于 ID3 算法; 如果以信息增益率为特征选择函数, 即 $A^\\ast = \\arg\\max_{A \\in \\mathcal{A}} g_R(D,A)$, 则算法对应于 C4.5 算法.\n二路划分会采用以特征的可能取值为切分点的二分法划分当前数据集, 例如与选择 Gini 指数最小的特征和切分点对应的特征值, 即 $(A^\\ast,a^\\ast) = \\arg\\min_{A \\in \\mathcal{A},a \\in V(A)} \\text{Gini}_d(D,A=a)$, 则算法对应于 CART 算法.\n为了降低过拟合风险, 可以对决策树进行剪枝. 常用的是后剪枝, 即先生成一棵完全生长的决策树, 然后根据泛化性能决定是否剪枝. 也可以采用正则化方法, 例如, 定义决策树 $T$ 的损失或代价函数:\n$$ C_\\alpha(T) = C(T) + \\alpha |T| $$其中 $C(T)$ 用于衡量 $T$ 对 $D$ 的拟合程度, $|T|$ 表示 $T$ 的叶结点个数, $\\alpha \\geq 0$ 用于权衡拟合程度和模型复杂度.\nCART 算法有特别的剪枝处理: 从 CART 算法生成得到完整决策树 $T_0$ 开始, 产生一个递增的权衡系数序列 $0=\\alpha_0 \u003c \\alpha_1 \u003c \\cdots \u003c \\alpha_n \u003c +\\infty$ 和一个嵌套的子树序列 $\\{T_0, T_1, \\cdots, T_n\\}$, $T_i$ 为 $\\alpha \\in [\\alpha_i, \\alpha_{i+1})$ 时的最优子树, $T_n$ 是根节点单独构成的树.\n如果是连续特征, 则可以考虑将其离散化, 例如, 通过二分法将其划分为两个区间, 选择最优划分点.\n现在继续从经验风险的角度来看决策树模型.采用 $0-1$ 损失函数, 设节点 $t$ 设置的标记是 $c_k$, 则在 $t$ 对应的数据集上的经验风险为\n$$ \\frac{1}{|D_t|} \\sum_{i=1}^{|D_t|} I(y_i \\neq c_k) $$显见, 等价于\n$$ \\max_{c_k \\in \\mathcal{Y}} \\frac{1}{|D_t|} \\sum_{i=1}^{|D_t|} I(y_i = c_k) $$从现在来看, 决策树构造过程中划分的单元都是矩形的, 即分类边界是若干与特征坐标轴平行的边界组成. 多变量决策树模型允许用若干特征的线性组合来划分数据集, 对每个非叶结点学习一个线性分类器.\n最小二乘回归树模型 CART 算法用于回归问题时, 采用平方误差损失函数选择属性和切分点.\n算法CART\n输入: 训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 特征集 $\\mathcal{A}=\\{A_1,A_2,\\cdots,A_m\\}$.\n输出: 回归树 $T$.\n设回归树将输入空间划分为 $M$ 个单元 $R_1,R_2,\\cdots,R_M$, 并在每个单元上有一个固定的输出值 $c_m$, 则回归树模型可以表示为\n$$ f(x)=\\sum_{m=1}^M c_m I(x \\in R_m) $$ 如果采用平方误差, 则 $R_m$ 的输出值 $c_m$ 应该是 $R_m$ 中所有样本输出值的均值, 即\n$$ \\hat{c}_m = \\frac{1}{|R_m|} \\sum_{x_i \\in R_m} y_i $$ 对于一个输入空间, 若选用第 $j$ 维特征变量作为切分变量, $s$ 作为切分点, 则可以将输入空间划分为两个区域\n$$ R_1(j,s) = \\{x|x^{(j)} \\leq s\\}, \\quad R_2(j,s) = \\{x|x^{(j)} \u003e s\\} $$则可以通过求解优化问题\n$$ \\min_{j,s} \\left[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + \\min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2\\right] $$来确定最优切分变量 $j$ 和切分点 $s$. 实际上这里的 $c_i$ 就应该取 2 步中的 $\\hat{c}_m$.\n从初始输入空间开始, 按照误差最小原则递归划分, 重复如上过程, 直到满足停止条件.\n对于剪枝, 和分类任务处理框架一致, 采用\n$$ C_\\alpha(T) = C(T) + \\alpha |T| $$计算损失, 其中\n$$C(T) = \\sum_{t=1}^{|T|} N_tQ_t(T) = \\sum_{t=1}^{|T|} \\sum_{x_i \\in R_t} (y_i-\\hat{c}_t)^2$$$N_t$ 表示叶结点 $t$ 中的样本数, $Q_t(T)$ 表示叶结点 $t$ 的均方损失, $\\hat{c}_t$ 表示叶结点 $t$ 的输出值均值.\n","date":"2025-03-18T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/","title":"机器学习基础(5) —— 决策树模型"},{"content":"k-近邻算法 k-近邻算法的主要思想是, 对于一个给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点, 然后根据这 $k$ 个样本点的类别进行多数占优的投票方式来预测 $x$ 的类别.\n在 $n$ 维实数空间 $\\mathbb{R}$ 中, 通常用 Minkowski 距离来度量两个点 $x_i, x_j$ 的相似性:\n定义\n设 $x_i, x_j \\in \\mathbb{R}^n$, 则 $x_i, x_j$ 之间的 Minkowski 距离 $\\text{dist}_p(x_i,x_j)$ 定义为\n$$ \\text{dist}_p(x_i,x_j) = \\left( \\sum_{l=1}^n |x_i^l - x_j^l|^p \\right)^{1/p} $$ $p=1$ 时, 就是 Manhattan 距离; $p=2$ 时, 就是 Euclidean 距离; $p=\\infty$ 时, 就是 Chebyshev 距离. 在必要时, 还可以给每个维度的特征值加权.\n算法k-近邻\n输入: 给定训练样本集 $D = \\{(x_i, y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_k\\}$, 以及距离度量 $\\text{dist}$.\n输出: 对于每个样本点 $x \\in \\mathbb{R}^n$, 预测其类别 $y$.\n基于度量 $\\text{dist}$, 对于给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点所构成的邻域 $N_k^{\\text{dist}}(x)$;\n采用如下的多数投票规则来预测 $x$ 的类别:\n$$ y = \\argmax_{c_i} \\sum_{x_j \\in N_k^{\\text{dist}}(x)} I(y_j = c_i) $$ 如果把 0-1 作为损失函数, 那么 k-近邻算法实际上就是让经验风险最小化.\n最近邻算法 在 k-近邻算法中, 当 $k=1$ 时, 称为最近邻算法. 因此, 特点是偏差小, 方差大. 这其实是特征空间的一个划分 $\\mathcal{X}=\\bigcup_{i=1}^n \\{R_i\\}$. 对每个划分单元 $R_i$, 该单元的数据点到其他样本的距离都不会小于到 $x_i$ 的距离.\n最近邻算法的扩展 给定样本集 $D = \\{(x_i,y_i)\\}_{i=1}^n$, 以 $D_i$ 表示属于类 $c_i$ 的样本集, 希望找一个方式把每个 $D_i$ 分成 $k$ 个簇 $(D_{i1}, D_{i2}, \\cdots, D_{ik})$, 使得数据分布的方差最小, 即\n$$ (D^\\ast_{i1}, D^\\ast_{i2}, \\cdots, D^\\ast_{il}) = \\argmin_{D_{i1}, D_{i2}, \\cdots, D_{ik}} \\sum_{j=1}^k \\sum_{(x_t,y_t) \\in D_{ij}} \\Vert x_t-c_{ij} \\Vert_2^2 $$然而很难找到最优解, 因此采用迭代的方式来近似求解:\n算法K-means\n输入: 给定训练样本集 $D = \\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_k\\}$, 以及距离度量 $\\text{dist}(x,y)=\\Vert x-y \\Vert_2$.\n输出: 对于每个样本点 $x \\in \\mathbb{R}^n$, 预测其类别 $y$.\n初始化 $k$ 个簇的中心 $c_{ij}$; 对每个 $(x_t, y_t) \\in D_i$ (即 $y_t=c_i$), 令 $$I_{x_t}= \\argmin_{j} \\Vert x_t-c_{ij} \\Vert_2^2$$ 即将 $x_t$ 分配到最近的簇; 对每个 $D_{ij}$, 更新均值 $$c_{ij} = \\frac{1}{|D_{ij}|} \\sum_{(x_t,y_t) \\in D_{ij}} x_t$$ 重复 2, 3 直到收敛. 有可能会使得某些离分类边界很近的点被错误分类. 引入学习向量量化方法 (LVQ 算法). 让同类和异类的点在构建过程中都能起作用.\n算法LVQ\n输入: 给定训练样本集 $D = \\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_k\\}$, 以及距离度量 $\\text{dist}(x,y)=\\Vert x-y \\Vert_2$.\n输出: 对于每个样本点 $x \\in \\mathbb{R}^n$, 预测其类别 $y$.\n对每个类 $c_m$ 随机选择 $k$ 个点 $I_{mi}$ 作为代表; 对每个样本点 $x_t$, 找到最近的代表元 $I_{m^\\ast i^\\ast}$, 即 $$I_{m^\\ast i^\\ast} = \\argmin_{m,i} \\Vert x_t - I_{mi} \\Vert_2^2$$ 如果 $y_t=c_{m^\\ast}$, 则 $$I_{m^\\ast i^\\ast} \\gets I_{m^\\ast i^\\ast} + \\eta(x_t - I_{m^\\ast i^\\ast})$$ 否则 $$I_{m^\\ast i^\\ast} \\gets I_{m^\\ast i^\\ast} - \\eta(x_t - I_{m^\\ast i^\\ast})$$ 重复 2, 3 直到收敛. 这里 $\\eta$ 是学习率.\n在 $\\eta=1$ 时, LVQ 算法相当于逐步地进行 k-means 算法.\n在最近邻算法和其扩展方法中, 每个簇的代表点也称为相应单元的原型. 这种方法也常被称作原型方法或免模型方法.\n","date":"2025-03-14T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/knn/","title":"机器学习基础(4) —— 基于近邻的分类方法"},{"content":"后验概率最大化准则 定义\n对训练样本集 $D=\\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathcal{X}$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_K\\}$, 将 $x$ 的类别预测为 $c_i$ 的 风险 为\n$$ R(Y=c_i | x) = \\sum_{j=1}^K \\lambda_{ij} P(Y=c_j | x) $$其中 $\\lambda_{ij}$ 是将属于 $c_j$ 的样本预测为 $c_i$ 的损失. 最优预测 $\\hat{y}$ 是使得风险最小的类别, 即\n$$ \\hat{y} = \\argmin_{c_i} R(Y=c_i | x) $$ 假设采用 $0-1$ 损失函数, 易知\n$$ R(Y=c_i | x) = 1 - P(Y=c_i | x) $$即输入 $x$ 的最优预测 $\\hat{y}$ 为使得后验概率 $P(y | x)$ 最大的类别.\n逻辑斯蒂回归模型 定义\n设 $\\mathcal{X}=\\mathbb{R}^n, \\mathcal{Y}=\\{c_1,c_2\\}$, 逻辑斯蒂回归模型 是如下的后验概率分布:\n$$ \\begin{aligned} P(Y=c_1 | x) \u0026= \\frac{\\exp(w \\cdot x + b)}{1+\\exp(w \\cdot x + b)} \\\\ P(Y=c_2 | x) \u0026= \\frac{1}{1+\\exp(w \\cdot x + b) } \\end{aligned} $$其中 $w,b$ 是模型参数.\n按照后验概率最大化准则, 显然当 $w \\cdot x + b \u003e 0$ 时, 预测为 $c_1$, 否则预测为 $c_2$.\n对于多类分类任务, 仍然可以使用逻辑斯蒂回归模型:\n$$ \\begin{aligned} p(y=c_i | x) \u0026= \\frac{\\exp(w_i \\cdot x + b_i)}{\\sum_{j=1}^{K-1} \\exp(w_j \\cdot x + b_j)}, \\quad i=1,2,\\cdots,K-1 \\\\ p(y=c_K | x) \u0026= \\frac{1}{\\sum_{j=1}^{K-1} \\exp(w_j \\cdot x + b_j)} \\end{aligned} $$给定 $D=\\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{0,1\\}$, 用 $\\theta=(w,b)$ 表示二项逻辑斯蒂回归模型的参数, 令\n$$ p(x;\\theta) = p(Y=1 | x;\\theta) $$则考虑似然函数为\n$$ \\begin{aligned} L(\\theta) \u0026= \\prod_{i=1}^n p(x_i;\\theta)^{y_i} (1-p(x_i;\\theta))^{1-y_i} \\\\ \\log L(\\theta) \u0026= \\sum_{i=1}^n y_i \\log p(x_i;\\theta) + (1-y_i) \\log (1-p(x_i;\\theta)) \\\\ \u0026= \\sum_{i=1}^N y_i(w \\cdot x_i + b) - \\log(1+\\exp(w \\cdot x_i + b)) \\end{aligned} $$对 $w,b$ 求偏导为 $0$, 得到\n$$ \\begin{aligned} \\frac{\\partial \\log L(\\theta)}{\\partial w} \u0026= \\sum_{i=1}^n x_i(y_i - p(x_i;\\theta)) = 0\\\\ \\frac{\\partial \\log L(\\theta)}{\\partial b} \u0026= \\sum_{i=1}^n (y_i - p(x_i;\\theta)) = 0 \\end{aligned} $$朴素 Bayes 分类器 定理Bayes 公式\n$$ \\begin{aligned} P(Y=c_i | x) \u0026= \\frac{P(x | Y=c_i) P(Y=c_i)}{P(x)} \\\\ \u0026= \\frac{P(x | Y=c_i) P(Y=c_i)}{\\sum_{j=1}^K P(x | Y=c_j) P(Y=c_j)} \\end{aligned} $$ 朴素 Bayes 假定特征之间相互独立, 即\n$$ p(X^1=x^1, X^2=x^2, \\cdots, X^n=x^n | Y=c_k) = \\prod_{j=1}^n p(X^j=x^j | Y=c_k) $$对于输入实例 $x=(x^1,x^2,\\cdots,x^n)$, 则后验概率\n$$ p(Y=c_k|x)=\\frac{\\left( \\prod_{i=1}^n p(X^i=x^i | Y=c_k) \\right) P(Y=c_k)}{\\sum_{j=1}^K \\left( \\prod_{i=1}^n p(X^i=x^i | Y=c_j) \\right) P(Y=c_j)} $$分母是固定的, 只需比较分子的大小即可. 但是一旦某个特征取值和分类没有同时出现, 后验概率直接为 $0$, 为了避免这种情况, 通常引入一些平滑技术:\n$$ p_{\\lambda}(Y=c_k) = \\frac{\\sum_{j=1}^NI(y_j=c_k)+\\lambda}{N+K\\lambda} $$$\\lambda=1$ 时称为 Laplace 平滑.\n","date":"2025-03-11T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/bayes/","title":"机器学习基础(3) —— 基于后验概率最大化准则的分类模型"},{"content":"虚函数 在类的定义中, 前面有 virtual 关键字的成员函数就是 虚函数. virtual 只用在类定义里的函数声明中, 写函数体时不用.\n派生类的指针可以赋给基类指针. 通过基类指针调用基类和派生类中的同名同参 虚 函数时:\n若该指针指向一个基类的对象, 那么被调用是基类的虚函数; 若该指针指向一个派生类的对象, 那么被调用的是派生类的虚函数. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class CBase { public: virtual void someVirtualFunction() { cout \u0026lt;\u0026lt; \u0026#34;base function\u0026#34; \u0026lt;\u0026lt; endl; } }; class CDerived : public CBase { public: virtual void someVirtualFunction() { cout \u0026lt;\u0026lt; \u0026#34;derived function\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { CDerived ODerived; CBase *p = \u0026amp;ODerived; p-\u0026gt;someVirtualFunction(); // derived function return 0; } 派生类的对象可以赋给基类引用. 类似于指针, 通过基类引用调用基类和派生类中的同名同参虚函数也是多态的.\n1 2 3 4 5 6 int main() { CDerived ODerived; CBase \u0026amp;r = ODerived; r.someVirtualFunction(); // derived function return 0; } 多态不能针对对象. 派生类中的虚函数的访问权限可以是 public, protected, private. 但是, 基类中的虚函数的访问权限不能是 private, 即使派生类中的虚函数的访问权限是 public. 反过来是可以的, 而且可以正常多态.\n实现原理 采用了动态联编的技巧. 每一个有虚函数的类 (或其派生类) 都有一个虚函数表，该类的任何对象中都放着虚函数表的指针. 虚函数表中列出了该类的虚函数地址. 多出来的 4 个字节就是用来放虚函数表的地址的. 在编译时, 调用语句被编译成对虚函数表的索引, 而不是对函数的直接调用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Base { public: int i; virtual void Print() { cout \u0026lt;\u0026lt; \u0026#34;Base:Print\u0026#34; \u0026lt;\u0026lt; endl; } }; class Derived : public Base { public: int n; virtual void Print() { cout \u0026lt;\u0026lt; \u0026#34;Drived:Print\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { Derived d; cout \u0026lt;\u0026lt; sizeof(Base) \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; sizeof(Derived); return 0; } 32 位系统下, 这个程序的输出是 8,12.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class A { public: virtual void func() { cout \u0026lt;\u0026lt; \u0026#34;A::func \u0026#34;; } }; class B : public A { public: virtual void func() { cout \u0026lt;\u0026lt; \u0026#34;B::func \u0026#34;; } }; int main() { A a; A *pa = new B(); pa-\u0026gt;func(); // 多态, B::func // 64 位程序 long long *p1 = (long long *)\u0026amp;a; long long *p2 = (long long *)pa; // 篡改了虚函数表指向 *p2 = *p1; pa-\u0026gt;func(); // A::func return 0; } 虚析构函数 在非构造/析构函数中调用虚函数时, 调用的是当前对象的虚函数, 而不是基类的虚函数, 是多态. 在构造/析构函数中调用虚函数时, 调用的是当前类的虚函数, 编译时确定, 不是多态.\n通过基类的指针删除派生类对象时，通常情况下只调用基类的析构函数. 为了解决这个问题, 可以把基类的析构函数声明为虚函数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class son { public: virtual ~son() { cout \u0026lt;\u0026lt; \u0026#34;bye from son\u0026#34; \u0026lt;\u0026lt; endl;} }; class grandson : public son { public: ~grandson() { cout \u0026lt;\u0026lt; \u0026#34;bye from grandson\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { son *pson; pson = new grandson(); delete pson; return 0; } 此时, 会先调用派生类的析构函数, 再调用基类的析构函数. 另外注意, 构造函数不能是虚函数.\n纯虚函数和抽象类 如果在虚函数后面加上 = 0, 则该虚函数是纯虚函数. 纯虚函数不可以有函数体, 只有声明.\n1 2 3 4 5 class A { public: virtual void print() = 0; // 纯虚函数 void fun() { cout \u0026lt;\u0026lt; \u0026#34;fun\u0026#34;; } } 一个类中有纯虚函数的类叫 抽象类. 抽象类不能实例化, 只能作为基类, 不过可以作为指针或引用类型. 在抽象类的成员函数内可以调用纯虚函数, 但是在构造函数或析构函数内部不能调用纯虚函数.\n","date":"2025-03-07T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/polymorphism/","title":"程序设计实习(4) —— 多态"},{"content":"基本概念 在定义一个新的类 B 时, 如果 B 类与拥有已有类 A 的全部特点, 那么就可以把 A 作为一个基类, 而把 B 作为基类的一个 派生类 (也称 子类). 派生类可以对基类:\n扩充: 在派生类中, 可以添加新的成员变量和成员函数 修改: 在派生类中, 可以重新编写从基类继承得到的成员 派生类一经定义后, 可以独立使用, 不依赖于基类.\n派生方式说明符：public, private, protected. 派生类的写法:\n1 2 3 class Derived : public Base { // code }; 关于内存上, 派生类对象的大小等于基类对象的大小加上派生类对象自己的成员变量的大小. 在派生类对象中包含着基类对象, 且基类对象的存储位置位于派生类对象新增的成员变量之前.\n覆盖 派生类可以定义一个和基类成员同名的成员, 这叫覆盖. 在派生类中访问这类成员时, 缺省的情况是访问派生类中定义的成员. 要在派生类中访问由基类定义的同名成员时, 要使用作用域符号::.\n注意, 对于成员变量, 在内存中, 两个变量占用不同的空间. 并不建议覆盖成员变量.\n关于权限及派生方式的访问权限, 可以参考下表:\n派生方式\\成员访问 public protected private public public protected 不可访问 protected protected protected 不可访问 private private private 不可访问 为此, 可以用基类构造函数初始化派生类对象的基类部分. 例如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Bug { private: int nLegs; int nColor; protected: int nType; public: Bug(int legs, int color); void PrintBug() {} }; class FlyBug : public Bug { int nWings; public: FlyBug(int legs, int color, int wings); }; Bug::Bug(int legs, int color) { nLegs = legs; nColor = color; } FlyBug::FlyBug(int legs, int color, int wings) : Bug(legs, color) { nType = 1; nWings = wings; } 派生类的对象生命周期 在创建派生类的对象时, 需要调用基类的构造函数：初始化派生类对象中从基类继承的成员. 在执行一个派生类的构造函数之前, 总是先执行基类的构造函数. 派生类的析构函数被执行时, 执行完派生类的析构函数后, 自动调用基类的析构函数.\n在创建派生类的对象时:\n先执行基类的构造函数, 用以初始化派生类对象中从基类继承的成员; 再执行成员对象类的构造函数, 用以初始化派生类对象中成员对象; 最后执行派生类自己的构造函数. 在派生类对象消亡时：\n先执行派生类自己的析构函数; 再依次执行各成员对象类的析构函数; 最后执行基类的析构函数. 总之, 析构函数的调用顺序与构造函数的调用顺序相反.\n对于 public 继承具有赋值兼容规则, 即\n1 2 3 4 class Base {}; class Derived : public Base {}; Base b; Derived d; 派生类对象可以赋值给基类对象\n1 b = d; 派生类对象的地址可以赋值给基类指针\n1 Base *pb = \u0026amp;d; 派生类对象的引用可以赋值给基类引用\n1 Base \u0026amp;rb = d; 注意, 只有 public 继承才具有赋值兼容规则, protected 和 private 不允许这种赋值方式.\n指针强转成基类后, 就不再能访问派生类的成员了. 如果需要, 可以再利用指针强转回来.\n1 2 3 Derived objDerived; Base *ptrBase = \u0026amp;objDerived; Derived *ptrDerived = (Derived *)ptrBase; 在声明派生类时，只需要列出它的直接基类, 派生类沿着类的层次自动向上继承它的间接基类. 构造的顺序即为派生类的继承顺序, 析构的顺序则相反.\n","date":"2025-03-05T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/inherit/","title":"程序设计实习(3) —— 继承与派生"},{"content":"线性可分支持向量机 定义\n对于一个数据集 $D$, 如果能找到一个超平面 $H: w^Tx + b = 0$, 将数据分为两类. 即对任意 $(x_i, y_i) \\in D$, 若 $y_i = 1$, 则 $w^Tx_i + b \\geq 0$; 若 $y_i = -1$, 则 $w^Tx_i + b \u003c 0$. 则称 $D$ 是 线性可分的 , 超平面 $H$ 是 $D$ 的一个 分离超平面.\n最优超平面不仅要能够将数据分开, 还要使得两类数据点到超平面的距离尽可能远.\n考虑到 $w,b$ 任意缩放都不影响超平面的位置, 我们可以规定 $w^Tx + b = 1$ 为最近的正类数据点满足的方程. 此时距离为 $1/{\\|w\\|}$, 要最大化这个量, 即化归成凸二次规划问题:\n$$ \\begin{aligned} \u0026 \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1, \\quad i = 1, 2, \\cdots, n \\end{aligned} $$只要 $D$ 是线性可分的, 上述问题一定有解且唯一. 对应的分类决策函数\n$$ f(x) = \\text{sign}(w^Tx + b) $$称为 线性可分支持向量机.\n引入 Lagrange 乘子 $\\alpha_i \\geq 0$:\n$$ L(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b) - 1) $$对 $w, b$ 求偏导为 $0$, 得到\n$$ \\begin{aligned} \u0026 w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\ \u0026 0 = \\sum_{i=1}^n \\alpha_i y_i \\end{aligned} $$代入 $L(w, b, \\alpha)$, 得到对偶问题:\n线性可分对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j \\\\ \u0026 \\text{s.t.} \\quad \\alpha_i \\geq 0, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 由 KKT 条件, 最优解一定满足\n$$ \\begin{aligned} \\alpha_i(y_i(w \\cdot x_i + b) - 1) \u0026= 0 \\\\ y_i(w \\cdot x_i + b) - 1 \u0026\\geq 0 \\\\ \\alpha_i \u0026\\geq 0 \\\\ \\end{aligned} $$由于 $\\alpha_i$ 不全为 $0$, 存在 $j$ 使得 $y_j(w \\cdot x_j + b) = 1$, 由此\n$$ b = y_j - w \\cdot x_j = y_j - \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x_j $$乘上 $\\alpha_jy_j$ 做累和, 有\n$$ 0=\\sum_{j=1}^n \\alpha_jy_jb = \\sum_{j=1}^n \\alpha_j - \\| w \\|^2 $$上式中 $\\alpha_i=0$ 的 $i$ 也成立, 因为都是 $0$ 不影响结果. 注意到 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$ 也只收到 $\\alpha_i \u003e 0$ 的影响, 而这些项的点都落在间隔边界\n$$ H_1: w \\cdot x + b = 1, \\quad H_2: w \\cdot x + b = -1 $$上, 称这些点 $x_i$ 为 支持向量.\n支持向量机的留一误差\n$$ \\hat{R}_{\\text{loo}} = \\frac{1}{n} \\sum_{i=1}^n I(f_{D-\\{x_i\\}}(x_i) \\neq y_i) $$则 $\\hat{R}_{\\text{loo}} \\le N_{SV}/n$, 其中 $N_{SV}$ 为支持向量的个数.\n线性支持向量机 要求 $D$ 线性可分有点苛刻. 容忍一些误差, 引入松弛变量 $\\xi_i \\geq 0$, 使得约束条件变为\n$$ y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i $$对于被错误分类的点, $\\xi_i$ 可以大于 $1$. 把 $\\xi_i \\ne 0$ 的点视为特异点, 那么希望特异点尽可能少, 于是优化目标变为\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n I(\\xi_i \\ne 0) \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$直接用 $\\xi_i$ 代替 $I(\\xi_i \\ne 0)$, 问题变为\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$既然要 $\\xi_i$ 尽可能小, 不妨取 $\\xi_i = 1 - y_i(w \\cdot x_i + b)$, 引入合页损失函数 $h(z) = \\max(0, 1-z)$, 即\n$$\\xi_i = h(y_i(w \\cdot x_i + b))$$则提出一个 $C$ 后, 优化目标变为\n$$ \\min_{w, b} \\frac{1}{2C} \\|w\\|^2 + \\sum_{i=1}^n h(y_i(w \\cdot x_i + b)) $$做了这么多, 只是相当于把 0-1 损失函数换成了合页损失函数.\n回到原问题, 引入 Lagrange 乘子 $\\alpha_i, \\beta_i \\geq 0$, 得到\n$$ L(w, b, \\xi, \\alpha, \\beta) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b) - 1 + \\xi_i) - \\sum_{i=1}^n \\beta_i \\xi_i $$对 $w, b, \\xi$ 偏导为 $0$, 得到\n$$ \\begin{aligned} \u0026 w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\ \u0026 0 = \\sum_{i=1}^n \\alpha_i y_i \\\\ \u0026 \\beta_i = C - \\alpha_i \\end{aligned} $$代入 $L(w, b, \\xi, \\alpha, \\beta)$, 得到对偶问题\n线性支持向量机对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 与线性可分支持向量机类似, 只是多了一个 $\\alpha_i \\leq C$ 的约束. 现在考虑 KKT 条件, 有\n$$ \\begin{aligned} \\alpha_i(y_i(w \\cdot x_i + b) - 1 + \\xi_i) \u0026= 0 \\\\ y_i(w \\cdot x_i + b) - 1 + \\xi_i \u0026\\geq 0 \\\\ \\beta_i \\xi_i \u0026= 0 \\\\ \\alpha_i \u0026\\geq 0 \\\\ \\beta_i \u0026\\geq 0 \\\\ \\alpha_i + \\beta_i\u0026=C \\end{aligned} $$则 $\\alpha_i \u003e 0$ 的点 $x_i$ 为支持向量, 满足 $y_i(w \\cdot x_i + b) = 1 - \\xi_i$. 这点与线性可分支持向量机的支持向量不同. 但进一步如果 $\\alpha_i \\lt C$ , 则 $\\beta_i \\gt 0$, 则 $\\xi_i=0$, 从而 $y_i(w \\cdot x_i + b) = 1$, 这样就一致了.\n进一步, 把 $y_i(w \\cdot x_i + b) = 1$ 两边乘 $y_i$, 类似有\n$$ b = y_j - \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x_j $$因而最优分类超平面为\n$$ \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x + b = 0 $$和决策函数\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x + b\\right) $$超平面法向量可以被唯一确定, 但是偏置不唯一.\nSMO 算法 SMO 算法是一种启发式算法, 用于求解支持向量机的对偶问题. SMO 算法的基本思想是: 每次选择两个变量, 固定其他变量, 优化这两个变量. 这样不断迭代, 直到收敛.\n设当前迭代的两个变量为 $\\alpha_i, \\alpha_j$, 则\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = -\\sum_{i=3}^n \\alpha_i y_i $$同乘 $y_1$, 有\n$$ \\alpha_1 + \\alpha_2 y_1y_2= -\\sum_{i=3}^n \\alpha_i y_1y_i $$记右边为 $\\gamma$, $s=y_1y_2 \\in \\{-1, 1\\}$, 则\n$$ \\alpha_1 + s\\alpha_2 = \\gamma $$记$K_{ij} = x_i \\cdot x_j$, $v_i = \\sum_{j=3}^{N} \\alpha_j y_j K_{ij}$, 则对偶问题转化为\n$$ \\begin{aligned} \u0026 \\max_{\\alpha_1, \\alpha_2} \\alpha_1 + \\alpha_2 - \\frac{1}{2} K_{11}\\alpha_1^2 - \\frac{1}{2} K_{22}\\alpha_2^2 - sK_{12}\\alpha_1\\alpha_2 - y_1v_1\\alpha_1 - y_2v_2\\alpha_2 \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\alpha_1 + s\\alpha_2 = \\gamma \\end{aligned} $$再由 $\\alpha_1 = \\gamma - s\\alpha_2$, 代入目标函数, 并对 $\\alpha_2$ 求导为 $0$, 得到\n$$ \\alpha_2 = \\frac{s(K_{11}-K_{12})\\gamma + y_2(v_1 - v_2) - s + 1}{K_{11} + K_{22} - 2K_{12}} $$代入 $v$ 的定义, 随后化简得\n$$ \\alpha_2 = \\alpha_2^* + y_2 \\frac{(y_2 - f(x_2))- (y_1-f(x_1))}{K_{11} + K_{22} - 2K_{12}} $$别忘了约束 $0 \\le \\alpha_1, \\alpha_2 \\le C$, 以及 $\\alpha_1 + s\\alpha_2 = \\gamma$, 对 $\\alpha_2$ 进行裁剪为 $\\alpha_2^{\\text{clip}}$. 相应地,\n$$ \\alpha_1 = \\alpha_1^* + s(\\alpha_2^* - \\alpha_2^{\\text{clip}}) $$最后, 更新 $b$. 假设在 $\\alpha_1, \\alpha_2$ 中, $0 \\lt \\alpha_i \\lt C$, 则\n$$ b = y_i - \\sum_{j=1}^n \\alpha_j y_j K_{ij} $$关于选取 $\\alpha_1, \\alpha_2$, 一般有两个原则:\n选择违反 KKT 条件最严重的两个变量. 选择两个变量使得目标函数有最大变化. 核方法和非线性支持向量机 对于非线性问题, 可以通过核方法将数据映射到高维空间, 从而在高维空间中找到一个线性超平面.\n假设有一个映射 $\\phi: \\mathcal{X} \\mapsto \\mathcal{Z}$, 则在 $\\mathcal{Z}$ 的线性支持向量机变为:\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$对应的对偶问题为\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\phi(x_i) \\cdot \\phi(x_j) \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$相应的分类决策函数为\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) \\cdot \\phi(x) + b\\right) $$然而, 直接计算 $\\phi(x_i) \\cdot \\phi(x_j)$ 的复杂度很高. 为此, 引入核函数\n定义\n设 $\\mathcal{X}$ 是输入空间, $\\mathcal{Z}$ 是特征空间, 如果存在一个从 $\\mathcal{X}$ 到 $\\mathcal{Z}$ 的映射 $\\phi$, 使得对任意 $x, x' \\in \\mathcal{X}$, 都有\n$$ K(x, x') = \\phi(x) \\cdot \\phi(x') $$则称 $K$ 为 核函数.\n注意, 这里我们不再需要显式地计算 $\\phi(x_i)$, 因为结果只与 $K(x_i, x_j)$ 有关.\n非线性支持向量机对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 此时, 分类决策函数为\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\\right) $$ 定义\n$\\mathcal{X}$ 上的函数 $K: \\mathcal{X} \\times \\mathcal{X} \\mapsto \\mathbb{R}$ 称为 正定对称核函数, 如果对任意 $x_1, x_2, \\cdots, x_n \\in \\mathcal{X}$, 核矩阵 (Gram 矩阵) $[K_{ij}]_{m \\times m}$ 是半正定的.\n常见的核函数有:\n线性核函数: $K(x, x') = x \\cdot x'$, 对应线性支持向量机. 多项式核函数: $K(x, x') = (x \\cdot x' + 1)^d, c \\gt 0$ 高斯核函数: $K(x, x') = \\exp\\left(-\\frac{\\|x-x'\\|^2}{2\\sigma^2}\\right), \\sigma \\gt 0$ ","date":"2025-02-28T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/","title":"机器学习基础(2) —— 支持向量机"},{"content":"对象间的运算和结构变量一样, 对象之间可以用 = 进行赋值, 但是不能用 ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;= 进行比较, 除非这些运算符经过了 \u0026ldquo;重载\u0026rdquo;. 运算符重载的实质是函数重载.\n重载为普通函数和成员函数均可. 重载为成员函数时, 重载函数的参数个数比运算符的操作数少一个.\n1 2 3 4 5 6 7 8 9 10 11 12 class Complex { public: double real, imag; Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} Complex operator-(const Complex \u0026amp;c); }; Complex operator+(const Complex \u0026amp;a, const Complex \u0026amp;b) { return Complex(a.real + b.real, a.imag + b.imag); } Complex Complex::operator-(const Complex \u0026amp;c) { return Complex(real - c.real, imag - c.imag); }; 重载 = 有时候希望赋值运算符两边的类型可以不匹配, 比如把一个 char * 类型的字符串赋值给一个字符串对象, 此时就需要重载赋值运算符 =. 赋值运算符 = 只能重载为成员函数, 通常返回类型为 *this 的引用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class String { char *str; public: String() : str(new char[1]) { str[0] = 0; } const char *c_str() { return str; } String \u0026amp;operator=(const char *s); ~String() { delete[] str; } }; String \u0026amp;String::operator=(const char*s) { delete[] str; str = new char[strlen(s) + 1]; strcpy(str, s); return *this; } int main() { String s; s = \u0026#34;Good Luck,\u0026#34;; // s.operator=(\u0026#34;Good Luck,\u0026#34;); cout \u0026lt;\u0026lt; s.c_str() \u0026lt;\u0026lt; endl; // String s2 = \u0026#34;hello!\u0026#34;; // 这是构造函数, 不是赋值 return 0; } 如不定义自己的赋值运算符, 那么 S1 = S2 实际上导致 S1.str 和 S2.str 指向同一地方. 为此要做深拷贝.\n1 2 3 4 5 6 7 8 9 10 11 12 13 String \u0026amp;operator=(const String \u0026amp;s) { // 防止自赋值导致问题 if (this == \u0026amp;s) return *this; delete[] str; str = new char[strlen(s.str) + 1]; strcpy(str, s.str); return *this } String(const String \u0026amp;s) { // 复制构造函数也要深拷贝 str = new char[strlen(s.str) + 1]; strcpy(str, s.str); } 也可以重载为友元函数.\n1 2 3 4 5 6 7 8 9 class Complex{ double real, imag; public: Complex(double r, double i) : real(r), imag(i) {} Complex operator+(double r); }; Complex Complex::operator+(double r) { return Complex(real + r, imag); } 这个可以解决 c = a + 2.5 的问题, 但是不能解决 c = 2.5 + a 的问题. 为此要重载为普通函数, 但普通函数又不能访问私有成员, 用友元函数.\n1 2 3 4 5 6 7 8 Complex operator+(double r, const Complex \u0026amp;c); class Complex { // --skip-- friend Complex operator+(double r, const Complex \u0026amp;c); }; Complex operator+(double r, const Complex \u0026amp;c) { return Complex(c.real + r, c.imag); } 重载流插入运算符 \u0026lt;\u0026lt; 和流提取运算符 \u0026gt;\u0026gt; cout 实际上是一个在 iostream 中定义的, ostream 类的对象.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Complex { double real, imag; public: Complex(double r = 0, double i = 0) : real(r), imag(i) {} friend ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Complex \u0026amp;c); friend istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;is, Complex \u0026amp;c); }; ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Complex \u0026amp;c) { os \u0026lt;\u0026lt; c.real \u0026lt;\u0026lt; \u0026#34;+\u0026#34; \u0026lt;\u0026lt; c.imag \u0026lt;\u0026lt; \u0026#34;i\u0026#34;; // 以\u0026#34;a+bi\u0026#34; 的形式输出 return os; } istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;is, Complex \u0026amp;c) { string s; is \u0026gt;\u0026gt; s; // 将 \u0026#34;a+bi\u0026#34; 作为字符串读入 int pos = s.find(\u0026#34;+\u0026#34;, 0); string sTmp = s.substr(0, pos); // 分离出代表实部的字符串 c.real = atof(sTmp.c_str()); // atof 库函数能将 const char* 指针指向的内容转换成 float sTmp = s.substr(pos + 1, s.length() - pos - 2); // 分离出代表虚部的字符串 c.imag = atof(sTmp.c_str()); return is; } 重载类型转换运算符 没有返回值, 因为转换函数的返回值类型就是要转换的类型.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { double real, imag; public: Complex(double r = 0, double i = 0) : real(r), imag(i) {} operator double() { return real; } }; int main() { Complex c(1.2, 3.4); cout \u0026lt;\u0026lt; (double)c \u0026lt;\u0026lt; endl; // 输出 1.2 double n = 2 + c; // double n = 2 + c.operator double() cout \u0026lt;\u0026lt; n; // 输出 3.2 return 0; } 重载自增自减运算符 ++ 和 -- 自增运算符 ++, 自减运算符 -- 有前置/后置之分, 为了区分所重载的是前置运算符还是后置运算符, C++ 规定前置运算符作为一元运算符重载, 后置运算符作为二元运算符重载, 多写一个没用的参数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 前置, 先加减再返回, 返回引用 // 重载为成员函数 T\u0026amp; operator++(); T\u0026amp; operator--(); // 重载为全局函数 T\u0026amp; operator++(T\u0026amp;); T\u0026amp; operator--(T\u0026amp;) // 后置, 先返回再加减, 返回值 // 重载为成员函数 T operator++(int); T operator--(int); // 重载为全局函数 T operator++(T\u0026amp;, int); T operator--(T\u0026amp;, int); 重载 -\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class A { private: int x; public: A() : x(5) {} int getX() { return x; } A *operator-\u0026gt;() { return this; } }; int main() { A a; cout \u0026lt;\u0026lt; a-\u0026gt;getX() \u0026lt;\u0026lt; endl; // a.operator-\u0026gt;()-\u0026gt;getX() return 0; } 看起来似乎没有什么用, 但是可以用来实现智能指针.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class client { public: int a; client(int x) : a(x) {} }; class proxy { client *target; public: proxy(client *t) : target(t) {} client *operator-\u0026gt;() const { return target; } }; class proxy2 { proxy *target; public: proxy2(proxy *t) : target(t) {} proxy \u0026amp;operator-\u0026gt;() const { return *target; } }; int main() { client x(3); proxy y(\u0026amp;x); proxy2 z(\u0026amp;y); cout \u0026lt;\u0026lt; x.a \u0026lt;\u0026lt; y-\u0026gt;a \u0026lt;\u0026lt; z-\u0026gt;a; // print \u0026#34;333\u0026#34; return 0; } 注意事项 运算符重载不改变运算符的优先级.\n以下运算符不能重载: ., .*(成员函数指针), ::, ?:(三目运算符), sizeof.\n重载运算符 (), [], -\u0026gt; 或者赋值运算符 = 时，运算符重载函数必须声明为类的成员函数.\n重载运算符是为了让它能作用于对象, 因此重载运算符不允许操作数都不是对象 (有一个是枚举类型也可以).\n1 void operator+(int a, char* b); // 错误 ","date":"2025-02-26T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/operator-overload/","title":"程序设计实习(2) —— 运算符重载"},{"content":"面向对象的程序设计 面向对象的程序设计方法:\n将某类客观事物共同特点 (属性) 归纳出来, 形成一个数据结构 (可以用多个变量描述事物的属性); 将这类事物所能进行的行为也归纳出来, 形成一个个函数, 这些函数可以用来操作数据结构. 面向对象的特点有 抽象, 封装, 继承, 多态.\n一般来说, 对象所占用的内存空间的大小, 等于所有成员变量的大小之和.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class CRectangle { public: int w, h; int Area() { return w * h; } int Perimeter() { return 2 * (w + h); } void Init(int w_, int h_) { w = w_; h = h_; } }; // 必须有分号 int main() { int w, h; CRectangle r; // r 是一个对象 cin \u0026gt;\u0026gt; w \u0026gt;\u0026gt; h; r.Init(w, h); cout \u0026lt;\u0026lt; r.Area() \u0026lt;\u0026lt; endl \u0026lt;\u0026lt; r.Perimeter(); return 0; } 和结构变量一样, 对象之间可以用 = 进行赋值, 但是不能用 ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;= 进行比较, 除非这些运算符经过了 重载.\n对象名.成员名 1 2 3 CRectangle r1, r2; r1.w = 5; r2.Init(5, 4); 指针-\u0026gt;成员名 1 2 3 4 5 CRectangle r1, r2; CRectangle *p1 = \u0026amp;r1; CRectangle *p2 = \u0026amp;r2; p1-\u0026gt;w = 5; p2-\u0026gt;Init(5, 4); 引用名.成员名 1 2 3 CRectangle r1; CRectangle \u0026amp;rr = r1; rr.w = 5; 引用 引用名是对象名的别名, 指向同一个对象. 语法: 类名 \u0026amp;引用名 = 对象名; 引用的好处是可以减少指针的使用, 使得代码更加简洁.\n1 2 3 4 5 6 7 8 9 // 要用指针, 否则参数传递会产生拷贝 void swap(int *a, int *b) { int tmp; tmp = *a; *a = *b; *b = tmp; } int n1, n2; swap(\u0026amp;n1, \u0026amp;n2); 可以改写为:\n1 2 3 4 5 6 7 8 void swap(int \u0026amp;a, int \u0026amp;b) { int tmp; tmp = a; a = b; b = tmp; } int n1, n2; swap(n1, n2); 引用还可以作为函数的返回值.\n1 2 3 4 5 6 int \u0026amp;ref(int \u0026amp;a) { return a; } int n = 5; ref(n) = 6; // n = 6 常量, 常引用, 常量指针 定义引用时, 前面加 const 关键字, 表示 常引用. 不能通过常引用修改对象的值, 即只读引用.\n1 2 3 4 int n; const int \u0026amp;r = n; r = 5; // 错误 n = 5; // 正确 const T\u0026amp; 和 T\u0026amp; 是不同的类型. T\u0026amp; 类型的引用或 T 类型的变量可以用来初始化 const T\u0026amp; 类型的引用. const T 类型的常变量和 const T\u0026amp; 类型的引用则不能用来初始化 T\u0026amp; 类型的引用, 除非进行强制类型转换. 不可通过常量指针修改其指向的内容, 但可以修改指针的指向 (引用不可以).\n1 2 3 4 5 int n, m; const int *p = \u0026amp;n; *p = 5; // 错误 n = 5; // 正确 p = \u0026amp;m; // 正确 函数参数为常量指针时, 可避免函数内部不小心改变参数指针所指地方的内.\n1 2 3 4 void myPrintf(const int *p) { *p = 5; // 错误 p = \u0026amp;m; // 正确 } 类成员的访问控制 public: 公有成员, 可以在类的外部访问. private: 私有成员, 只能在类的内部访问, 缺省默认为 private. protected: 保护成员, 只能在类的内部和派生类中访问. 1 2 3 4 5 6 7 8 9 class className { // 这三个关键字可以出现多次, 没有顺序要求 private: // 私有属性和函数 public: // 公有属性和函数 protected: // 保护属性和函数 }; 类内部可以访问当前对象和同类其他对象的私有成员.\n\u0026ldquo;隐藏\u0026rdquo; 的目的是强制对成员变量的访问一定要通过成员函数进行, 那么以后成员变量的类 型等属性修改后, 只需要更改成员函数即可.\nstruct 和 class 的唯一区别是默认的访问控制权限不同, struct 默认为 public, class 默认为 private.\n函数重载和缺省参数 函数名相同, 参数个数或类型不同, 注意没有返回值类型不同.\n1 2 3 double _max(double f1, double f2); int _max(int n1, int n2); int _max(int n1, int n2, int n3); C++ 中, 定义函数的时候可以让 最右边 的连续若干个参数有缺省值, 那么调用函数的时候, 若相应位置不写参数, 参数就是缺省值.\n1 2 3 4 void func(int a, int b = 0, int c = 0); func(1); // a = 1, b = 0, c = 0 func(1, 2); // a = 1, b = 2, c = 0 func(1, , 8); // 错误 成员函数也可以重载或有缺省参数.\n构造函数 构造函数是一种特殊的成员函数: 名字与类名相同, 没有返回值 (void 也不行). 作用是初始化对象的数据成员.\n如果定义类时没有定义构造函数, 编译器会生成一个默认的无参构造函数. 如果定义了, 默认构造函数就不会生成. 对象生成时, 构造函数自动调用. 生成之后不能再执行构造函数. 一个类可以有多个构造函数, 可以重载.\n1 2 3 4 5 6 7 8 9 10 11 class Complex { private: double real; double imag; public: void Set(double r, double i); }; //编译器自动生成默认构造函数 // 两种写法均可. Complex c1; Complex *pc = new Complex; 手动加入构造函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Complex { private: double real; double imag; public: Complex(double r, double i = 0) { // 带缺省参数 real = r; imag = i; } }; Complex *pc1 = new Complex; // error, 没有参数 Complex c2(2); // OK Complex *pc2 = new Complex(3, 4); 构造当然也可以 private, 这样就不能用来生成对象, 但是可以用来实现单例模式.\n构造函数还可以用在数组.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class CSample { int x; public: CSample() {} CSample(int n) { x = n; } CSample(int m, int n) { x = m + n; } }; int main() { CSample array1[2]; // 两次无参 CSample array2[2] = {4, CSample(5, 3)}; // 两次有参 CSample array3[2] = {3}; // 一次有参一次无参 CSample *array4 = new CSample[2]; delete[] array4; return 0; } 复制构造函数只有一个参数, 且参数是本类的引用 (或常量引用). 如果没有定义, 编译器会生成一个默认的复制构造函数. 如果定义了, 默认复制构造函数就不会生成.\n1 2 3 4 5 6 7 8 class Complex { private: double real; double imag; }; Complex c1; Complex c2(c1); // 默认复制构造函数 1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real; double imag; Complex() {} // 必须要写, 否则编译器不会生成默认构造函数 Complex(const Complex \u0026amp;c) { real = c.real; imag = c.imag; cout \u0026lt;\u0026lt; \u0026#34;Copy Constructor called\u0026#34;; } }; Complex c1; Complex c2(c1); // 自定义复制构造函数 复制构造函数的调用时机:\n用一个对象去初始化另一个对象.\n1 2 Complex c1; Complex c2(c1); 一个对象作为函数参数传递给一个非引用类型的参数.\n1 2 3 void func(Complex c); Complex c1; func(c1); 一个对象作为函数返回值返回.\n1 2 Complex func(); Complex c1 = func(); 注意: 对象之间的赋值操作, 不会调用复制构造函数.\n1 2 Complex c1, c2; c1 = c2; // 不会调用复制构造函数 考虑到对象作为函数参数会掉用复制构造函数, 为了避免不必要的开销, 可以使用引用传递.\n手动写复制构造函数的目的一般是为了实现深拷贝.\n转换构造函数的目的是实现类型的自动转换. 不以说明符 explicit 声明 {且可以用单个参数调用 (C++11 前)} 的构造函数被称为转换构造函数.\n1 2 3 4 5 6 7 8 9 10 11 12 class Complex { public: double real, imag; Complex(int i) { // 类型转换构造函数 real = i; imag = 0; } }; int main() { Complex c1 = 9; // 隐式调用, 转换成一个临时 Complex 对象 return 0; } 如果加了 explicit 关键字, 则只能显式调用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real, imag; explicit Complex(int i) { // 类型转换构造函数 real = i; imag = 0; } }; int main() { Complex c1 = 9; // 错误 Complex c2(9); // 正确 return 0; } 析构函数 析构函数是类的一个特殊成员函数, 名字由波浪号 ~ 加类名构成, 没有参数, 也没有返回值, 对象消亡时即自动被调用, 作用是释放对象所占用的资源.\n如果定义类时没写析构函数, 则编译器生成缺省析构函数.缺省析构函数什么也不做. 如果定义了析构函数, 缺省析构函数就不会生成. 一个类只有一个析构函数, 不能重载.\n在数组生命周期结束时, 编译器会自动调用数组中每个元素的析构函数. delete 一个对象时, 会调用对象的析构函数. (注意, new 数组要用 delete[])\n1 2 3 4 5 Ctest *pTest; pTest = new Ctest; // 构造函数调用 delete pTest; // 析构函数调用 pTest = new Ctest[3]; // 构造函数调用 3 次 delete[] pTest; // 析构函数调用 3 次 this 指针 this 是一个指向对象本身的指针. 把 car.foo() 翻译成 C 就是 foo(\u0026amp;car)\n1 2 3 4 5 6 7 8 9 class A { int i; public: void hello() { cout \u0026lt;\u0026lt; \u0026#34;hello\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { A *p = NULL; p-\u0026gt;hello(); } 能运行且输出, 但是是未定义行为. 而且一旦 hello() 中用到了 this, 就会出错.\n非静态成员函数中可以直接使用 this 来代表指向该函数作用的对象的指针.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real, imag; void print() { cout \u0026lt;\u0026lt; real \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; imag; } Complex(double r, double i) : real(r), imag(i) {} Complex addOne() { this-\u0026gt;real++; // 等价于 real++; this-\u0026gt;print(); // 等价于 print() return *this; // 返回对象本身 } }; 静态成员 静态成员即加了 static 关键字的成员.\n静态成员变量是类的所有对象共享的. sizeof 不包括静态成员变量. 本质是全局变量. 静态成员函数是类的所有对象共享的函数, 静态成员函数只能访问静态成员变量和静态成员函数, 不能访问普通成员变量和普通成员函数, 也不可以用 this 指针. 本质是全局函数.\n访问静态成员可以不通过对象访问. 类名::静态成员名.\n1 2 int Rectangle::edges = 4; Rectangle::printTotal(); 即使类的对象不存在, 静态成员变量也存在.\n成员对象和封闭类 有成员对象的类叫封闭类.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CTyre { private: int radius; int width; public: CTyre(int r, int w) : radius(r), width(w) {} }; class CEngine {}; class CCar { private: int price; // 价格 CTyre tyre; CEngine engine; public: CCar(int p, int tr, int tw); }; CCar::CCar(int p, int tr, int w) : price(p), tyre(tr, w) {} 这个例子 CCar 必须有构造函数, 因为 CTyre 和没有默认构造函数.\n封闭类对象生成时, 先执行所有对象成员的构造函数, 然后才执行封闭类的构造函数. 对象成员的构造函数调用次序和对象成员在类中的说明次序一致, 与它们在成员初始化列表中出现的次序无关. 当封闭类的对象消亡时, 先执行封闭类的析构函数, 然后再执行成员对象的析构函数. 次序和构造函数的调用次序相反. 封闭类的对象, 如果是用默认复制构造函数初始化的, 那么它里面包含的成员对象也会用复制构造函数初始化. 友元 友元分为友元函数和友元类两种.\n友元函数: 一个类的友元函数可以访问该类的私有成员.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CCar; // 提前声明 CCar 类, 以便后面的 CDriver 类使用 class CDriver { public: void modifyCar(CCar *pCar); }; class CCar { private: int price; friend int mostExpensiveCar(CCar cars[], int total); // 声明友元 friend void CDriver::modifyCar(CCar *pCar); // 声明友元 // 可以将一个类的成员函数 (包括构造/析构函数) 说明为另一个类的友元。 }; void CDriver::modifyCar(CCar *pCar) { pCar-\u0026gt;price += 1000; } int mostExpensiveCar(CCar cars[], int total) { int tmpMax = -1; for (int i = 0; i \u0026lt; total; ++i) if (cars[i].price \u0026gt; tmpMax) tmpMax = cars[i].price; return tmpMax; } 如果 A 是 B 的友元类, 那么 A 的成员函数可以访问 B 的私有成员.\n1 2 3 4 5 6 7 8 9 10 11 12 class CCar { private: int price; friend class CDriver; // 声明 CDriver 为友元类 }; class CDriver { public: CCar myCar; void modifyCar() { myCar.price += 1000; // 因 CDriver 是 CCar 的友元类, 故此处可以访问其私有成员 } }; 友元类之间的关系不能传递, 不能继承.\n常量对象, 常量成员函数 如果不希望某个对象的值被改变, 则定义该对象的时候可以在前面加 const 关键字变为常量对象.\n在类的成员函数说明后面可以加 const 关键字, 则该成员函数成为常量成员函数. 常量成员函数内部不能改变属性的值, 也不能调用非常量成员函数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Sample { private: int value; public: void func() {}; Sample() {} void SetValue() const { value = 0; // wrong func(); // wrong } }; const Sample Obj; Obj.SetValue(); // 常量对象上可以使用常量成员函数 对于\n1 2 3 4 5 6 int getValue() const { return n; } int getValue() { return 2 * n; } 两个函数, 名字和参数表都一样, 但是一个是 const, 一个不是, 算重载.\n加上 mutable 关键字的成员变量, 即使在常量成员函数中也可以被修改.\n1 2 3 4 5 6 7 8 9 10 class CTest { public: bool getData() const { m_n1++; return m_b2; } private: mutable int m_n1; bool m_b2; }; ","date":"2025-02-19T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/class-and-object/","title":"程序设计实习(1) —— 类和对象"},{"content":"基础数学工具 定义\n随机变量 $X$ 的 期望 $E[X]$ 定义为\n$$ E[X] = \\sum_{x} x \\cdot P(X=x) $$随机变量 $X$ 的 方差 $\\text{Var}(X)$ 定义为\n$$ \\text{Var}(X) = E[(X - E[X])^2] $$标准差 $\\sigma(X)$ 定义为\n$$ \\sigma(X) = \\sqrt{\\text{Var}(X)} $$ 定理Markov 不等式\n设 $X$ 是一个非负随机变量, 期望存在, 那么对于任意 $t \u003e 0$ 有\n$$ P(X \\geq t) \\leq \\frac{E[X]}{t} $$ 定理Chebyshev 不等式\n设 $X$ 是一个随机变量, 期望和方差都存在, 那么对于任意 $t \u003e 0$ 有\n$$ P(|X - E[X]| \\geq t) \\leq \\frac{\\text{Var}(X)}{t^2} $$ 定义\n随机变量 $X$ 和 $Y$ 的 协方差 $\\text{Cov}(X, Y)$ 定义为\n$$ \\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] $$如果 $\\text{Cov}(X, Y) = 0$, 则称 $X$ 和 $Y$ 不相关.\n协方差具有对称性, 双线性.\n定义\n随机向量 $X=(X_1, X_2, \\ldots, X_n)$ 的 协方差矩阵 $C(X)$ 定义为\n$$ C(X) = E[(X - E[X])(X - E[X])^T] = (\\text{Cov}(X_i, X_j))_{ij} $$ 定义\nGauss 分布 (正态分布) 的概率密度函数为\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) $$Laplace 分布 的概率密度函数为\n$$ f(x) = \\frac{1}{2b} \\exp(-\\frac{|x-\\mu|}{b}) $$ 最优化问题\n$$ \\begin{aligned} \u0026 \\min f(x) \\\\ \\text{s.t. } \u0026 c_i(x) \\leq 0, i = 1, 2, \\dots, k \\\\ \u0026 h_j(x) = 0, j = 1, 2, \\dots, l \\end{aligned} $$构造 Lagrange 函数\n$$ L(x, \\alpha, \\beta) = f(x) + \\sum_{i=1}^{k} \\alpha_i c_i(x) + \\sum_{j=1}^{l} \\beta_j h_j(x) $$引入 Karush-Kuhn-Tucker (KKT) 条件\n$$ \\begin{aligned} \u0026 \\nabla_x L(x, \\alpha, \\beta) = 0 \\\\ \u0026 c_i(x) \\leq 0, i = 1, 2, \\dots, k \\\\ \u0026 h_j(x) = 0, j = 1, 2, \\dots, l \\\\ \u0026 \\alpha_i c_i(x) = 0, i = 1, 2, \\dots, k \\\\ \u0026 \\alpha_i \\geq 0, i = 1, 2, \\dots, k \\end{aligned} $$基本概念和术语 定义\n监督学习: 基于标记数据 $T=\\{ (x_i,y_i) \\}_{i=1}^N$, 学习一个从输入空间到输出空间的映射 $f: \\mathcal{X} \\mapsto \\mathcal{Y}$. 利用此对未见数据进行预测. 通常分为 回归 和 分类 两类.\n无监督学习: 基于未标记数据 $T=\\{ x_i \\}_{i=1}^N$, 发现其中隐含的知识模式. 聚类 是典型的无监督学习任务.\n半监督学习: 既有标记数据又有未标记数据 (通常占比较大).\n强化学习: 通过观察环境的反馈, 学习如何选择动作以获得最大的奖励.\n模型评估与选择 损失函数 模型基于算法按照一定策略给出假设 $h \\in \\mathcal{H}$, 通过 损失函数 $L(h(x), y)$ 衡量假设的好坏.\n0-1 损失函数: $$L(h(x), y) = \\mathbb{I}(h(x) \\neq y) = \\begin{cases} 0, \u0026 h(x) = y \\\\ 1, \u0026 h(x) \\neq y \\end{cases}$$ 平方损失函数: $$L(h(x), y) = (h(x) - y)^2$$平均损失 $R(h) = E_{x \\sim D} [L(h(x), y)]$ 称为 泛化误差.\n容易验证, 对于 0-1 损失函数, 准确率 $a = 1-R(h)$.\n二分类 对于二分类问题, 样本预测结果有四种情况:\n真正例 (True Positive, TP): 预测为正例, 实际为正例. 假正例 (False Positive, FP): 预测为正例, 实际为负例. 真负例 (True Negative, TN): 预测为负例, 实际为负例. 假负例 (False Negative, FN): 预测为负例, 实际为正例. 由此引入\n准确率(查准率): $P = \\frac{TP}{TP+FP}$. 召回率(查全率): $R = \\frac{TP}{TP+FN}$. $F_1$ 度量: 考虑到二者抵触, 引入调和均值 $F_1 = \\frac{2PR}{P+R}$. 过拟合和正则化 为了防止由于模型过于复杂而导致的过拟合, 可以通过 正则化 方法来限制模型的复杂度.\n$$ \\min \\sum_{i=1}^{N} L(h(x_i), y_i) + \\lambda J(h) $$其中 $J(h)$ 是随着模型复杂度增加而增加的函数. $\\lambda$ 是正则化参数.\n怎么选取合适的 $\\lambda$ ? 一般是先给出若干候选, 在验证集上进行评估, 选取泛化误差最小的.\n数据集划分 一般将数据集划分为 训练集 $T$ 和 测试(验证)集 $T^\\prime$.\n留出法 (hold-out): 分层无放回地随机采样. 也叫简单交叉验证. $k$ 折交叉验证 ($k$-fold cross validation): 将数据集分为 $k$ 个大小相等的子集, 每次取其中一个作为验证集, 其余作为训练集, 最后以这 $k$ 次的平均误差作为泛化误差的估计. 当 $k=|D|$ 时称为留一 (leave-one-out) 验证法. 自助法 (bootstrapping): 从数据集中有放回地采样 $|D|$ 个数据作为训练集, 没抽中的作为验证集. 因而训练集 $T$ 和原始数据集 $D$ 的分布未必一致, 对数据分布敏感的模型不适用. 偏差-方差分解 为什么泛化误差会随着模型复杂度的增加而先减小后增大?\n定义\n偏差 (bias): 模型预测值的期望与真实值之间的差异. 体现了模型的拟合能力.\n$$\\text{Bias}(x) = E_T[h_T(x)-c(x)] = \\bar{h}(x) - c(x)$$方差 (variance): 模型预测值的方差. 体现了模型的对数据扰动的稳定性.\n$$\\text{Var}(x) = E[(h(x) - \\bar{h}(x))^2]$$ 现在对泛化误差进行分解:\n$$ \\begin{aligned} R(h) \u0026= E_T[(h_T(x) - c(x))^2] \\\\ \u0026= E_T[h_T^2(x) - 2h_T(x)c(x) + c^2(x)] \\\\ \u0026= E_T[h_T^2(x)] - 2c(x)E_T[h_T(x)] + c^2(x) \\\\ \u0026= E_T[h_T^2(x)] - \\bar{h}^2(x) + \\bar{h}^2(x) - 2\\bar{h}(x)c(x) + c^2(x) \\\\ \u0026= E_T[(h_T(x) - \\bar{h}(x))^2] + (\\bar{h}(x) - c(x))^2 \\\\ \u0026= \\text{Var}(x) + \\text{Bias}^2(x) \\end{aligned} $$当然, 由于噪声存在, $y$ 未必一定等于 $c(x)$, 不妨设 $y=c(x)+\\varepsilon$, 其中 $\\varepsilon \\sim \\Epsilon$ 期望为 $0$. 可以证明\n定理偏差-方差分解\n$$ E_{T \\sim D^{|T|}, \\varepsilon \\sim \\Epsilon} [(h_T(x)-y)^2] = \\text{Bias}^2(x) + \\text{Var}(x) + E[\\varepsilon^2] $$即泛化误差可以分解为偏差、方差和噪声三部分.\n起初, 模型较为简单, 偏差在泛化误差起主导作用. 随着模型复杂度的增加, 拟合能力增强, 偏差减小, 但带来过拟合风险, 算法对数据扰动敏感, 方差增大. 方差占比逐渐增大, 最终导致泛化误差增大.\n","date":"2025-02-18T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/intro/","title":"机器学习基础(1) —— 概述"},{"content":"凸优化 凸问题的可行集都是凸集.\n定理\n凸优化问题的任意局部极小点都是全局最优点.\n证明\n假设 $x$ 是局部极小, $y$ 全局最优且 $f(y) \u003c f(x)$.\n考虑 $z = \\theta x + (1-\\theta) y$, 则由于 $z$ 是可行点的凸组合, 也是可行点. 由于 $f$ 是凸函数, 有\n$$ f(z) \\leq \\theta f(x) + (1-\\theta) f(y) \u003c f(x) $$取 $\\theta \\to 1$, 则 $f(z) \\to f(x)$, 与局部最小性矛盾.\n线性规划 所谓 线性规划(LP) 问题是指目标函数和约束条件都是线性的优化问题. 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 Gx \\le e \\end{aligned} $$$\\ell_1$ 和 $\\ell_\\infty$ 范数实际上也是线性的.\n示例: 最大球问题 凸多边形\n$$P = \\{ x \\mid a_i^Tx \\le b_i \\}$$的 Chebyshev 中心是最大半径内切球的中心. 代入得\n$$ \\sup \\{a_i^T (x_c + u) \\mid \\Vert u \\Vert_2 \\le r \\} = a_i^Tx_c + r \\Vert a_i \\Vert_2 \\le b_i $$这也变成了一个线性规划问题.\n二次规划 二次规划问题是指目标函数是二次的的优化问题.\n例如, 对于线性约束条件的问题, 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 \\frac{1}{2} x^T P x + q^T x + r \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 Gx \\le e \\end{aligned} $$也有 带二次约束的二次规划 (QCQP).\n我们归结为 二次锥规划 (SOCP):\n$$ \\begin{aligned} \\min \\quad \u0026 f^T x \\\\ \\text{s.t.} \\quad \u0026 \\Vert A_i x + b_i \\Vert_2 \\le c_i^T x + d_i, \\quad i = 1, \\ldots, m \\\\ \u0026 Fx = g \\end{aligned} $$示例: 最小范数问题 令 $\\bar{v}_i = A_ix+b_i \\in \\mathbb{R}^{n_i}$, 则 $\\min_x \\sum_i \\Vert \\bar{v}_i \\Vert_2$ 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_i v_{i0} \\\\ \\text{s.t.} \\quad \u0026\\bar{v}_i = A_i x + b_i \\\\ \u0026(v_{i0}, \\bar{v}_i) \\succeq_\\mathcal{Q} 0 \\end{aligned} $$其中 $\\mathcal{Q}$ 是二次锥.\n示例: 最小化最大函数和问题 设 $\\theta(x)=(\\theta_1(x), \\theta_2(x), \\cdots, \\theta_m(x))^T$. $\\theta_{[i]}$ 是 $\\theta_i$ 的非递增方式的排序. 则 $\\min_{x\\in Q} \\sum_{i=1}^m \\theta_{[i]}(x)$ 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_{i=1}^m u_i + kt \\\\ \\text{s.t.} \\quad \u0026 x \\in Q \\\\ \u0026 \\theta_i(x)\\le u_i + t \\\\ \u0026 u_i \\ge 0 \\end{aligned} $$半定优化 半定优化 (SDP) 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 c^Tx \\\\ \\text{s.t.} \\quad \u0026 x_1A_1 + \\cdots + x_nA_n + B \\succeq 0 \\\\ \u0026 Gx=h \\end{aligned} $$其实是线性规划在矩阵空间的推广. 仍然考虑标准形式:\n$$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c C, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 \\left\u003c A_i, X \\right\u003e = b_i, \\quad i = 1, \\ldots, m \\\\ \u0026 X \\succeq 0 \\end{aligned} $$和对偶形式:\n$$ \\begin{aligned} \\max \\quad \u0026 \\sum_{i=1}^m b_i y_i \\\\ \\text{s.t.} \\quad \u0026 C - \\sum_{i=1}^m y_i A_i \\succeq 0 \\end{aligned} $$示例: 二次约束二次规划问题 $$ \\begin{aligned} \\min \\quad \u0026 x^T A_0 x + 2b_0^T x + c_0 \\\\ \\text{s.t.} \\quad \u0026 x^T A_i x + 2b_i^T x + c_i \\le 0, \\quad i = 1, \\ldots, m \\end{aligned} $$其中 $A_i$ 是 $n \\times n$ 的对称矩阵, 这个问题在 $A_i$ 不定时实际上是 NP-hard 的. 考虑它的半定松弛, 记 $X=x^Tx$ 注意到有\n$$ x^T A_i x + 2b_i^T x + c_i = \\left\u003c A_i, X \\right\u003e + 2\\left\u003c b_i, x \\right\u003e + c_i = \\left\u003c \\begin{bmatrix} A_i \u0026 b_i \\\\ b_i^T \u0026 c_i \\end{bmatrix}, \\begin{bmatrix} X \u0026 x \\\\ x^T \u0026 1 \\end{bmatrix} \\right\u003e $$我们记作 $\\left\u003c \\bar{A}_i, \\bar{X} \\right\u003e$. 注意到, 现在唯一的非线性部分是约束 $X=xx^T$, 我们将其松弛成半正定约束 $X \\succeq xx^T$. 可以证明, $\\bar{X} \\succeq 0$ 等价于 $X \\succeq xx^T$. 这样我们就得到了一个半定优化问题:\n$$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c A_0, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 \\left\u003c \\bar{A}_i, \\bar{X} \\right\u003e \\le 0, \\quad i = 1, \\ldots, m \\\\ \u0026 \\bar{X} \\succeq 0 \\\\ \u0026 \\bar{X}_{n+1,n+1} = 1 \\end{aligned} $$示例: 最大割问题 令 $G$ 为一个无向图, 节点集为 $V = \\{1, 2, \\cdots, n\\}$, 边集为 $E$. 设 $w_{ij} = w_{ji} \\ge 0$ 为边 $(i, j) \\in E$ 上的权重, 要找 $S \\subseteq V$ 使得 $S$ 与 $\\bar{S}$ 之间相连边的权重之和最大化.\n我们定义 $x_j = 1, j \\in S$ 和 $x_j = -1, j \\in \\bar{S}$, 则\n$$ \\begin{aligned} \\max \\quad \u0026 \\sum_{(i, j) \\in E} \\frac{1}{2} (1-x_i x_j) w_{ij} \\\\ \\text{s.t.} \\quad \u0026 x_i = \\pm 1, \\quad i = 1, \\ldots, n \\end{aligned} $$然而这是一个离散优化问题, 考虑对它做松弛. 令 $W=(w_{ij}) \\in \\mathbb{S}^n$ 为权重矩阵, $C=-\\frac{1}{4}(\\text{Diag}(W\\mathbf{1})-W)$ 是 Laplacian 矩阵的 $-1/4$ 倍. 则\n$$ \\begin{aligned} \\min \\quad \u0026 x^T C x \\\\ \\text{s.t.} \\quad \u0026 x_i^2 = 1, \\quad i = 1, \\ldots, n \\end{aligned} $$仍令 $X=x^Tx$, 则容易看出与下问题等价:\n$$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c C, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 X_{ii} = 1, \\quad i = 1, \\ldots, n \\\\ \u0026 X \\succeq 0 \\\\ \u0026 \\text{rank}(X) = 1 \\end{aligned} $$示例: 极小化最大特征值问题 $$ \\min \\quad \\lambda_{\\max}(A_0 + \\sum_{i=1}^m x_i A_i) $$注意到:\n$$\\lambda_{\\max}(A) \\le t \\Leftrightarrow A \\preceq tI$$于是我们有 SDP 形式:\n$$ \\begin{aligned} \\min \\quad \u0026 z \\\\ \\text{s.t.} \\quad \u0026 A_0 + \\sum_{i=1}^m x_i A_i \\preceq zI \\end{aligned} $$示例: 极小化二范数问题 $$ \\min \\quad \\Vert A_0 + \\sum_{i=1}^m x_i A_i \\Vert_2 $$记 $A = A_0 + \\sum_{i=1}^m x_i A_i$. 注意到:\n$$ \\Vert A \\Vert_2 \\le t \\Leftrightarrow A^TA \\preceq t^2I \\Leftrightarrow \\begin{bmatrix} tI \u0026 A \\\\ A^T \u0026 tI \\end{bmatrix} \\succeq 0 $$于是我们有 SDP 形式:\n$$ \\begin{aligned} \\min \\quad \u0026 t \\\\ \\text{s.t.} \\quad \u0026 \\begin{bmatrix} tI \u0026 A \\\\ A \u0026 tI \\end{bmatrix} \\succeq 0 \\end{aligned} $$示例: 特征值优化问题 $$ \\min \\quad \\sum_{i=1}^n \\lambda_{[i]}(A_0 + \\sum_{j=1}^m x_j A_j) $$其中 $\\lambda_{[i]}(A)$ 表示 $A$ 的第 $i$ 大特征值. 前面的极小最大函数和提到它等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_{i=1}^n u_i + kt \\\\ \\text{s.t.} \\quad \u0026 u_i+t \\ge \\lambda_i(A_0 + \\sum_{j=1}^m x_j A_j), \\quad i = 1, \\ldots, n \\\\ \u0026 u_i \\ge 0 \\end{aligned} $$设 $u_i = \\lambda_i(X)$, 则写成 SDP 形式:\n$$ \\begin{aligned} \\min \\quad \u0026 kt + \\text{Tr}(X) \\\\ \\text{s.t.} \\quad \u0026 tI + X \\succeq A_0 + \\sum_{j=1}^m z_j A_j \\\\ \u0026 X \\succeq 0 \\end{aligned} $$","date":"2025-02-15T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/opt-problem/","title":"最优化方法(4) —— 优化问题"},{"content":"基本线性代数知识 定义\n给定函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$, 且 $f$ 在 $x$ 一个邻域内有定义, 若存在 $g \\in \\mathbb{R}^n$, 使得\n$$ \\lim_{p \\to 0} \\frac{f(x+p)-f(x)-g^Tp}{\\Vert p \\Vert} = 0 $$其中 $\\Vert \\cdot \\Vert$ 是向量范数, 则称 $f$ 在 $x$ 处 可微. 此时, $g$ 称为 $f$ 在 $x$ 处的 梯度, 记为 $\\nabla f(x)$.\n显然, 如果梯度存在, 令 $p = \\varepsilon e_i$, 易得\n$$ \\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right) $$ 定义\n如果函数 $f(x): \\mathbb{R}^n \\mapsto \\mathbb{R}$ 在点 $x$ 处的二阶偏导数 $\\dfrac{\\partial^2 f}{\\partial x_i \\partial x_j}$ 存在, 则称 $f$ 在 $x$ 处 二次可微. 此时, $n \\times n$ 矩阵\n$$ \\nabla^2 f(x) = \\begin{pmatrix} \\dfrac{\\partial^2 f}{\\partial x_1^2} \u0026 \\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\dfrac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026 \\dfrac{\\partial^2 f}{\\partial x_2^2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\dfrac{\\partial^2 f}{\\partial x_n \\partial x_1} \u0026 \\dfrac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_n^2} \\end{pmatrix} $$称为 $f$ 在 $x$ 处的 Hessian 矩阵. 若 $\\nabla^2 f(x)$ 在 $D$ 上连续, 则称 $f$ 在 $D$ 上 二次连续可微.\n可以证明, 若 $f$ 在 $D$ 上二次连续可微, 则 $\\nabla^2 f(x)$ 为对称矩阵.\n多元函数的梯度可以推广到变量是矩阵的情形.\n定义\n给定函数 $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$, 且 $f$ 在 $X$ 一个邻域内有定义, 若存在 $G \\in \\mathbb{R}^{m \\times n}$, 使得\n$$ \\lim_{V \\to 0} \\frac{f(X+V)-f(X)-\\left\u003c G, V \\right\u003e}{\\Vert V \\Vert} = 0 $$其中 $\\Vert \\cdot \\Vert$ 是矩阵范数, 则称 $f$ 在 $X$ 处 (Fréchet)可微. 此时, $G$ 称为 $f$ 在 $X$ 处的 梯度, 记为 $\\nabla f(X)$.\n矩阵的可微有另一种较为简单常用的定义.\n定义\n给定函数 $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$, 若存在矩阵 $G \\in \\mathbb{R}^{m \\times n}$, 使得\n$$ \\lim_{t \\to 0} \\frac{f(X+tV)-f(X)}{t} = \\left\u003c G, V \\right\u003e $$则称 $f$ 在 $X$ 处 (Gâteaux)可微.\n例如:\n$f(X) = \\text{tr}(AX^TB)$, 此时 $\\nabla f(X) = BA$.\n$f(X, Y)=\\frac{1}{2} \\Vert XY-A \\Vert_F^2$. 此时\n$$ \\begin{aligned} \u0026f(X,Y+tV)-f(X,Y) \\\\ \u0026= \\frac{1}{2} \\Vert X(Y+tV)-A \\Vert_F^2 - \\frac{1}{2} \\Vert XY-A \\Vert_F^2 \\\\ \u0026= \\frac{1}{2} \\Vert XY - A + tVX \\Vert_F^2 - \\frac{1}{2} \\Vert XY - A \\Vert_F^2 \\\\ \u0026= \\frac{1}{2} \\Vert tVX \\Vert_F^2 + \\left\u003c XY-A, tVX \\right\u003e \\\\ \u0026= t \\left\u003c X^T(XY-A), V \\right\u003e + o(t) \\end{aligned} $$所以 $\\frac{\\partial f}{\\partial Y} = X^T(XY-A)$, 类似地, $\\frac{\\partial f}{\\partial X} = (XY-A)Y^T$.\n$f(X)=\\ln\\text{det}(X)$, $X$ 为正定矩阵. 此时\n$$ \\begin{aligned} \u0026f(X+tV)-f(X) \\\\ \u0026= \\ln\\text{det}(X+tV) - \\ln\\text{det}(X) \\\\ \u0026= \\ln\\text{det}(I+tX^{-1/2}VX^{-1/2}) \\end{aligned} $$考虑 $X^{-1/2}VX^{-1/2}$ 的特征值 $\\lambda_i$, 则由特征值之和为迹, 有\n$$ \\begin{aligned} \u0026= \\ln\\text{det}\\prod_{i=1}^n (1+t\\lambda_i) \\\\ \u0026= \\sum_{i=1}^n \\ln(1+t\\lambda_i) \\\\ \u0026= \\sum_{i=1}^n t\\lambda_i + o(t) \\\\ \u0026= t\\text{tr}(X^{-1/2}VX^{-1/2}) + o(t) \\\\ \u0026= t\\text{tr}(X^{-1}V) + o(t) \\\\ \u0026= t\\left\u003c X^{-T}, V \\right\u003e + o(t) \\end{aligned} $$所以 $\\nabla f(X) = X^{-T}$.\n定义\n广义实数 是一种扩充实数域的数, 记为 $\\bar{\\mathbb{R}} = \\mathbb{R} \\cup \\{ \\pm \\infty \\}$. 映射 $f: \\mathbb{R}^n \\mapsto \\bar{\\mathbb{R}}$ 称为 广义实值函数.\n定义\n给定广义实值函数 $f$ 和非空集合 $X$. 如果存在 $x \\in X$ 使得 $f(x) \u003c +\\infty$, 并且对任意的 $x \\in X$, 都有 $f(x) \u003e -\\infty$, 那么称函数 $f$ 关于集合 $X$ 是 适当的．\n定义\n对于广义实值函数 $f: \\mathbb{R}^n \\mapsto \\bar{\\mathbb{R}}$,\n$C_\\alpha = \\{x \\mid f(x) \\le \\alpha \\}$ 称为 $f$ 的 $\\alpha$-下水平集. $\\text{epi} f = \\{ (x, t) \\mid f(x) \\le t \\}$ 称为 $f$ 的 上方图. 若 $\\text{epi} f$ 为闭集, 则称 $f$ 为闭函数. 若对任意的 $x \\in \\mathbb{R}^n$, 有 $\\liminf_{y \\to x} f(y) \\ge f(x)$, 则称 $f$ 为 下半连续函数. 定理\n对于广义实值函数 $f$, 以下命题等价:\n$f(x)$ 的任意 $\\alpha$-下水平集都是闭集; $f(x)$ 是下半连续的; $f(x)$ 是闭函数. 证明\n(1) $\\Rightarrow$ (2): 反证, 假设 $x_k \\to \\bar{x}$ 但 $\\liminf_{k \\to \\infty} f(x_k) \u003c f(\\bar{x})$. 取 $t$ 介于二者之间.\n考虑到 $\\liminf_{k \\to \\infty} f(x_k) \u003c t$, 则有无穷多 $x_k$ 使得 $f(x_k) \\le t$, 即这些 $x_k$ 在 $C_t$ 中. 由于 $C_t$ 是闭集, 则 $\\bar{x} \\in C_t$, 即 $f(\\bar{x}) \\le t$, 矛盾.\n(2) $\\Rightarrow$ (3): 考虑 $(x_k,y_k) \\in \\text{epi} f \\to (\\bar{x},\\bar{y})$, 由于 $f$ 下半连续, 则\n$$ f(\\bar{x}) \\le \\liminf_{k \\to \\infty} f(x_k) = \\liminf_{k \\to \\infty} y_k = \\bar{y} $$即 $(\\bar{x}, \\bar{y}) \\in \\text{epi} f$.\n(3) $\\Rightarrow$ (1): 考虑 $x_k \\in C_\\alpha \\to \\bar{x}$, 则 $(x_k, \\alpha) \\in \\text{epi} f \\to (\\bar{x}, \\alpha)$, 所以 $(\\bar{x}, \\alpha) \\in \\text{epi} f$, 即 $f(\\bar{x}) \\le \\alpha$, 所以 $\\bar{x} \\in C_\\alpha$.\n适当闭函数的和, 复合, 逐点上确界仍然是闭函数.\n凸函数 定义\n适当函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 凸函数, 如果 $\\text{dom} f$ 是凸集, 且对任意的 $x, y \\in \\text{dom} f$ 和 $\\theta \\in [0,1]$, 有\n$$ f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y) $$ 易知仿射函数既是凸函数又是凹函数. 所有的范数都是凸函数.\n定义\n若存在常数 $m \u003e 0$, 使得 $g(x) = f(x) - \\frac{m}{2} \\Vert x \\Vert^2$ 是凸函数, 则称 $f$ 是 强凸函数, $m$ 称为 强凸参数.\n定理凸函数判定定理\n适当函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 是凸函数的充要条件是, 对任意的 $x \\in \\text{dom} f$, 函数 $g: \\mathbb{R} \\mapsto \\mathbb{R}$ 是凸函数, 其中\n$$g(t) = f(x+tv), \\quad \\text{dom}g = \\{ t \\mid x + tv \\in \\text{dom} f \\}$$ 定理一阶条件\n对于定义在凸集上的可微函数 $f$, $f$ 是凸函数当且仅当\n$$ f(y) \\ge f(x) + \\nabla f(x)^T(y-x), \\quad \\forall x, y \\in \\text{dom} f $$证明\n必要性: 设 $f$ 凸, 则 $\\forall x, y \\in \\text{dom} f, t \\in [0,1]$, 有\n$$tf(y)+(1-t)f(x) \\ge f(x+t(y-x))$$令 $t \\to 0$, 即\n$$f(y)-f(x) \\ge \\frac{f(x+t(y-x))-f(x)}{t} \\to \\nabla f(x)^T(y-x)$$充分性: $\\forall x, y \\in \\text{dom}f, t\\in (0,1)$, 取 $z = tx+(1-t)y$, 则\n$$ \\begin{aligned} f(x) \u0026\\ge f(z) + \\nabla f(z)^T(x-z) \\\\ f(y) \u0026\\ge f(z) + \\nabla f(z)^T(y-z) \\end{aligned} $$一式乘以 $t$, 二式乘以 $1-t$, 相加即得.\n定理梯度单调性\n设 $f$ 为可微函数, 则 $f$ 为凸函数当且仅当 $\\text{dom} f$ 为凸集且 $\\nabla f$ 为单调映射.\n$$(\\nabla f(x) - \\nabla f(y))^T(x-y) \\ge 0$$证明\n必要性: 根据一阶条件, 有\n$$ \\begin{aligned} f(y) \u0026\\ge f(x) + \\nabla f(x)^T(y-x) \\\\ f(x) \u0026\\ge f(y) + \\nabla f(y)^T(x-y) \\end{aligned} $$相加即可.\n充分性: 考虑 $g(t)=f(x+t(y-x))$, 则 $g^\\prime(t)=\\nabla f(x+t(y-x))^T (y-x)$, 从而 $g^\\prime (t) \\ge g^\\prime (0)$.\n$$ \\begin{aligned} f(y) \u0026= g(1) = g(0) + \\int_{0}^1 g^\\prime(t) dt \\\\ \u0026\\ge g(0) + \\int_{0}^1 g^\\prime(0) dt = g(0) + g^\\prime(0) \\\\ \u0026= f(x) + \\nabla f(x)^T(y-x) \\end{aligned} $$ 定理\n函数 $f(x)$ 是凸函数当且仅当 $\\text{epi}f$ 是凸集.\n定理二阶条件\n设 $f$ 为定义在凸集上的二阶连续可微函数, $f$ 是凸函数当且仅当 $\\nabla^2 f(x) \\succeq 0, \\forall x \\in \\text{dom} f$. 若不取等, 则为严格凸函数.\n证明\n必要性: 反设 $f(x)$ 在 $x$ 处 $\\nabla^2 f(x) \\prec 0$, 则存在 $v \\in \\mathbb{R}^n$, 使得 $v^T \\nabla^2 f(x) v \u003c 0$, 考虑 Peano 余项\n$$ f(x+tv)=f(x)+t\\nabla f(x)^Tv+\\frac{t^2}{2}v^T\\nabla^2 f(x+tv)v + o(t^2) $$取 $t$ 充分小,\n$$ \\frac{f(x+tv)-f(x)-t\\nabla f(x)^T v}{t^2}=\\frac{1}{2}v^T\\nabla^2 f(x+tv)v + o(1) \u003c 0 $$这和一阶条件矛盾.\n充分性: 对于任意的 $x, y \\in \\text{dom} f$, 有\n$$ \\begin{aligned} f(y) \u0026= f(x)+\\nabla f(x)^T(y-x)+\\frac{1}{2}(y-x)^T\\nabla^2 f(z)(y-x) \\\\ \u0026\\ge f(x)+\\nabla f(x)^T(y-x) \\end{aligned} $$由一阶条件, $f$ 为凸函数.\n保凸运算 下面举一些重要的例子.\n逐点取上界: 若对每个 $y \\in A$, $f(x,y)$ 都是关于 $x$ 的凸函数, 则\n$$g(x)=\\sup_{y \\in A} f(x,y)$$也是凸函数.\n$C$ 的支撑函数 $f(x)=\\sup_{y \\in C} y^Tx$ 是凸函数. $C$ 到 $x$ 的最远距离 $f(x)=\\sup_{y \\in C} \\Vert x-y \\Vert$ 是凸函数. 对称阵 $X \\in \\mathbb{S}^n$ 的最大特征值 $\\lambda_{\\max}(X)=\\sup_{\\Vert x \\Vert=1} x^TXx$ 是凸函数. 标量函数的复合: 若 $g: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 是凸函数, $h: \\mathbb{R} \\mapsto \\mathbb{R}$ 是单调不减的凸函数, 则\n$$f(x) = h(g(x))$$也是凸函数. 凹同理.\n如果 $g$ 凸, 则 $f(x) = \\exp(g(x))$ 也是凸函数. 如果 $g$ 凹, 则 $f(x) = 1/g(x)$ 也是凸函数. 取下确界: 若 $f(x, y)$ 关于 $(x, y)$ 整体是凸函数, $C$ 是凸集, 则\n$$g(x) = \\inf_{y \\in C} f(x, y)$$也是凸函数.\n凸集 $C$ 到 $x$ 的距离 $f(x)=\\inf_{y \\in C} \\Vert x-y \\Vert$ 是凸函数. 透视函数: 若 $f: \\mathbb{R}^{n} \\mapsto \\mathbb{R}$ 是凸函数, 则\n$$g(x, t) = tf(x/t), \\quad \\text{dom} g = \\{ (x, t) \\mid x / t \\in \\text{dom} f, t \u003e 0 \\}$$也是凸函数.\n相对熵函数 $g(x,t)=t\\log t-t\\log x$ 是凸函数. 若 $f$ 凸, 则 $g(x)=(c^T+d)f((Ax+b)/(c^T+d))$ 也是凸函数. 共轭函数: 任意适当函数 $f$ 的共轭函数\n$$f^\\ast(y)=\\sup_{x \\in \\text{dom} f} (\\left\u003c x, y \\right\u003e - f(x))$$是凸函数.\n凸函数的推广 定义\n$f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 拟凸的, 如果 $\\text{dom} f$ 是凸集, 且对任意 $\\alpha$, 下水平集 $C_\\alpha$ 是凸集.\n若 $f$ 是拟凸的, 则称 $-f$ 是 拟凹的. 若 $f$ 既是拟凸又是拟凹的, 则称 $f$ 是 拟线性的.\n注意: 拟凸函数的和不一定是拟凸函数.\n定理\n拟凸函数满足类 Jenson 不等式: 对拟凸函数 $f$ 和 $\\forall x, y \\in \\text{dom} f, \\theta \\in [0,1]$, 有 $$f(\\theta x + (1-\\theta)y) \\le \\max\\left\\{f(x),f(y)\\right\\}$$ 拟凸函数满足一阶条件: 定义在凸集上的可微函数 $f$ 拟凸当且仅当 $$f(y) \\le f(x) \\Rightarrow \\nabla f(x)^T(y-x) \\le 0$$ 定义\n如果正值函数 $f$ 满足 $\\log f$ 是凸函数, 则 $f$ 称为 对数凸函数; 若为凹函数, 则 $f$ 称为 对数凹函数.\n例如, 正态分布\n$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\text{det} \\Sigma}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$是对数凹函数.\n对数凹函数的乘积, 积分都是对数凹的, 但加和不一定是对数凹的.\n在广义不等式下, 也可以定义凸凹性.\n定义\n$f: \\mathbb{R}^n \\mapsto \\mathbb{R}^m$ 称为 $K$-凸函数, 如果 $\\text{dom} f$ 是凸集, 且 $$f(\\theta x+(1-\\theta)y \\preceq_K \\theta f(x)+(1-\\theta)f(y))$$ 对任意 $x,y \\in \\text{dom} f, 0 \\le \\theta \\le 1$ 成立.\n例如, $f: \\mathbb{S}^m \\mapsto \\mathbb{S}^m$, $f(X)=X^2$ 是 $\\mathbb{S}^m_+$-凸函数. 这点利用 $z^TX^2z=\\Vert Xz \\Vert^2$ 是关于 $X$ 的凸函数即可得知.\n","date":"2025-01-25T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-function/","title":"最优化方法(3) —— 凸函数"},{"content":"范数 定义\n记号 $\\Vert \\cdot \\Vert: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 向量范数, 若满足:\n正定性: $\\Vert x \\Vert \\geq 0$, 且 $\\Vert x \\Vert = 0 \\Leftrightarrow x = 0$; 齐次性: $\\Vert \\alpha x \\Vert = \\vert \\alpha \\vert \\Vert x \\Vert$; 三角不等式: $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$. $\\ell_p$ 范数是最常见的向量范数\n$$ \\Vert x \\Vert_p = \\left( \\sum_{i=1}^n \\vert x_i \\vert^p \\right) ^{\\frac{1}{p}} $$特别地, 当 $p = \\infty$ 时, $\\Vert x \\Vert_\\infty = \\max_i \\vert x_i \\vert$.\n向量范数可以自然地推广到矩阵范数. 常见的矩阵范数有:\n和范数: $\\Vert A \\Vert_1 = \\sum_{i,j} \\vert A_{ij} \\vert$; Frobenius 范数: $\\Vert A \\Vert_F = \\sqrt{\\sum_{i,j} A_{ij} ^2} = \\sqrt{\\text{tr}(A^T A)}$; 算子范数: $\\Vert A \\Vert_{(m,n)}=\\max_{\\Vert x \\Vert_n = 1} \\Vert Ax \\Vert_m$. 特别地, 当 $m = n = p$ 时: $p=1$ 时, $\\Vert A \\Vert_{p=1} = \\max_j \\sum_i \\vert A_{ij} \\vert$; $p=2$ 时, $\\Vert A \\Vert_{p=2} = \\sqrt{\\lambda_{\\max}(A^T A)}$, 亦称为 谱范数. $p=\\infty$ 时, $\\Vert A \\Vert_{p=\\infty} = \\max_i \\sum_j \\vert A_{ij} \\vert$. 核范数: $\\Vert A \\Vert_\\ast = \\sum_i \\sigma_i$, 其中 $\\sigma_i$ 为 $A$ 的奇异值. 定理Cauchy 不等式\n$$\\vert \\left\u003c X, Y \\right\u003e \\vert \\leq \\Vert X \\Vert \\Vert Y \\Vert$$等号成立当且仅当 $X$ 与 $Y$ 线性相关.\n凸集 定义\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in \\mathbb{R}$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 仿射集.\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in [0, 1]$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 凸集.\n换言之, 仿射集要求过任意两点的直线都在集合内, 而凸集要求过任意两点的线段都在集合内. 显然, 仿射集都是凸集. 线性方程组的解集是一个仿射集, 而线性规划问题的可行域是一个凸集. 可以证明, 仿射集均可表示为某个线性方程组的解集.\n定理\n若 $S$ 是凸集, 则 $kS = \\left\\{ ks \\mid k \\in \\mathbb{R}, s \\in S \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S + T = \\left\\{ s + t \\mid s \\in S, t \\in T \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S \\cap T$ 也是凸集. 凸集的内部和闭包均为凸集. 可以证明, 任意多个凸集的交集仍为凸集.\n定义\n形如 $x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_kx_k$, 其中 $\\theta_i \\geq 0$ 且 $\\sum_i \\theta_i = 1$, 的表达式称为 $x$ 的 凸组合. 集合 $S$ 的所有点的凸组合构成的集合称为 $S$ 的 凸包, 记为 $\\text{conv}(S)$.\n定理\n若 $\\text{conv} S \\subseteq S$, 则 $S$ 为凸集, 反之亦然.\n证明\n先证明正方向. 对任意 $x,y \\in S, \\theta \\in [0,1]$, 有 $\\theta x + (1-\\theta) y \\in \\text{conv} S \\subseteq S$, 故 $S$ 为凸集.\n再证明反方向, 对凸组合的维数 $k$ 采用数学归纳法证明之.\n若 $k=1$, 显然成立. 假设对于 $k-1$ 成立, 则对于 $k$, 考虑\n$$ \\begin{aligned} x \u0026= \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_k x_k \\\\ \u0026= (1-\\theta_k)\\left(\\frac{\\theta_1}{1-\\theta_k} x_1 + \\frac{\\theta_2}{1-\\theta_k} x_2 + \\cdots + \\frac{\\theta_{k-1}}{1-\\theta_k} x_{k-1}\\right) + \\theta_k x_k \\end{aligned} $$前面大括号内的表达式为 $k-1$ 个凸组合, 故在 $S$ 中. 于是 $x$ 又成为两个点的凸组合, 由于 $S$ 为凸集, 故 $x \\in S$. 则 $\\text{conv} S \\subseteq S$.\n定理\n$\\text{conv}S$ 是包含 $S$ 的最小凸集; $\\text{conv}S$ 是所有包含 $S$ 的凸集的交集. 证明\n显然第一个是第二个的推论, 只证明第二个.\n已知凸集的交是凸集, 从而所有包含 $S$ 的凸集的交集 $X$ 是凸集. 且 $\\text{conv} S$ 是包含 $S$ 的凸集, 则 $X \\subseteq \\text{conv} S$.\n另一方面, $S \\subseteq X$, 则 $\\text{conv} S \\subseteq \\text{conv}X$, 而 $X$ 是凸集, 则 $\\text{conv}X = X$, 即 $\\text{conv}S \\subseteq X$. 综上, $\\text{conv}S = X$.\n仿照凸组合和凸包, 也可以定义仿射组合和仿射包 $\\text{affine} S$, 不再赘述.\n定义\n形如 $x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_kx_k$, 其中 $\\theta_i \\geq 0$ 的表达式称为 $x$ 的 锥组合. 若集合 $S$ 中任意点的锥组合都在 $S$ 中, 则称 $S$ 为凸锥.\n常见凸集 定义\n任取非零向量 $a\\in \\mathbb{R}^n$, 形如\n$$ \\left\\{ x \\mid a^Tx =b \\right\\} $$的集合称为 超平面, 形如\n$$ \\left\\{ x \\mid a^Tx \\le b \\right\\} $$的集合称为 半空间.\n定义\n满足线性等式和不等式组的点的集合称为 多面体, 即\n$$ \\left\\{x \\mid Ax \\le b, Cx = d\\right\\} $$其中 $A \\in \\mathbb{R}^{m \\times n}, C \\in \\mathbb{R}^{p \\times n}$.\n定义\n对中心 $x_c$ 和半径 $r$, 形如\n$$ B(x_c, r) = \\left\\{ x \\mid \\Vert x - x_c \\Vert \\le r \\right\\} = \\left\\{ x_c + ru \\mid \\Vert u \\Vert \\le 1 \\right\\} $$的集合称为 球.\n对中心 $x_c$ 和对称正定矩阵 $P$, 非奇异矩阵 $A$, 形如\n$$ \\left\\{ x \\mid (x-x_c)^TP(x-x_c) \\le 1 \\right\\} = \\left\\{ x_c + Au \\mid \\Vert u \\Vert \\le 1 \\right\\} $$的集合称为 椭球.\n定义\n形如\n$$ \\left\\{(x,t) \\mid \\Vert x \\Vert \\le t \\right\\} $$的集合称为 (范数)锥.\n保凸运算 定理\n仿射运算保凸, 即对 $f(x)=Ax+b$, 则凸集在 $f$ 下的像是凸集, 凸集在 $f$ 下的原像是凸集.\n考虑双曲锥\n$$ \\left\\{ x \\mid x^TPx \\le \\left( c^Tx \\right)^2, c^Tx \\ge 0, P \\in \\mathbb{S}_+^n \\right\\} $$$\\mathbb{S}_+^n$ 表示半正定矩阵. 双曲锥可以表示为二阶锥\n$$ \\left\\{ x \\mid \\Vert Ax \\Vert_2 \\le c^Tx, c^Tx \\ge 0, A^TA = P \\right\\} $$这个可以由二次范数锥得到.\n透视变换 $P: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n$:\n$$ P(x,t) = \\frac{x}{t}, \\quad \\text{dom} P = \\left\\{ (x,t) \\mid t \u003e 0 \\right\\} $$保凸.\n分式线性变换 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}^m$:\n$$ f(x) = \\frac{Ax+b}{c^Tx+d}, \\quad \\text{dom} f = \\left\\{ x \\mid c^Tx+d \u003e 0 \\right\\} $$保凸.\n广义不等式和对偶锥 定义\n我们称一个凸锥 $K \\subseteq \\mathbb{R}^n$ 为 适当锥, 当其还满足\n$K$ 是闭集; $K$ 是实心的, 即 $\\text{int} K \\neq \\emptyset$; $K$ 是尖的, 即内部不包含直线: 若 $x \\in \\text{int} K, -x \\in \\text{int} K$. 则一定有 $x = 0$. 例如\n非负卦限 $K=\\mathbb{R}_+^n=\\left\\{ x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\right\\}$ 是适当锥. 半正定锥 $K=\\mathbb{S}_+^n$ 是适当锥. $[0,1]$ 上的有限非负多项式 $K=\\left\\{ x \\in \\mathbb{R}^n \\mid x_1 + x_2t + \\cdots + x_nt^{n-1} \\ge 0, t \\in [0,1] \\right\\}$ 是适当锥. 可以在 适当锥 上定义广义不等式.\n定义\n对于适当锥 $K$ , 定义偏序 广义不等式 为\n$$x \\preceq_K y \\Leftrightarrow y - x \\in K$$严格版本:\n$$x \\prec_K y \\Leftrightarrow y - x \\in \\text{int} K$$ 广义不等式是一个偏序关系, 具有自反性, 反对称性, 传递性, 可加性, 非负缩放性, 不再赘述.\n定义\n令锥 $K$ 为全空间 $\\Omega$ 的子集, 则 $K$ 的 对偶锥 为\n$$ K^\\ast = \\left\\{ y \\mid \\left\u003c x, y \\right\u003e \\ge 0, \\forall x \\in K \\right\\} $$ 例如\n非负卦限是自对偶锥. 半正定锥是自对偶锥. 定理\n设 $K$ 是一锥, $K^\\ast$ 是其对偶锥, 则满足:\n$K^\\ast$ 是锥 (即使 $K$ 不是锥); $K^\\ast$ 是凸且闭的; 若 $\\text{int} \\neq \\emptyset$, 则 $K^\\ast$ 是尖的. 若 $K$ 是尖的, 则 $\\text{int} K^\\ast \\neq \\emptyset$. 若 $K$ 是适当锥, 则 $K^\\ast$ 是适当锥. $K^{\\ast\\ast}$ 是 $K$ 的凸包. 特别地, 若 $K$ 是凸且闭的, 则 $K^\\ast=K$. 适当锥的对偶锥仍是适当锥, 则适当锥 $K$ 的对偶锥 $K^\\ast$ 也可以诱导广义不等式.\n定义\n对于适当锥 $K$, 定义其对偶锥 $K^\\ast$ 上的 对偶广义不等式 为:\n$$x \\preceq_{K^\\ast} y \\Leftrightarrow y - x \\in K^\\ast$$其满足\n$x \\preceq_{K} y \\Leftrightarrow \\lambda^Tx \\le \\lambda^Ty, \\forall \\lambda \\succeq_{K^\\ast} K^\\ast$. $y \\succeq_{K^\\ast} 0 \\Leftrightarrow y^Tx \\ge 0, \\forall x \\succeq_K 0$. 分离超平面定理 定理分离超平面定理\n如果 $C$ 和 $D$ 是不相交的凸集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 分开, 即存在 $a \\neq 0, b$ 使得\n$$ \\begin{aligned} a^Tx \u0026\\le b, \\quad \\forall x \\in C \\\\ a^Tx \u0026\\ge b, \\quad \\forall x \\in D \\end{aligned} $$ 简要想法是找距离最近的一对点, 以这两点的中点为中心, 以两点的连线为法向量构造超平面.\n定理严格分离定理\n如果 $C$ 和 $D$ 是不相交的凸集, 且 $C$ 是闭集, $D$ 是紧集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 严格分开, 即存在 $a \\neq 0, b$ 使得\n$$ \\begin{aligned} a^Tx \u0026\\lt b, \\quad \\forall x \\in C \\\\ a^Tx \u0026\\gt b, \\quad \\forall x \\in D \\end{aligned} $$ 定义\n给定集合 $C$ 和边界点 $x_0$, 如果 $a\\ne 0$ 满足 $a^Tx \\le a^T x_0, \\forall x \\in C$, 则称\n$$ \\left\\{ x \\mid a^Tx = a^T x_0 \\right\\} $$为 $C$ 的 支撑超平面.\n由分离超平面的特殊情况 ($D$ 为单点集) 可以得到支撑超平面的存在性.\n定理支撑超平面定理\n若 $C$ 是凸集, 则 $C$ 的任意边界点处存在支撑超平面.\n","date":"2025-01-16T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-set/","title":"最优化方法(2) —— 凸集"},{"content":"概要 最优化问题的一般形式:\n$$ \\begin{aligned} \\min_{x} \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 g_i(x) \\leq 0, \\quad i = 1, 2, \\ldots, m \\\\ \u0026 h_j(x) = 0, \\quad j = 1, 2, \\ldots, p \\end{aligned} $$稀疏优化 考虑线性方程组 $Ax = b$, 优化函数 $\\min_{x \\in R^n} {\\Vert x \\Vert}_0, {\\Vert x \\Vert}_1, {\\Vert x \\Vert}_2$, 分别指代 $x$ 的非零元个数, $l_1, l_2$ 范数. LASSO(least absolute shrinkage and selection operator) 问题:\n$$ \\min_{x \\in \\mathbb{R}^n} \\mu {\\Vert x \\Vert}_1 + \\frac{1}{2} {\\Vert Ax - b \\Vert}_2^2 $$低秩矩阵优化 考虑矩阵 $M$, 希望 $X$ 在描述 $M$ 有效特征元素的同时, 尽可能保证 $X$ 的低秩性质. 低秩矩阵问题:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\text{rank}(X) \\quad \\text{s.t.} \\quad X_{ij} = M_{ij}, \\quad (i, j) \\in \\Omega $$核范数 ${\\Vert X \\Vert}_*$ 为所有奇异值的和. 也有二次罚函数的形式:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\mu {\\Vert X \\Vert}_* + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} (X_{ij} - M_{ij})^2 $$对于低秩情形, $X=LR^T$, 其中 $L \\in \\mathbb{R}^{m \\times r}, R \\in \\mathbb{R}^{n \\times r}$, $r \\ll m,n$ 为秩. 优化问题可写为:\n$$ \\min_{L,R} \\alpha {\\Vert L \\Vert}^2_F + \\beta {\\Vert R \\Vert}^2_F + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} ([LR^T]_{ij} - M_{ij})^2 $$引入正则化系数 $\\alpha, \\beta$ 来消除 $L,R$ 在常数缩放下的不确定性.\n深度学习 机器学习的问题通常形如\n$$ \\min_{x \\in W} \\frac{1}{N} \\sum_{i=1}^N \\ell(f(a_i, x), b_i) + \\lambda R(x) $$ 基本概念 定义\n设 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$, $x \\in \\mathbb{R}^n$ 的可行区域为 $S$. 若存在一个邻域 $N(x)$, 使得 $\\forall x \\in N(x) \\cap S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的局部极小点. 若 $\\forall x \\in S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的全局极小点.\n大多数的问题是不能显式求解的, 通常要使用迭代算法.\n定义\n称算法是 Q-线性收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} \\le a, \\quad a \\in (0, 1) $$称算法是 Q-超线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 0 $$称算法是 Q-次线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 1 $$称算法是 Q-二次收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert^2}} \\le a, \\quad a \u003e 0 $$ 定义\n设 $x_k$ 是迭代算法产生的序列且收敛到 $x^\\ast$, 如果存在 Q-线性收敛于 $0$ 的非负序列 $t_k$, 且\n$$ \\Vert x_k - x^\\ast \\Vert \\le t_k $$则称 $x_k$ 是 R-线性收敛 的.\n一般来说, 收敛准则可以是\n$$ \\frac{f(x_k) - f^\\ast}{\\max\\left\\{\\left|f^\\ast \\right|, 1\\right\\}} \\le \\varepsilon $$也可以是\n$$ \\nabla f(x_k) \\le \\varepsilon $$如果有约束要求, 还要同时考虑到约束违反度. 对于实际的计算机算法, 会设计适当的停机准则, 例如\n$$ \\frac{{\\Vert x_{k+1} - x_k \\Vert}}{\\max\\left\\{\\Vert x_k \\Vert, 1\\right\\}} \\le \\varepsilon $$","date":"2025-01-12T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/intro/","title":"最优化方法(1) —— 简介"}]