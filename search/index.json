[{"content":"介绍 在论文 [1] Unknown-material 中, 作者首次介绍了 Adam 优化器. 此算法一经出现立刻爆火, 现在在深度学习当中已经成为一种最常用的优化算法.\n算法Adam\nAdam 的更新公式如下:\n$$ \\begin{aligned} m_t \u0026 = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \u0026 = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\ \\hat{m}_t \u0026 = \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v}_t \u0026 = \\frac{v_t}{1 - \\beta_2^t} \\\\ \\theta_t \u0026 = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t \\end{aligned} $$其中 $\\odot$ 表示逐元素相乘. 超参数通常取 $\\beta_1=0.9, \\beta_2=0.999, \\epsilon=10^{-8}$.\n可以认为 Adam 本身直接由 SGD 而来. 在此基础上 Adam 引入了几个重要技术:\n移动平均 (Moving Average): $m_t$ 不是通过对梯度直接求和, 而是按照 $\\beta_1$ 和 $\\beta_2$ 的比例进行移动平均, 保证了梯度的稳定性. 自适应学习率 (Adaptive Learning Rate): $v_t$ 通过对梯度的二阶矩进行估计, 使得学习率可以自适应地调整. 偏差修正 (Bias Correction): 由于在训练开始移动平均几乎为 0, 对其引入偏差修正可以加快初始化时刻的收敛速度. 当然还有加上衰减的 AdamW, 以及其他的变种, 以适应 Transformer 等模型的训练.\n收敛 \u0026hellip; 吗? 论文 [1] 中提到我们可以引入误差量来衡量收敛性:\n定义\n称 累积误差 为\n$$R(T) = \\sum_{t=1}^T (f_t(\\theta_t) - f_t(\\theta^*))$$其中 $\\theta^*$ 是最优解, 即 $\\theta^* = \\arg\\min_{\\theta} \\sum_{t=1}^T f_t(\\theta)$.\n可以认为, 当 $R(T)/T \\to 0$ 时算法收敛. 作者在文献中对 Adam 的收敛性给了自己证明, 里面的细节太多, 这里只给粗略过程.\n鉴于偏差修正只在初期有较大影响, 之后对于收敛性的讨论, 以下证明对其不予考虑 (原论文有), 此外忽略微小项 $\\epsilon$.\n在原始的 Adam 中 $\\beta_1, \\alpha$ 都是常数, 实际上此时难以证明. 因此原文中, 对参数做了随时间动态调整:\n$$\\alpha_t = \\frac{\\alpha}{\\sqrt{t}}, \\beta_{1,t}=\\beta_1 \\lambda^{t-1}, \\lambda \\in (0,1)$$注意: 以下定理的证明有争议!\n定理\n假设 $f_t$ 梯度有界, $\\theta_t$ 之间的距离有界, 即 $\\| g_t \\|_{\\infty} \\le G, \\|\\theta_i-\\theta_j\\|_{\\infty} \\le D$, 且 $\\beta_1^4 \u003c \\beta_2$, Adam 中超参数 $\\alpha, \\beta_1$ 遵从如上动态调整, 则 $R(T) \\le \\mathcal{O}(\\sqrt{T})$, 因而 Adam 收敛.\n证明\n首先, 可以证明:\n$$ f_t(\\theta_t) - f_t(\\theta^*) \\le g_t^T(\\theta_t - \\theta^*) = \\sum_{i=1}^d g_{t,i}(\\theta_{t,i} - \\theta^*_i) $$$d$ 个分量求和并不会影响量级, 从而我们只需要关心第 $i$ 个分量, 因而下面我们不妨设 $\\theta_t, g_t, m_t, v_t$ 等都是一维的. (或者可以用 $\\theta_t = \\theta_{t,i}$ 来表示), 那么我们只要证明:\n$$ \\sum_{t=1}^{T} g_t(\\theta_t - \\theta^*) \\le \\mathcal{O}(\\sqrt{T}) $$既然要估计 $\\theta_t - \\theta^*$, 由学习率公式, 我们可以得到:\n$$(\\theta_{t+1} - \\theta^*) = (\\theta_t - \\theta^*) - \\alpha_t \\frac{m_t}{\\sqrt{v_t}}$$取平方, 有:\n$$(\\theta_{t+1} - \\theta^*)^2 = (\\theta_t - \\theta^*)^2 - 2\\alpha_t \\frac{m_t}{\\sqrt{v_t}}(\\theta_t - \\theta^*) + \\alpha_t^2 \\frac{m_t^2}{v_t}$$要把 $m_t$ 换成 $g_t$, 由 $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ 代入得到:\n$$(\\theta_{t+1} - \\theta^*)^2 = (\\theta_t - \\theta^*)^2 - 2\\alpha_t \\frac{\\beta_{1,t} m_{t-1} + (1 - \\beta_{1,t}) g_t}{\\sqrt{v_t}}(\\theta_t - \\theta^*) + \\alpha_t^2 \\frac{m_t^2}{v_t} $$把需要处理的量放在左边:\n$$ (1-\\beta_{1,t})g_t(\\theta_t - \\theta^*) = \\frac{\\sqrt{v_t}\\left((\\theta_t - \\theta^*)^2-(\\theta_{t+1} - \\theta^*)^2\\right)}{2\\alpha_t} - \\beta_{1,t} m_{t-1}(\\theta_t - \\theta^*) + \\frac{\\alpha_t m_t^2}{2\\sqrt{v_t}} $$这左边可以由 $1 \\ge 1-\\beta_{1,t} \\ge 1-\\beta_{1,1}$ 直接看作 $g_t(\\theta_t - \\theta^*)$ 量级, 右边现在已经分成三个部分了, 只需要累和来看每个部分的量级.\n一个显然的结论是, 在移动平均下, 易见 $m_t \\le G (m_0 \\le G), v_t \\le G^2 (v_0 \\le G^2)$.\n第一项: 忽略常数 $2$, 暂记 $\\gamma_t = \\frac{\\sqrt{v_t}}{\\alpha_t} = \\mathcal{O}(\\sqrt{T})$, 只要考虑:\n$$ M_1=\\sum_{t=1}^{T} \\gamma_t \\left((\\theta_t - \\theta^*)^2-(\\theta_{t+1} - \\theta^*)^2\\right) $$利用 Abel 求和法则, 可以得到:\n$$ M_1 =\\gamma_1(\\theta_1 - \\theta^*)^2 - \\gamma_{t+1}(\\theta_{T+1} - \\theta^*)^2 + \\sum_{t=1}^{T} (\\gamma_{t+1} - \\gamma_t)(\\theta_t - \\theta^*)^2 $$一般来说 $\\gamma_t = \\mathcal{O}(\\sqrt{T})$ 应该是单调不减的, 在 $\\gamma_t \\le \\gamma_{t+1}$ 的情况下, 可以得到:\n$$ \\begin{aligned} M_1 \u0026\\le \\gamma_1(\\theta_1 - \\theta^*)^2 + \\sum_{t=1}^{T} (\\gamma_{t+1} - \\gamma_t)D^2 \\\\ \u0026= C + (\\gamma_{t+1} - \\gamma_1)D^2 = \\mathcal{O}(\\sqrt{T}) \\end{aligned} $$ 问题就出在这个 \u0026ldquo;一般来说\u0026rdquo; 上, 因为尽管引入参数衰减, 实际上 Adam 并不能保证 $\\gamma_t$ 是单调不减的. 后面会提到这里的争议.\n第二项: 直接放缩即可:\n$$ \\begin{aligned} M_2 \u0026=\\sum_{t=1}^{T} \\beta_{1,t}m_{t-1}(\\theta_t - \\theta^*) \\le \\sum_{t=1}^{T} \\beta_{1,t} |m_{t-1}| D \\\\ \u0026\\le GD \\sum_{t=1}^{T} \\beta_{1,t} = G D \\beta_1 \\frac{1-\\lambda^T}{1-\\lambda} = \\mathcal{O}(1) \\end{aligned} $$ 第三项: $v_t$ 未必有下界, 有点麻烦! 直接写通式:\n$$ \\begin{aligned} m_t \u0026= \\sum_{s=1}^t (1-\\beta_{1,s})\\left(\\prod_{r=s+1}^{t}\\beta_{1,r}\\right)g_s \\le \\sum_{s=1}^t \\beta_1^{t-s}g_s\\\\ v_t \u0026= (1-\\beta_2)\\sum_{s=1}^t \\beta_2^{t-s}g_s^2 \\end{aligned} $$既然要控制 $\\frac{m_t^2}{\\sqrt{v_t}}$, 结合 $\\beta_1^4 \u003c \\beta_2$, 那么考虑 Young 不等式:\n$$ \\begin{aligned} m_t^4 \u0026\\le \\left(\\sum_{s=1}^t \\beta_2^{t-s}g_s^2 \\right) \\left(\\sum_{s=1}^t \\frac{\\beta_1^{\\frac{4}{3}(t-s)}}{\\beta_2^{\\frac{1}{3}(t-s)}}g_s^{\\frac{2}{3}} \\right)^{3} \\\\ \u0026\\le v_t^2 G^2 \\left(\\frac{1-\\mu^t}{1-\\mu}\\right)^3 = v_t^2 \\mathcal{O}(1) \\end{aligned} $$这里 $\\mu = \\beta_1^{\\frac{4}{3}} / \\beta_2^{\\frac{1}{3}} \u003c 1$. 因此:\n$$ M_3 =\\sum_{t=1}^{T} \\alpha_t^2 \\frac{m_t^2}{\\sqrt{v_t}} \\le C \\sum_{t=1}^{T} \\alpha_t^2 = C \\sum_{t=1}^{T} \\mathcal{O}\\left(\\frac{1}{t}\\right) = \\mathcal{O}(\\ln T) $$ 自此, 三项综合可以得到:\n$$ R(T) \\le \\mathcal{O}(\\sqrt{T}) + \\mathcal{O}(1) + \\mathcal{O}(\\ln T) = \\mathcal{O}(\\sqrt{T}) $$ Objection! 论文 [1] 的这个漏洞显然为人诟病. 于是论文 [2] Unknown-material 中, 作者指出 Adam 并不总是收敛的. 论文中给出了一个反例:\n我们取 $\\beta_1=0, \\beta_2=\\frac{1}{1+C^2}$. 设考虑在时间 $t$ 观测到的函数 $f_t$ 为:\n$$ f_t(x) = \\begin{cases} Cx \u0026 t \\equiv 1 \\pmod {3} \\\\ -x \u0026 \\text{Otherwise} \\\\ \\end{cases}, \\quad x \\in [-1,1] $$其中 $C\u003e2$ 是一个常数. 从宏观尺度上, $f=\\frac{1}{3}(C-2)x$, 最低点在 $x=-1$ 处. 显然 $f$ 和其他超参数满足定理条件, 然而经过 (冗长枯燥的) 计算可以得知, 由于每三次迭代中 $f$ 就会有两次向错误的方向更新, 加上移动平均导致的历史遗忘, 会导致无法正确收敛. 具体过程可以参考原文附录.\n为了解决这个问题, 作者提出为了确保 $\\gamma$ 是单调不减的, 可以在更新时让 $v_{t+1}$ 与 $v_t$ 取一个最大值作为更新值. 由此, 引出 Amsgrad 算法:\n算法Amsgrad\nAmsgrad 的更新公式如下:\n$$ \\begin{aligned} m_t \u0026 = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \u0026 = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\ \\hat{v}_t \u0026= \\max(v_t, \\hat{v}_{t-1}) \\\\ \\theta_t \u0026 = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot m_t \\end{aligned} $$此处省略了偏差修正.\n论文 [2] 采取实验证明, Amsgrad 具有更好的收敛性.\n辩护与和解 ","date":"2025-03-30T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/paper-reading/adam-convergence/","title":"论文阅读 - Adam 的收敛性分析"},{"content":"集成学习概述 定义\n集成学习 是指通过将相对比较容易构建但泛化性能一般的多个学习器进行结合, 以获得比单一学习器更好的泛化性能的一种机器学习方法.\n根据个体分类器是否由同一学习算法, 集成学习可以分为 同质集成 和 异质集成 两大类; 根据个体分类器的依赖关系, 可以将学习方法分为 序列化方法(串行集成) 和 并行化方法(并行集成) 两种.\n提升方法 PAC 框架下, 概念类强可学习和其弱可学习等价, 但弱可学习实现更容易, 提升方法就是指将弱学习算法提升为强学习算法的方法.\nAdaBoost 算法 算法AdaBoost\n输入: 给定训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i \\in \\mathcal{X}, y_i \\in \\mathcal{Y}=\\{-1,+1\\}, i=1,2,\\cdots,N$; 弱学习算法 $\\mathcal{L}$ 以及基本分类器个数 $T$;\n输出: 最终分类器 $f(x)$\n准备一个权重向量 $W_t=(w_{i,t})_{i=1}^N$, 表示第 $t$ 轮训练数据的权重分布, 初始时 $w_{i,1}=\\frac{1}{N}$; 在第 $t$ 轮学习中, 应用算法 $\\mathcal{L}$ 基于训练数据集 $D$ 和权重向量 $W_t$ 学得具有最小训练误差的基本分类器 $f_t(x)$, 即 $$f_t = \\arg\\min_{f} \\sum_{i=1}^N w_{i,t} \\mathbb{I}(f(x_i) \\neq y_i)$$ 计算 $f_t(x)$ 的误差率 $$e_t = \\sum_{i=1}^N w_{i,t} \\mathbb{I}(f_t(x_i) \\neq y_i)$$ 计算 $f_t(x)$ 的权值 $$\\alpha_t = \\frac{1}{2} \\ln \\frac{1-e_t}{e_t}$$ 按照投票权值更新训练数据集的权重分布 $$w_{i,t+1} = \\frac{w_{i,t}}{Z_t} \\exp(-\\alpha_t y_i f_t(x_i))$$ 其中 $Z_t$ 是规范化因子, 使得 $w_{i,t+1}$ 成为一个概率分布. 经过 $T$ 轮迭代后, 得到最终分类器 $$ \\begin{aligned} f(x) \u0026= \\text{sign}(G(x)) \\\\ G(x) \u0026= \\sum_{t=1}^T \\alpha_t f_t(x) \\end{aligned} $$ $G(x)$ 的符号决定了 $x$ 的类别, $|G(x)|$ 表示分类的确信度. 注意:\n$e_t$ 越小, $\\alpha_t$ 越大, 表示 $f_t(x)$ 的权重越大; $\\alpha_t$ 不仅仅平衡了 $f_t(x)$ 的权重, 还调节了样本分布的权重: $$w_{i,t+1}=\\begin{cases} \\frac{w_{i,t}}{Z_t} \\exp(\\alpha_t), \u0026 y_i=f_t(x_i) \\\\ \\frac{w_{i,t}}{Z_t} \\exp(-\\alpha_t), \u0026 y_i \\neq f_t(x_i) \\end{cases} $$ 对于那些错误样本, 下次迭代时的权重会增大, 以便让弱分类器更关注这些样本. 关于为什么要这样赋值 $\\alpha_t$, 由表达式可以得到 $$\\exp(\\alpha_t)e_t = \\exp(-\\alpha_t)(1-e_t)$$ 这表明分配给错误样本的权重之和与正确样本的权重之和相等. 计算可知 $$Z_t = \\sum_{i=1}^N w_{i,t} \\exp(-\\alpha_t y_i f_t(x_i)) = 2 \\sqrt{e_t(1-e_t)}$$ ","date":"2025-03-28T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/integrated-learning/","title":"机器学习基础(7) —— 集成学习"},{"content":"特征的线性组合 之前的二分类问题中, 如果把 $z=w^Tx+b$ 看做是衍生的新特征, 实际上感知机的模型就是 $y=\\text{sign}(z)$. 二项逻辑斯蒂回归模型中, $P(y=1 \\mid x) = \\sigma(z)=\\frac{1}{1+e^{-z}}$. 相当于引入了一个 sigmoid 函数进行非线性变换.\n因此, 神经网络应运而生, 主要想法:\n通过各维特征线性组合得到新特征 基于衍生特征通过非线性变换得到新特征 再对新特征进行线性组合和非线性变换, 逐层叠加 通过嵌套逼近复杂函数 多层前馈神经网络 设当前的 (衍生) 特征向量是\n$$ z = \\left(z^{(1)},z^{(2)},\\cdots,z^{(m)}\\right)^T $$进行线性组合\n$$ v \\cdot z - \\theta = \\sum_{i=1}^m v_i z^{(i)} - \\theta $$再通过非线性变换 (考虑到数学性质, 通常是 sigmoid 函数):\n$$ t = g(v \\cdot z - \\theta) $$ 定义\n多层前馈神经网络 是常见的神经网络模型:\n逐层排列神经元, 仅限于相邻层之间的完全连接; 接受外部输入信号的神经元在同一层, 称为 输入层; 最后一层神经元输出网络的结果, 称为 输出层; 输入层和输出层之间的神经元称为 隐藏层; 输入层直接接受激活函数, 输出层和隐藏层都对接受到的信号做激活函数变换. 所谓 感知机, 就是没有隐藏层的前馈神经网络.\n前面学到的感知机学习能力有限, 例如它无法解决异或问题. 但是, 只要再加一层隐藏层, 就可以解决.\n考虑一个单隐层的神经网络:\n输入层有 $n$ 个神经元来接受输入信号; 输出层有 $k$ 个神经元来输出结果, 且第 $l$ 个神经元的阈值是 $\\theta_l$; 隐藏层有 $m$ 个神经元, 第 $t$ 个神经元的阈值是 $\\gamma_t$. 输入层到隐藏层的权重是 $w_{jt}$, 隐藏层到输出层的权重是 $v_{tl}$. 因而, 隐藏层的输出是\n$$ z^{(t)}(x)=\\sigma \\left(\\sum_{j=1}^n w_{jt} x^{(j)} - \\gamma_t \\right) $$输出层的输出是\n$$ y^{(l)}(x) = \\sigma \\left( \\sum_{t=1}^m v_{tl} z^{(t)} - \\theta_l \\right) $$参数集为 $\\Theta = \\{w_{jt},v_{tl},\\gamma_t,\\theta_l\\}$\n误差反向传播算法 我们采用平方误差作为预测损失函数, 则\n$$ R(\\Theta) = \\sum_{i=1}^N R_i(\\Theta) = \\sum_{i=1}^N \\| y_i - \\hat{y}_i \\| ^2 = \\sum_{i=1}^N \\sum_{l=1}^k (y_i^{(l)} - \\hat{y}_i^{(l)})^2 $$依然采用经验风险最小化策略, 通过梯度下降法来求解参数集 $\\Theta$. 求偏导可得:\n$$ \\begin{aligned} \\frac{\\partial R_i(\\Theta)}{\\partial v_{tl}} \u0026= \\delta_i^{(l)}z^{(t)}(x_i) \\\\ \\frac{\\partial R_i(\\Theta)}{\\partial \\theta_l} \u0026= -\\delta_i^{(l)} \\\\ \\frac{\\partial R_i(\\Theta)}{\\partial w_{jt}} \u0026= s_i^{(t)} x_i^{(j)} \\\\ \\frac{\\partial R_i(\\Theta)}{\\partial \\gamma_t} \u0026= -s_i^{(t)} \\end{aligned} $$其中\n$$ \\begin{aligned} \\delta_i^{(l)}\u0026=-2(y_i^{(l)}-\\hat{y}_i^{(l)})\\hat{y}_i^{(l)}(1-\\hat{y}_i^{(l)}) \\\\ s_i^{(t)} \u0026= z^{(t)}(x_i)(1-z^{(t)}(x_i))\\sum_{l=1}^k v_{tl}\\delta_i^{(l)} \\end{aligned} $$给定学习率 $\\eta$, 按照 $\\alpha = \\alpha - \\eta \\frac{\\partial R_i(\\Theta)}{\\partial \\alpha}$ 进行迭代更新.\n采用正则化策略来缓解过拟合问题:\n$$ \\hat{\\Theta} = \\arg\\min_{\\Theta} (R(\\Theta) + \\lambda J(\\Theta)) $$其中 $J(\\Theta)$ 是正则化项, 通常是参数的 $L_2$ 范数, 所有参数的平方和.\n关于激活函数, 除了 sigmoid 函数, 还有 tanh 函数, ReLU 函数等. 前两者函数性质连续, 但是在部分情况可能导数接近 $0$, 从而导致梯度消失问题. 相对之下, ReLU 函数梯度计算简单. 还有带泄漏的 ReLU 函数:\n$$ f(x) = \\begin{cases} x \u0026 x\u003e0 \\\\ \\lambda x \u0026 x \\leq 0 \\end{cases} $$等等.\n","date":"2025-03-25T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/nn-beginner/","title":"机器学习基础(6) —— 神经网络学习初步"},{"content":"对偶理论 对于一般的约束优化问题:\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 c_i(x) \\leq 0, \\quad i \\in \\mathcal{I} \\\\ \u0026 c_i(x) = 0, \\quad i \\in \\mathcal{E} \\end{aligned} $$Lagrange 函数为:\n$$ L(x, \\lambda, \\nu) = f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) + \\sum_{i \\in \\mathcal{E}} \\nu_i c_i(x) $$其中 $\\lambda_i \\ge 0$.\n定义\nLagrange 对偶函数 $g: \\mathbb{R}_+^m \\times \\mathbb{R}^p \\to [-\\infty, +\\infty)$ 定义为:\n$$ g(\\lambda, \\nu) = \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu) $$ 定理弱对偶原理\n若 $\\lambda \\ge 0$, 则 $g(\\lambda, \\nu) \\le p^\\ast$.\n证明\n对 $x_0 \\in \\mathcal{X}$, 有:\n$$ g(\\lambda, \\nu) = \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu) \\le L(x_0, \\lambda, \\nu) \\le f(x_0) $$对 $x_0$ 取 $\\inf$ 得到\n$$ g(\\lambda, \\nu) \\le \\inf_{x \\in \\mathcal{X}} f(x) = p^\\ast $$ 定义\nLagrange 对偶问题 形式如下:\n$$ \\max_{\\lambda \\ge 0, \\nu} g(\\lambda, \\nu)= \\max_{\\lambda \\ge 0, \\nu} \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu) $$称 $\\lambda, \\nu$ 为对偶变量, 设最优值为 $q^\\ast$. $q^\\ast$ 是 $p^\\ast$ 的最优下界, 称 $p^\\ast-q^\\ast$ 为 对偶间隙.\n示例: 线性规划问题 $$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 x \\ge 0 \\end{aligned} $$Lagrange 函数为:\n$$ \\begin{aligned} L(x, \\lambda, \\nu) \u0026= c^T x + \\lambda^T (Ax - b) - \\nu^T x \\\\ \u0026= -b^T \\lambda + (c + A^T \\lambda - \\nu)^T x \\end{aligned} $$对偶函数:\n$$ g(\\lambda, \\nu) = \\inf_{x \\ge 0} L(x, \\lambda, \\nu) = \\begin{cases} -b^T \\lambda, \u0026 c + A^T \\lambda - \\nu = 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 -b^T \\lambda \\\\ \\text{s.t.} \\quad \u0026 c + A^T \\lambda - \\nu = 0 \\\\ \u0026 \\nu \\ge 0 \\end{aligned} $$可以计算证明, 线性规划问题和对偶问题互为对偶.\n示例: 范数最小化问题 $$ \\begin{aligned} \\min \\quad \u0026 \\|x\\| \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$对偶函数:\n$$ g(\\nu) = \\inf_x(\\|X\\| - \\nu^T (Ax - b)) = \\begin{cases} b^T\\nu, \u0026 \\|A^T \\nu\\|_* \\le 1 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} $$其中 $\\|v\\|_*=\\sup_{\\|x\\|\\le 1} x^T v$ 为 $v$ 的对偶范数 (关于为什么自证). 因此对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 b^T \\nu \\\\ \\text{s.t.} \\quad \u0026 \\|A^T \\nu\\|_* \\le 1 \\end{aligned} $$示例: 最大割问题 上回说到, 最大割问题可以写成:\n$$ \\begin{aligned} \\max \\quad \u0026 x^T W x \\\\ \\text{s.t.} \\quad \u0026 x_i^2 = 1 \\end{aligned} $$首先加负号变成 $\\min$, Lagrange 函数为:\n$$ \\begin{aligned} L(x, y) \u0026= -x^T W x + \\sum_{i=1}^n y_i (x_i^2 - 1) \\\\ \u0026= x^T(\\text{diag}(y) - W) x - \\mathbf{1}^T y \\end{aligned} $$对偶函数:\n$$ g(y) = \\inf_x L(x, y) = \\begin{cases} -\\mathbf{1}^T y, \u0026 \\text{diag}(y) - W \\succeq 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 -\\mathbf{1}^T y \\\\ \\text{s.t.} \\quad \u0026 \\text{diag}(y) - W \\succeq 0 \\end{aligned} $$再来考虑这个对偶问题的对偶问题.\n对偶函数:\n$$ \\begin{aligned} g(X)\u0026=\\inf_y(\\mathbf{1}^Ty) - \\left\u003c\\text{diag}(y) - W, X\\right\u003e \\\\ \u0026= \\inf \\left(\\sum_{i=1}^n (1-X_{ii})y_i + \\left\u003c W, X \\right\u003e \\right) \\\\ \u0026= \\begin{cases} \\left\u003c W, X \\right\u003e, \u0026 X_{ii} = 1, i = 1, \\ldots, n \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 \\left\u003c W, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 X_{ii} = 1, i = 1, \\ldots, n \\\\ \u0026 X \\succeq 0 \\end{aligned} $$示例: 共轭函数 $$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 Ax \\le b \\\\ \u0026 Cx = d \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\lambda, \\nu) \u0026= \\inf_x(f(x) + \\lambda^T (Ax - b) + \\nu^T (Cx - d)) \\\\ \u0026=\\inf_x(f(x) + (A^T \\lambda + C^T \\nu)^T x - b^T \\lambda - d^T \\nu) \\\\ \u0026=-f^\\ast(-A^T \\lambda - C^T \\nu) - b^T \\lambda - d^T \\nu \\end{aligned} $$其中 $f^\\ast(v) = \\sup_x(x^T v - f(x))$ 为 $f$ 的共轭函数.\n对偶性 定义\n弱对偶性: $d^\\ast \\le p^\\ast$. 对一般约束优化问题成立. 强对偶性: $d^\\ast = p^\\ast$, 且若一个线性规划问题有最优解, 则其对偶问题有最优解, 且最优值相等. 一般不成立, 但通常对凸优化问题成立. 称保证凸问题强对偶性成立的条件为 约束品性.\n考虑下面这个不满足强对偶性的例子.\n$$ \\begin{aligned} \\min \\quad \u0026 x_0-x_1 \\\\ \\text{s.t.} \\quad \u0026 x_0 \\ge \\sqrt{x_1^2+1} \\end{aligned} $$显然, $x_0-x_1 \u003e 0$, 但当 $x_0 \\to \\infty$ 时, $x_0-x_1 \\to 0$, 因此 $p^\\ast = 0$, 但是不可达. 而对偶问题是:\n$$ \\begin{aligned} \\max \\quad \u0026 \\lambda \\\\ \\text{s.t.} \\quad \u0026 1 \\ge \\sqrt{1 + \\lambda^2} \\end{aligned} $$ 因此 $\\lambda \\le 0$, $d^\\ast = 0$. 虽然 $d^\\ast = p^\\ast$, 但原问题是没有最优解的.\n当然, 也有使得 $p^\\ast \\ne d^\\ast$ 的例子.\n改写问题形式 当对偶问题难以推导或没有价值时, 可以尝试改写原问题的形式.\n引入新变量与等式约束; 将显式约束隐式化或将隐式约束显式化; 改变目标函数或者约束函数的形式. 例如, 用 $\\phi(f(x))$ 取代 $f(x)$, 其中 $\\phi$ 是凸的增函数. 先来看几个引入等式约束的例子.\n示例: 函数值最小化问题 $$ \\min f(Ax+b) $$直接做对偶是常数, 没有意义. 可以改写为:\n$$ \\begin{aligned} \\min \\quad \u0026 f(y) \\\\ \\text{s.t.} \\quad \u0026 y = Ax + b \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 b^T \\nu-f^\\ast(\\nu) \\\\ \\text{s.t.} \\quad \u0026 A^T \\nu = 0 \\end{aligned} $$示例: 范数逼近问题 $$ \\min \\|Ax-b\\|_2 $$改写成:\n$$ \\begin{aligned} \\min \\quad \u0026 \\|y\\| \\\\ \\text{s.t.} \\quad \u0026 y = Ax - b \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\nu) \u0026= \\inf_{x,y}(\\|y\\| + \\nu^Ty - \\nu^TAx + \\nu^Tb) \\\\ \u0026= \\begin{cases} \\nu^T b + \\inf_y(\\|y\\| + \\nu^Ty), \u0026 A^T \\nu = 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\\\ \u0026= \\begin{cases} \\nu^T b, \u0026 A^T \\nu = 0, \\|\\nu\\|_* \\le 1 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 \\nu^T b \\\\ \\text{s.t.} \\quad \u0026 A^T \\nu = 0 \\\\ \u0026 \\|\\nu\\|_* \\le 1 \\end{aligned} $$示例: L1 正则化问题 $$ \\min_{x\\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax-b\\|^2 + \\mu \\|x\\|_1 $$改写成:\n$$ \\begin{aligned} \\min \\quad \u0026 \\frac{1}{2}\\|r\\|^2 + \\mu \\|x\\|_1 \\\\ \\text{s.t.} \\quad \u0026 r = Ax - b \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\nu) \u0026= \\inf_{x,r}\\left(\\frac{1}{2}\\|r\\|_2^2 + \\mu \\|x\\|_1 + \\lambda^T(r-Ax+b)\\right) \\\\ \u0026= \\inf_{x,r}\\left(\\frac{1}{2}\\|r\\|^2 + \\lambda^Tr + \\mu \\|x\\|_1 - (A^T\\lambda)^Tx + b^T\\lambda\\right) \\\\ \u0026= \\begin{cases} b^T \\lambda - \\frac{1}{2} \\| \\lambda \\|^2, \u0026 \\| A^T \\lambda \\|_{\\infty} \\le \\mu \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 b^T \\lambda - \\frac{1}{2} \\| \\lambda \\|^2 \\\\ \\text{s.t.} \\quad \u0026 \\| A^T \\lambda \\|_{\\infty} \\le \\mu \\end{aligned} $$现在考虑显式和隐式约束转化的例子.\n示例: 带边界约束的线性规划问题 $$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 -\\mathbf{1} \\le x \\le \\mathbf{1} \\end{aligned} $$我们把边界要求隐藏在目标函数中:\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) =\\begin{cases} c^T x, \u0026 -\\mathbf{1} \\le x \\le \\mathbf{1} \\\\ +\\infty, \u0026 \\text{otherwise} \\end{cases} \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\nu) \u0026= \\inf_{-\\mathbf{1} \\le x \\le \\mathbf{1}}(c^Tx+\\nu^T(Ax-b)) \\\\ \u0026= -b^T \\nu - \\|A^T \\nu + c\\|_1 \\end{aligned} $$对偶问题:\n$$ \\begin{aligned} \\max \\quad \u0026 -b^T \\nu - \\|A^T \\nu + c\\|_1 \\\\ \\text{s.t.} \\quad \u0026 \\nu \\ge 0 \\end{aligned} $$示例: 广义不等式约束优化问题 $$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 c_i(x) \\preceq_{K_i} 0, \\quad i \\in \\mathcal{I} \\\\ \u0026 c_i(x) = 0, \\quad i \\in \\mathcal{E} \\end{aligned} $$其中对于 $i \\in \\mathcal{I}$, $c_i: \\mathbb{R}^n \\to \\mathbb{R}^{k_i}$ 是向量值函数, $K_i$ 是适当锥. 对于 $i \\in \\mathcal{E}$, $c_i: \\mathbb{R}^n \\to \\mathbb{R}$ 是标量函数. 实际上这种情况下, 其 Lagrange 函数为:\n$$ L(x, \\lambda, \\nu) = f(x) + \\sum_{i \\in \\mathcal{I}} \\left\u003c\\lambda_i, c_i(x)\\right\u003e + \\sum_{i \\in \\mathcal{E}} \\nu_i c_i(x), \\quad \\lambda_i \\in K_i^\\ast $$这里 $K_i^\\ast$ 是 $K_i$ 的对偶锥. 此时也依然满足 $L(x, \\lambda, \\nu) \\le f(x)$. 对偶问题依然是 $\\max_{\\lambda \\in K_i^\\ast, \\nu} g(\\lambda, \\nu)$.\n示例: 半定规划问题 $$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c C, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 \\left\u003c A_i, X \\right\u003e = b_i, \\quad i = 1, \\ldots, m \\\\ \u0026 X \\succeq 0 \\end{aligned} $$对偶函数:\n$$ \\begin{aligned} g(\\lambda, \\nu) \u0026= \\inf_X \\left( \\left\u003c C, X \\right\u003e + \\sum_{i=1}^m \\lambda_i (\\left\u003c A_i, X \\right\u003e - b_i) - \\left\u003c\\nu,X\\right\u003e \\right) \\\\ \u0026= \\begin{cases} b^T\\lambda, \u0026 \\sum_{i=1}^m \\lambda_i A_i - C +\\nu \\succeq 0 \\\\ -\\infty, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$对偶问题:\n$$ \\max \\quad b^T\\lambda \\\\ \\text{s.t.} \\quad \\sum_{i=1}^m \\lambda_i A_i - C +\\nu \\succeq 0 $$可以证明, 半定规划问题和对偶问题互为对偶.\n带约束凸优化问题的最优性理论 综合来看, 前面的问题其实都可以写成如下的形式:\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 c_i(x) \\le 0, i=1, \\ldots, m \\end{aligned} $$其中 $f(x)$ 为适当的凸函数, $c_i(x)$ 也是凸函数且 $\\text{dom} c_i = \\mathbb{R}^n$.\n定义\n给定集合 $\\mathcal{D}$, 设其仿射包为 $\\text{affine} \\mathcal{D}$, 则其 相对内点集 定义为:\n$$ \\text{relint} \\mathcal{D} = \\{x \\in \\mathcal{D} \\mid \\exists r \u003e 0, B(x,r) \\cap \\text{affine} \\mathcal{D} \\subset \\mathcal{D}\\} $$ 定义\n若对于凸优化问题\n$$ \\begin{aligned} \\min \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 c_i(x) \\le 0, i=1, \\ldots, m \\end{aligned} $$存在 $x \\in \\text{relint} \\mathcal{D}$, 使得\n$$c_i(x) \u003c 0, i=1, \\ldots, m$$则称对于该问题 Slater 约束品性 成立. 该品性也称为 Slater 条件.\n注意: 如果某个不等式约束是仿射函数时, Slater 可以对这个不等式约束放宽到 $c_i(x) \\le 0$.\n定理\n若 Slater 约束品性成立, 则强对偶性成立.\n证明\n这里假设 $\\mathcal{D}$ 内部非空, $A$ 是行满秩的 (否则可以去掉冗余约束), $p^\\ast$ 是有限的. 要证明当 $d^\\ast \u003e -\\infty$ 时, 存在对偶可行解 $(\\lambda^\\ast, \\nu^\\ast)$ 使得 $g(\\lambda^\\ast, \\nu^\\ast) = d^\\ast=p^\\ast$.\n定义集合\n$$ \\begin{aligned} \\mathbb{A}\u0026= \\{(u,v,t) \\mid \\exists x \\in \\mathcal{D}, c_i(x) \\le u_i (i=1, \\ldots, m), Ax-b=v, f(x) \\le t\\} \\\\ \\mathbb{B}\u0026= \\{(0,0,s) \\mid s \\le p^\\ast\\} \\end{aligned} $$若 $(0,0,t) \\in \\mathbb{A} \\cap \\mathbb{B}$, 则 $f(x) \\le t\u003c p^\\ast$ 矛盾. 因而两集合不交. 由于两者都是凸集, 由超平面分离定理, 存在 $(\\lambda, \\nu, \\mu) \\ne 0$ 和 $\\alpha$ 使得\n$$ \\begin{aligned} \\lambda^Tu + \\nu^Tv + \\mu t \u0026\\ge \\alpha, \\quad \\forall (u,v,t) \\in \\mathbb{A} \\\\ \\lambda^Tu + \\nu^Tv + \\mu t \u0026\\le \\alpha, \\quad \\forall (u,v,t) \\in \\mathbb{B} \\end{aligned} $$显见, $\\lambda \\ge 0, \\mu \\ge 0$, 否则可以让 $\\mu_i, t \\to +\\infty$ 使得集合 $\\mathbb{A}$ 上不等式左侧无下界. 由 $\\mathbb{B}$ 的不等式, 立得 $\\mu p^\\ast \\le \\alpha$.\n对于 $(u,v,t)=(c_i(x), Ax-b, f(x))$, 代入有\n$$ \\sum_{i=1}^m \\lambda_i c_i(x) + \\nu^T(Ax-b) + \\mu f(x) \\ge \\alpha \\ge \\mu p^\\ast $$若 $\\mu \u003e 0$, 则上式恰好对应 Lagrange 函数:\n$$ L(x, \\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}) \\ge p^\\ast $$故 $g(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}) \\ge p^\\ast$, 再结合弱对偶性, $g(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}) = p^\\ast$. 则此情况下强对偶性成立, 且最优解可达.\n若 $\\mu=0$, 可以得到对于所有 $x \\in \\mathcal{D}$, 有\n$$ \\sum_{i=1}^m \\lambda_i c_i(x) + \\nu^T(Ax-b) \\ge 0 $$令 $x_S$ 为满足 Slater 条件的点, $Ax_S=b, c_i(x_S) \u003c 0$, 但 $\\lambda \\ge 0$ 则必须 $\\lambda = 0$. 因此\n$$ \\nu^T(Ax-b) \\ge 0, \\quad \\forall x \\in \\mathcal{D} $$$x_S$ 恰好是谷底的 $0$, 四周全部都 $\\ge 0$ 不太可能. 更具体地, 由于 $(\\lambda, \\nu, \\mu) \\ne 0$, 则 $\\nu \\ne 0$. $A$ 是行满秩的, 则 $A^T \\nu \\ne 0$, 因为 $x_S \\in \\text{int}\\mathcal{D}$, 则存在微扰 $e$ 使得\n$$\\widetilde{x} = x_S + e \\in \\mathcal{D}, \\quad v^TAe \u003c 0$$但是\n$$ v^TAe = v^TA(\\widetilde{x}-x_S) = v^T(A\\widetilde{x} - b) \\ge 0$$矛盾. 因此 $\\mu \u003e 0$, 强对偶性成立.\n现在假设 Slater 成立, $x^\\ast, \\lambda^\\ast$ 是原问题和对偶问题的最优解. 由强对偶性,\n$$ \\begin{aligned} f(x^\\ast) = g(\\lambda^\\ast) \u0026= \\inf_{x} f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) + \\sum_{i \\in \\mathcal{E}} \\lambda_i c_i(x) \\\\ \u0026\\le f(x^\\ast) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x^\\ast) + \\sum_{i \\in \\mathcal{E}} \\lambda_i c_i(x^\\ast) \\\\ \u0026\\le f(x^\\ast) \\end{aligned} $$因此等号要成立, 即 $\\lambda_i c_i(x^\\ast) = 0, i \\in \\mathcal{I}$, 这称为互补条件 (complementary slackness).\n定理凸优化问题的一阶充要条件\n对于凸优化问题, 用 $a_i$ 表示矩阵 $A^T$ 的第 $i$ 列, 如 果 Slater 条件成立, 那么$x^\\ast, \\lambda^\\ast$ 分别是原始, 对偶全局最优解当且仅当\n稳定性条件: $ 0 = \\nabla f(x^\\ast) + \\sum_{i \\in \\mathcal{I}} \\lambda_i^\\ast \\nabla c_i(x^\\ast) + \\sum_{i \\in \\mathcal{E}} \\lambda_i^\\ast a_i$ 原始可行性条件: $Ax^\\ast = b, i \\in \\mathcal{E}; c_i(x^\\ast) \\le 0, i \\in \\mathcal{I}$ 对偶可行性条件: $\\lambda_i^\\ast \\ge 0, i \\in \\mathcal{I}$ 互补松弛条件: $\\lambda_i^\\ast c_i(x^\\ast) = 0, i \\in \\mathcal{I}$ 证明\n必要性已知, 只证明充分性.\n考虑 Lagrange 函数\n$$ L(x, \\lambda) = f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) + \\sum_{i \\in \\mathcal{E}} \\lambda_i (a_i^T x - b_i) $$固定 $\\lambda = \\bar{\\lambda}$, 易见 $L$ 是关于 $x$ 的凸函数. 由凸函数全局最优点的一阶充要性可知, 此时 $\\bar{x}$ 就是全局的极小点. 由 Lagrange 对偶函数的定义, 有\n$$ L(\\bar{x}, \\bar{\\lambda}) = \\inf_x L(x, \\bar{\\lambda}) = g(\\bar{\\lambda}) $$又由原始可行性条件和互补松弛条件, 有\n$$ L(\\bar{x}, \\bar{\\lambda}) = f(\\bar{x}) $$由弱对偶性\n$$ L(\\bar{x}, \\bar{\\lambda}) = f(\\bar{x}) \\ge p^\\ast\\ge d^\\ast \\ge g(\\bar{\\lambda}) = L(\\bar{x}, \\bar{\\lambda}) $$因此等号成立, $p^\\ast = d^\\ast$, 强对偶性成立, $\\bar{x}, \\bar{\\lambda}$ 是全局最优解.\n例子: 仿射空间的投影问题 $$ \\begin{aligned} \\min \\quad \u0026 \\|x - y\\|^2 \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$Lagrange 函数为\n$$ L(x, \\lambda) = \\|x - y\\|^2 + \\lambda^T(Ax - b) $$Slater 条件成立, 由一阶充要条件, 有\n$$ \\begin{aligned} x^\\ast - y + A^T \\lambda^\\ast \u0026= 0 \\\\ Ax^\\ast \u0026= b \\end{aligned} $$解之\n$$ \\begin{aligned} \\lambda^\\ast \u0026= (AA^T)^{-1}(Ay-b) \\\\ x^\\ast \u0026= y - A^T(AA^T)^{-1}(Ay-b) \\end{aligned} $$显然, $x^\\ast$ 是 $y$ 在 $Ax=b$ 上的投影.\n例子: 基追踪问题 $$ \\begin{aligned} \\min \\quad \u0026 \\|x\\|_1 \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\end{aligned} $$这个函数实际是不光滑的. 把 $x$ 写作 $x^+-x^-$, 再令 $y=[x^+, x^-]$, 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\mathbf{1}^T y \\\\ \\text{s.t.} \\quad \u0026 [A, -A] y = b \\\\ \u0026 y \\ge 0 \\end{aligned} $$按照 KKT 条件, 有\n$$ \\begin{aligned} \\mathbf{1} + [A, -A]^T \\lambda^\\ast - \\nu^\\ast \u0026= 0 \\\\ [A, -A] y^\\ast \u0026= b \\\\ y^\\ast \u0026\\ge 0 \\\\ \\nu^\\ast \u0026\\ge 0 \\\\ \\nu^\\ast \\odot y^\\ast \u0026= 0 \\end{aligned} $$直接推导也得到相应结果, 二者实际上是等价的. (利用 $x^\\ast=y_i^\\ast-y_{i+n}^\\ast$, 代入验证最优点处方向导数为 $0$ 即可)\n","date":"2025-03-23T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/opt-theory/","title":"最优化方法(5) —— 最优性理论"},{"content":"特征的分类能力评估 定义\n给定数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i=\\left(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(m)}\\right) \\in \\mathcal{X}$ 是第 $i$ 个样本的特征向量, $y_i \\in \\mathcal{Y}=\\{c_1,c_2,\\cdots,c_K\\}$ 是第 $i$ 个样本的标签. 假设数据集 $D$ 根据特征分成了 $K$ 个子集 $D_1,D_2,\\cdots,D_K$, 定义 经验熵 为\n$$ H(D) = -\\sum_{k=1}^K \\frac{|D_k|}{|D|} \\log_2 \\frac{|D_k|}{|D|} $$现在给定某维特征 $A$ 和其取值集合 $\\{a_1,a_2,\\cdots,a_m\\}$, 根据 $A$ 的取值将数据集 $D$ 分成了 $m$ 个子集 $D_1^A,D_2^A,\\cdots,D_m^A$, 并进一步考虑 $D_i^A$ 中的标签分布, 定义 条件经验熵 为\n$$ H(D|A) = \\sum_{i=1}^m \\frac{|D_i^A|}{|D|} H(D_i^A) $$ 如果条件经验熵和经验熵之差越大, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.\n定义\n属性 $A$ 对数据集 $D$ 的 信息增益 $g(D,A)$ 定义为\n$$ g(D,A) = H(D) - H(D|A) $$ 考虑到信息增益的计算会偏向于选择取值较多的特征, 为了避免这种情况, 引入信息增益率来评估特征的分类能力.\n定义\n特征 $A$ 的 分裂信息 $IV(A)$ 定义为\n$$ IV(A) = -\\sum_{i=1}^m \\frac{|D_i^A|}{|D|} \\log_2 \\frac{|D_i^A|}{|D|} $$特征 $A$ 的 信息增益率 $g_R(D,A)$ 定义为\n$$ g_R(D,A) = \\frac{g(D,A)}{IV(A)} $$ 分裂信息其实就是按照 $A$ 取值作划分的经验熵.\n除了信息增益和信息增益率, 还有 Gini 指数可以用来评估特征的分类能力.\n定义\n数据集 $D$ 的 Gini 指数 $\\text{Gini}(D)$ 定义为\n$$ \\text{Gini}(D) = 1 - \\sum_{k=1}^K \\left(\\frac{|D_k|}{|D|}\\right)^2 $$特征 $A$ 的 Gini 指数 $\\text{Gini}(D,A)$ 定义为\n$$ \\text{Gini}(D,A) = \\sum_{i=1}^m \\frac{|D_i^A|}{|D|} \\text{Gini}(D_i^A) $$如果按照特征 $A$ 是否取值为 $a_i$ 对数据集 $D$ 进行划分 $D=D_i^A \\cup (D-D_i^A)$, 则 $A=a_i$ 的 Gini 指数 $\\text{Gini}_d(D,A=a_i)$ 定义为\n$$ \\text{Gini}_d(D,A=a_i) = \\frac{|D_i^A|}{|D|} \\text{Gini}(D_i^A) + \\frac{|D-D_i^A|}{|D|} \\text{Gini}(D-D_i^A) $$ Gini 指数可以看作任取两个样本, 它们的标签不一致的概率. 如果 Gini 指数越小, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.\n决策树模型 算法生成决策树\n输入: 训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 特征集 $\\mathcal{A}=\\{A_1,A_2,\\cdots,A_m\\}$, 最优特征选择函数 $F$.\n输出: 决策树 $T$.\n若数据集 $D$ 中所有样本的标签都是 $c_k$, 则生成一个类标记为 $c_k$ 的叶结点, 返回 $T$; 若 $A=\\emptyset$, 且 $D$ 非空, 则生成一个单节点树, 并以 $D$ 中样本数最多的类标记作为该节点的类标记, 返回 $T$; 计算 $A^\\ast=F(D,\\mathcal{A})$; 对 $A^\\ast$ 的每一个取值 $a_i$, 构造一个对应于 $D_i$ 的子节点; 若 $D_i=\\emptyset$, 则将子节点标记为叶结点, 类标记为 $D$ 中样本数最多的类标记; 否则, 将 $D_i$ 中样本数最多的类标记作为该节点的类标记 对每个 $D_i$ 对应的非叶子节点, 以 $D_i$ 为训练集, 以 $\\mathcal{A}-\\{A^\\ast\\}$ 为特征集, 递归调用 1-6 步, 构建决策树 $T$. 如果以信息增益为特征选择函数, 即 $A^\\ast = \\arg\\max_{A \\in \\mathcal{A}} g(D,A)$, 则算法对应于 ID3 算法; 如果以信息增益率为特征选择函数, 即 $A^\\ast = \\arg\\max_{A \\in \\mathcal{A}} g_R(D,A)$, 则算法对应于 C4.5 算法.\n二路划分会采用以特征的可能取值为切分点的二分法划分当前数据集, 例如与选择 Gini 指数最小的特征和切分点对应的特征值, 即 $(A^\\ast,a^\\ast) = \\arg\\min_{A \\in \\mathcal{A},a \\in V(A)} \\text{Gini}_d(D,A=a)$, 则算法对应于 CART 算法.\n为了降低过拟合风险, 可以对决策树进行剪枝. 常用的是后剪枝, 即先生成一棵完全生长的决策树, 然后根据泛化性能决定是否剪枝. 也可以采用正则化方法, 例如, 定义决策树 $T$ 的损失或代价函数:\n$$ C_\\alpha(T) = C(T) + \\alpha |T| $$其中 $C(T)$ 用于衡量 $T$ 对 $D$ 的拟合程度, $|T|$ 表示 $T$ 的叶结点个数, $\\alpha \\geq 0$ 用于权衡拟合程度和模型复杂度.\nCART 算法有特别的剪枝处理: 从 CART 算法生成得到完整决策树 $T_0$ 开始, 产生一个递增的权衡系数序列 $0=\\alpha_0 \u003c \\alpha_1 \u003c \\cdots \u003c \\alpha_n \u003c +\\infty$ 和一个嵌套的子树序列 $\\{T_0, T_1, \\cdots, T_n\\}$, $T_i$ 为 $\\alpha \\in [\\alpha_i, \\alpha_{i+1})$ 时的最优子树, $T_n$ 是根节点单独构成的树.\n如果是连续特征, 则可以考虑将其离散化, 例如, 通过二分法将其划分为两个区间, 选择最优划分点.\n现在继续从经验风险的角度来看决策树模型.采用 $0-1$ 损失函数, 设节点 $t$ 设置的标记是 $c_k$, 则在 $t$ 对应的数据集上的经验风险为\n$$ \\frac{1}{|D_t|} \\sum_{i=1}^{|D_t|} I(y_i \\neq c_k) $$显见, 等价于\n$$ \\max_{c_k \\in \\mathcal{Y}} \\frac{1}{|D_t|} \\sum_{i=1}^{|D_t|} I(y_i = c_k) $$从现在来看, 决策树构造过程中划分的单元都是矩形的, 即分类边界是若干与特征坐标轴平行的边界组成. 多变量决策树模型允许用若干特征的线性组合来划分数据集, 对每个非叶结点学习一个线性分类器.\n最小二乘回归树模型 CART 算法用于回归问题时, 采用平方误差损失函数选择属性和切分点.\n算法最小二乘回归树模型\n输入: 训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 特征集 $\\mathcal{A}=\\{A_1,A_2,\\cdots,A_m\\}$.\n输出: 回归树 $T$.\n设回归树将输入空间划分为 $M$ 个单元 $R_1,R_2,\\cdots,R_M$, 并在每个单元上有一个固定的输出值 $c_m$, 则回归树模型可以表示为\n$$ f(x)=\\sum_{m=1}^M c_m I(x \\in R_m) $$ 如果采用平方误差, 则 $R_m$ 的输出值 $c_m$ 应该是 $R_m$ 中所有样本输出值的均值, 即\n$$ \\hat{c}_m = \\frac{1}{|R_m|} \\sum_{x_i \\in R_m} y_i $$ 对于一个输入空间, 若选用第 $j$ 维特征变量作为切分变量, $s$ 作为切分点, 则可以将输入空间划分为两个区域\n$$ R_1(j,s) = \\{x|x^{(j)} \\leq s\\}, \\quad R_2(j,s) = \\{x|x^{(j)} \u003e s\\} $$则可以通过求解优化问题\n$$ \\min_{j,s} \\left[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + \\min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2\\right] $$来确定最优切分变量 $j$ 和切分点 $s$. 实际上这里的 $c_i$ 就应该取 2 步中的 $\\hat{c}_m$.\n从初始输入空间开始, 按照误差最小原则递归划分, 重复如上过程, 直到满足停止条件.\n对于剪枝, 和分类任务处理框架一致, 采用\n$$ C_\\alpha(T) = C(T) + \\alpha |T| $$计算损失, 其中\n$$C(T) = \\sum_{t=1}^{|T|} N_tQ_t(T) = \\sum_{t=1}^{|T|} \\sum_{x_i \\in R_t} (y_i-\\hat{c}_t)^2$$$N_t$ 表示叶结点 $t$ 中的样本数, $Q_t(T)$ 表示叶结点 $t$ 的均方损失, $\\hat{c}_t$ 表示叶结点 $t$ 的输出值均值.\n","date":"2025-03-18T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/","title":"机器学习基础(5) —— 决策树模型"},{"content":"k-近邻算法 k-近邻算法的主要思想是, 对于一个给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点, 然后根据这 $k$ 个样本点的类别进行多数占优的投票方式来预测 $x$ 的类别.\n在 $n$ 维实数空间 $\\mathbb{R}$ 中, 通常用 Minkowski 距离来度量两个点 $x_i, x_j$ 的相似性:\n定义\n设 $x_i, x_j \\in \\mathbb{R}^n$, 则 $x_i, x_j$ 之间的 Minkowski 距离 $\\text{dist}_p(x_i,x_j)$ 定义为\n$$ \\text{dist}_p(x_i,x_j) = \\left( \\sum_{l=1}^n |x_i^l - x_j^l|^p \\right)^{1/p} $$ $p=1$ 时, 就是 Manhattan 距离; $p=2$ 时, 就是 Euclidean 距离; $p=\\infty$ 时, 就是 Chebyshev 距离. 在必要时, 还可以给每个维度的特征值加权.\n定义\n给定训练样本集 $D = \\{(x_i, y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_k\\}$, 以及距离度量 $\\text{dist}$, k-近邻算法 的基本步骤如下:\n基于度量 $\\text{dist}$, 对于给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点所构成的邻域 $N_k^{\\text{dist}}(x)$;\n采用如下的多数投票规则来预测 $x$ 的类别:\n$$ y = \\arg\\max_{c_i} \\sum_{x_j \\in N_k^{\\text{dist}}(x)} I(y_j = c_i) $$ 如果把 0-1 作为损失函数, 那么 k-近邻算法实际上就是让经验风险最小化.\n最近邻算法 在 k-近邻算法中, 当 $k=1$ 时, 称为最近邻算法. 因此, 特点是偏差小, 方差大. 这其实是特征空间的一个划分 $\\mathcal{X}=\\bigcup_{i=1}^n \\{R_i\\}$. 对每个划分单元 $R_i$, 该单元的数据点到其他样本的距离都不会小于到 $x_i$ 的距离.\n最近邻算法的扩展 给定样本集 $D = \\{(x_i,y_i)\\}_{i=1}^n$, 以 $D_i$ 表示属于类 $c_i$ 的样本集, 希望找一个方式把每个 $D_i$ 分成 $k$ 个簇 $(D_{i1}, D_{i2}, \\cdots, D_{ik})$, 使得数据分布的方差最小, 即\n$$ (D^\\ast_{i1}, D^\\ast_{i2}, \\cdots, D^\\ast_{il}) = \\arg\\min_{D_{i1}, D_{i2}, \\cdots, D_{ik}} \\sum_{j=1}^k \\sum_{(x_t,y_t) \\in D_{ij}} \\Vert x_t-c_{ij} \\Vert_2^2 $$然而很难找到最优解, 因此采用迭代的方式来近似求解:\n定义\nK-means 算法 的基本步骤如下:\n初始化 $k$ 个簇的中心 $c_{ij}$; 对每个 $(x_t),(y_t) \\in D_i$ (即 $y_t=c_i$), 令 $$I_{x_t}= \\arg\\min_{j} \\Vert x_t-c_{ij} \\Vert_2^2$$ 即将 $x_t$ 分配到最近的簇; 对每个 $D_{ij}$, 更新均值 $$c_{ij} = \\frac{1}{|D_{ij}|} \\sum_{(x_t,y_t) \\in D_{ij}} x_t$$ 重复 2, 3 直到收敛. 有可能会使得某些离分类边界很近的点被错误分类. 引入学习向量量化方法 (LVQ 算法). 让同类和异类的点在构建过程中都能起作用.\n定义\nLVQ 算法 的基本步骤如下:\n对每个类 $c_m$ 随机选择 $k$ 个点 $I_{mi}$ 作为代表; 对每个样本点 $x_t$, 找到最近的代表元 $I_{m^\\ast i^\\ast}$, 即 $$I_{m^\\ast i^\\ast} = \\arg\\min_{m,i} \\Vert x_t - I_{mi} \\Vert_2^2$$ 如果 $y_t=c_{m^\\ast}$, 则 $$I_{m^\\ast i^\\ast} \\gets I_{m^\\ast i^\\ast} + \\eta(x_t - I_{m^\\ast i^\\ast})$$ 否则 $$I_{m^\\ast i^\\ast} \\gets I_{m^\\ast i^\\ast} - \\eta(x_t - I_{m^\\ast i^\\ast})$$ 重复 2, 3 直到收敛. 这里 $\\eta$ 是学习率.\n在 $\\eta=1$ 时, LVQ 算法相当于逐步地进行 k-means 算法.\n在最近邻算法和其扩展方法中, 每个簇的代表点也称为相应单元的原型. 这种方法也常被称作原型方法或免模型方法.\n","date":"2025-03-14T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/knn/","title":"机器学习基础(4) —— 基于近邻的分类方法"},{"content":"后验概率最大化准则 定义\n对训练样本集 $D=\\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathcal{X}$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_K\\}$, 将 $x$ 的类别预测为 $c_i$ 的 风险 为\n$$ R(Y=c_i | x) = \\sum_{j=1}^K \\lambda_{ij} P(Y=c_j | x) $$其中 $\\lambda_{ij}$ 是将属于 $c_j$ 的样本预测为 $c_i$ 的损失. 最优预测 $\\hat{y}$ 是使得风险最小的类别, 即\n$$ \\hat{y} = \\arg\\min_{c_i} R(Y=c_i | x) $$ 假设采用 $0-1$ 损失函数, 易知\n$$ R(Y=c_i | x) = 1 - P(Y=c_i | x) $$即输入 $x$ 的最优预测 $\\hat{y}$ 为使得后验概率 $P(y | x)$ 最大的类别.\n逻辑斯蒂回归模型 定义\n设 $\\mathcal{X}=\\mathbb{R}^n, \\mathcal{Y}=\\{c_1,c_2\\}$, 逻辑斯蒂回归模型是如下的后验概率分布:\n$$ \\begin{aligned} P(Y=c_1 | x) \u0026= \\frac{\\exp(w \\cdot x + b)}{1+\\exp(w \\cdot x + b)} \\\\ P(Y=c_2 | x) \u0026= \\frac{1}{1+\\exp(w \\cdot x + b) } \\end{aligned} $$其中 $w,b$ 是模型参数.\n按照后验概率最大化准则, 显然当 $w \\cdot x + b \u003e 0$ 时, 预测为 $c_1$, 否则预测为 $c_2$.\n对于多类分类任务, 仍然可以使用逻辑斯蒂回归模型:\n$$ \\begin{aligned} p(y=c_i | x) \u0026= \\frac{\\exp(w_i \\cdot x + b_i)}{\\sum_{j=1}^{K-1} \\exp(w_j \\cdot x + b_j)}, \\quad i=1,2,\\cdots,K-1 \\\\ p(y=c_K | x) \u0026= \\frac{1}{\\sum_{j=1}^{K-1} \\exp(w_j \\cdot x + b_j)} \\end{aligned} $$给定 $D=\\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{0,1\\}$, 用 $\\theta=(w,b)$ 表示二项逻辑斯蒂回归模型的参数, 令\n$$ p(x;\\theta) = p(Y=1 | x;\\theta) $$则考虑似然函数为\n$$ \\begin{aligned} L(\\theta) \u0026= \\prod_{i=1}^n p(x_i;\\theta)^{y_i} (1-p(x_i;\\theta))^{1-y_i} \\\\ \\log L(\\theta) \u0026= \\sum_{i=1}^n y_i \\log p(x_i;\\theta) + (1-y_i) \\log (1-p(x_i;\\theta)) \\\\ \u0026= \\sum_{i=1}^N y_i(w \\cdot x_i + b) - \\log(1+\\exp(w \\cdot x_i + b)) \\end{aligned} $$对 $w,b$ 求偏导为 $0$, 得到\n$$ \\begin{aligned} \\frac{\\partial \\log L(\\theta)}{\\partial w} \u0026= \\sum_{i=1}^n x_i(y_i - p(x_i;\\theta)) = 0\\\\ \\frac{\\partial \\log L(\\theta)}{\\partial b} \u0026= \\sum_{i=1}^n (y_i - p(x_i;\\theta)) = 0 \\end{aligned} $$朴素 Bayers 分类器 定理Bayers 公式\n$$ \\begin{aligned} P(Y=c_i | x) \u0026= \\frac{P(x | Y=c_i) P(Y=c_i)}{P(x)} \\\\ \u0026= \\frac{P(x | Y=c_i) P(Y=c_i)}{\\sum_{j=1}^K P(x | Y=c_j) P(Y=c_j)} \\end{aligned} $$ 朴素 Bayers 假定特征之间相互独立, 即\n$$ p(X^1=x^1, X^2=x^2, \\cdots, X^n=x^n | Y=c_k) = \\prod_{j=1}^n p(X^j=x^j | Y=c_k) $$对于输入实例 $x=(x^1,x^2,\\cdots,x^n)$, 则后验概率\n$$ p(Y=c_k|x)=\\frac{\\left( \\prod_{i=1}^n p(X^i=x^i | Y=c_k) \\right) P(Y=c_k)}{\\sum_{j=1}^K \\left( \\prod_{i=1}^n p(X^i=x^i | Y=c_j) \\right) P(Y=c_j)} $$分母是固定的, 只需比较分子的大小即可. 但是一旦某个特征取值和分类没有同时出现, 后验概率直接为 $0$, 为了避免这种情况, 通常引入一些平滑技术:\n$$ p_{\\lambda}(Y=c_k) = \\frac{\\sum_{j=1}^NI(y_j=c_k)+\\lambda}{N+K\\lambda} $$$\\lambda=1$ 时称为 Laplace 平滑.\n","date":"2025-03-11T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/bayers/","title":"机器学习基础(3) —— 基于后验概率最大化准则的分类模型"},{"content":"虚函数 在类的定义中, 前面有 virtual 关键字的成员函数就是 虚函数. virtual 只用在类定义里的函数声明中, 写函数体时不用.\n派生类的指针可以赋给基类指针. 通过基类指针调用基类和派生类中的同名同参 虚 函数时:\n若该指针指向一个基类的对象, 那么被调用是基类的虚函数; 若该指针指向一个派生类的对象, 那么被调用的是派生类的虚函数. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class CBase { public: virtual void someVirtualFunction() { cout \u0026lt;\u0026lt; \u0026#34;base function\u0026#34; \u0026lt;\u0026lt; endl; } }; class CDerived : public CBase { public: virtual void someVirtualFunction() { cout \u0026lt;\u0026lt; \u0026#34;derived function\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { CDerived ODerived; CBase *p = \u0026amp;ODerived; p-\u0026gt;someVirtualFunction(); // derived function return 0; } 派生类的对象可以赋给基类引用. 类似于指针, 通过基类引用调用基类和派生类中的同名同参虚函数也是多态的.\n1 2 3 4 5 6 int main() { CDerived ODerived; CBase \u0026amp;r = ODerived; r.someVirtualFunction(); // derived function return 0; } 多态不能针对对象. 派生类中的虚函数的访问权限可以是 public, protected, private. 但是, 基类中的虚函数的访问权限不能是 private, 即使派生类中的虚函数的访问权限是 public. 反过来是可以的, 而且可以正常多态.\n实现原理 采用了动态联编的技巧. 每一个有虚函数的类 (或其派生类) 都有一个虚函数表，该类的任何对象中都放着虚函数表的指针. 虚函数表中列出了该类的虚函数地址. 多出来的 4 个字节就是用来放虚函数表的地址的. 在编译时, 调用语句被编译成对虚函数表的索引, 而不是对函数的直接调用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Base { public: int i; virtual void Print() { cout \u0026lt;\u0026lt; \u0026#34;Base:Print\u0026#34; \u0026lt;\u0026lt; endl; } }; class Derived : public Base { public: int n; virtual void Print() { cout \u0026lt;\u0026lt; \u0026#34;Drived:Print\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { Derived d; cout \u0026lt;\u0026lt; sizeof(Base) \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; sizeof(Derived); return 0; } 32 位系统下, 这个程序的输出是 8,12.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class A { public: virtual void func() { cout \u0026lt;\u0026lt; \u0026#34;A::func \u0026#34;; } }; class B : public A { public: virtual void func() { cout \u0026lt;\u0026lt; \u0026#34;B::func \u0026#34;; } }; int main() { A a; A *pa = new B(); pa-\u0026gt;func(); // 多态, B::func // 64 位程序 long long *p1 = (long long *)\u0026amp;a; long long *p2 = (long long *)pa; // 篡改了虚函数表指向 *p2 = *p1; pa-\u0026gt;func(); // A::func return 0; } 虚析构函数 在非构造/析构函数中调用虚函数时, 调用的是当前对象的虚函数, 而不是基类的虚函数, 是多态. 在构造/析构函数中调用虚函数时, 调用的是当前类的虚函数, 编译时确定, 不是多态.\n通过基类的指针删除派生类对象时，通常情况下只调用基类的析构函数. 为了解决这个问题, 可以把基类的析构函数声明为虚函数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class son { public: virtual ~son() { cout \u0026lt;\u0026lt; \u0026#34;bye from son\u0026#34; \u0026lt;\u0026lt; endl;} }; class grandson : public son { public: ~grandson() { cout \u0026lt;\u0026lt; \u0026#34;bye from grandson\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { son *pson; pson = new grandson(); delete pson; return 0; } 此时, 会先调用派生类的析构函数, 再调用基类的析构函数. 另外注意, 构造函数不能是虚函数.\n纯虚函数和抽象类 如果在虚函数后面加上 = 0, 则该虚函数是纯虚函数. 纯虚函数不可以有函数体, 只有声明.\n1 2 3 4 5 class A { public: virtual void print() = 0; // 纯虚函数 void fun() { cout \u0026lt;\u0026lt; \u0026#34;fun\u0026#34;; } } 一个类中有纯虚函数的类叫 抽象类. 抽象类不能实例化, 只能作为基类, 不过可以作为指针或引用类型. 在抽象类的成员函数内可以调用纯虚函数, 但是在构造函数或析构函数内部不能调用纯虚函数.\n","date":"2025-03-07T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/polymorphism/","title":"程序设计实习(4) —— 多态"},{"content":"基本概念 在定义一个新的类 B 时, 如果 B 类与拥有已有类 A 的全部特点, 那么就可以把 A 作为一个基类, 而把 B 作为基类的一个 派生类 (也称 子类). 派生类可以对基类:\n扩充: 在派生类中, 可以添加新的成员变量和成员函数 修改: 在派生类中, 可以重新编写从基类继承得到的成员 派生类一经定义后, 可以独立使用, 不依赖于基类.\n派生方式说明符：public, private, protected. 派生类的写法:\n1 2 3 class Derived : public Base { // code }; 关于内存上, 派生类对象的大小等于基类对象的大小加上派生类对象自己的成员变量的大小. 在派生类对象中包含着基类对象, 且基类对象的存储位置位于派生类对象新增的成员变量之前.\n覆盖 派生类可以定义一个和基类成员同名的成员, 这叫覆盖. 在派生类中访问这类成员时, 缺省的情况是访问派生类中定义的成员. 要在派生类中访问由基类定义的同名成员时, 要使用作用域符号::.\n注意, 对于成员变量, 在内存中, 两个变量占用不同的空间. 并不建议覆盖成员变量.\n关于权限及派生方式的访问权限, 可以参考下表:\n派生方式\\成员访问 public protected private public public protected 不可访问 protected protected protected 不可访问 private private private 不可访问 为此, 可以用基类构造函数初始化派生类对象的基类部分. 例如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Bug { private: int nLegs; int nColor; protected: int nType; public: Bug(int legs, int color); void PrintBug() {} }; class FlyBug : public Bug { int nWings; public: FlyBug(int legs, int color, int wings); }; Bug::Bug(int legs, int color) { nLegs = legs; nColor = color; } FlyBug::FlyBug(int legs, int color, int wings) : Bug(legs, color) { nType = 1; nWings = wings; } 派生类的对象生命周期 在创建派生类的对象时, 需要调用基类的构造函数：初始化派生类对象中从基类继承的成员. 在执行一个派生类的构造函数之前, 总是先执行基类的构造函数. 派生类的析构函数被执行时, 执行完派生类的析构函数后, 自动调用基类的析构函数.\n在创建派生类的对象时:\n先执行基类的构造函数, 用以初始化派生类对象中从基类继承的成员; 再执行成员对象类的构造函数, 用以初始化派生类对象中成员对象; 最后执行派生类自己的构造函数. 在派生类对象消亡时：\n先执行派生类自己的析构函数; 再依次执行各成员对象类的析构函数; 最后执行基类的析构函数. 总之, 析构函数的调用顺序与构造函数的调用顺序相反.\n对于 public 继承具有赋值兼容规则, 即\n1 2 3 4 class Base {}; class Derived : public Base {}; Base b; Derived d; 派生类对象可以赋值给基类对象\n1 b = d; 派生类对象的地址可以赋值给基类指针\n1 Base *pb = \u0026amp;d; 派生类对象的引用可以赋值给基类引用\n1 Base \u0026amp;rb = d; 注意, 只有 public 继承才具有赋值兼容规则, protected 和 private 不允许这种赋值方式.\n指针强转成基类后, 就不再能访问派生类的成员了. 如果需要, 可以再利用指针强转回来.\n1 2 3 Derived objDerived; Base *ptrBase = \u0026amp;objDerived; Derived *ptrDerived = (Derived *)ptrBase; 在声明派生类时，只需要列出它的直接基类, 派生类沿着类的层次自动向上继承它的间接基类. 构造的顺序即为派生类的继承顺序, 析构的顺序则相反.\n","date":"2025-03-05T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/inherit/","title":"程序设计实习(3) —— 继承与派生"},{"content":"线性可分支持向量机 定义\n对于一个数据集 $D$, 如果能找到一个超平面 $H: w^Tx + b = 0$, 将数据分为两类. 即对任意 $(x_i, y_i) \\in D$, 若 $y_i = 1$, 则 $w^Tx_i + b \\geq 0$; 若 $y_i = -1$, 则 $w^Tx_i + b \u003c 0$. 则称 $D$ 是 线性可分的 , 超平面 $H$ 是 $D$ 的一个 分离超平面.\n最优超平面不仅要能够将数据分开, 还要使得两类数据点到超平面的距离尽可能远.\n考虑到 $w,b$ 任意缩放都不影响超平面的位置, 我们可以规定 $w^Tx + b = 1$ 为最近的正类数据点满足的方程. 此时距离为 $1/{\\|w\\|}$, 要最大化这个量, 即化归成凸二次规划问题:\n$$ \\begin{aligned} \u0026 \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1, \\quad i = 1, 2, \\cdots, n \\end{aligned} $$只要 $D$ 是线性可分的, 上述问题一定有解且唯一. 对应的分类决策函数\n$$ f(x) = \\text{sign}(w^Tx + b) $$称为 线性可分支持向量机.\n引入 Lagrange 乘子 $\\alpha_i \\geq 0$:\n$$ L(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b) - 1) $$对 $w, b$ 求偏导为 $0$, 得到\n$$ \\begin{aligned} \u0026 w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\ \u0026 0 = \\sum_{i=1}^n \\alpha_i y_i \\end{aligned} $$代入 $L(w, b, \\alpha)$, 得到对偶问题:\n线性可分对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j \\\\ \u0026 \\text{s.t.} \\quad \\alpha_i \\geq 0, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 由 KKT 条件, 最优解一定满足\n$$ \\begin{aligned} \\alpha_i(y_i(w \\cdot x_i + b) - 1) \u0026= 0 \\\\ y_i(w \\cdot x_i + b) - 1 \u0026\\geq 0 \\\\ \\alpha_i \u0026\\geq 0 \\\\ \\end{aligned} $$由于 $\\alpha_i$ 不全为 $0$, 存在 $j$ 使得 $y_j(w \\cdot x_j + b) = 1$, 由此\n$$ b = y_j - w \\cdot x_j = y_j - \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x_j $$乘上 $\\alpha_jy_j$ 做累和, 有\n$$ 0=\\sum_{j=1}^n \\alpha_jy_jb = \\sum_{j=1}^n \\alpha_j - \\| w \\|^2 $$上式中 $\\alpha_i=0$ 的 $i$ 也成立, 因为都是 $0$ 不影响结果. 注意到 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$ 也只收到 $\\alpha_i \u003e 0$ 的影响, 而这些项的点都落在间隔边界\n$$ H_1: w \\cdot x + b = 1, \\quad H_2: w \\cdot x + b = -1 $$上, 称这些点 $x_i$ 为 支持向量.\n支持向量机的留一误差\n$$ \\hat{R}_{\\text{loo}} = \\frac{1}{n} \\sum_{i=1}^n I(f_{D-\\{x_i\\}}(x_i) \\neq y_i) $$则 $\\hat{R}_{\\text{loo}} \\le N_{SV}/n$, 其中 $N_{SV}$ 为支持向量的个数.\n线性支持向量机 要求 $D$ 线性可分有点苛刻. 容忍一些误差, 引入松弛变量 $\\xi_i \\geq 0$, 使得约束条件变为\n$$ y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i $$对于被错误分类的点, $\\xi_i$ 可以大于 $1$. 把 $\\xi_i \\ne 0$ 的点视为特异点, 那么希望特异点尽可能少, 于是优化目标变为\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n I(\\xi_i \\ne 0) \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$直接用 $\\xi_i$ 代替 $I(\\xi_i \\ne 0)$, 问题变为\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$既然要 $\\xi_i$ 尽可能小, 不妨取 $\\xi_i = 1 - y_i(w \\cdot x_i + b)$, 引入合页损失函数 $h(z) = \\max(0, 1-z)$, 即\n$$\\xi_i = h(y_i(w \\cdot x_i + b))$$则提出一个 $C$ 后, 优化目标变为\n$$ \\min_{w, b} \\frac{1}{2C} \\|w\\|^2 + \\sum_{i=1}^n h(y_i(w \\cdot x_i + b)) $$做了这么多, 只是相当于把 0-1 损失函数换成了合页损失函数.\n回到原问题, 引入 Lagrange 乘子 $\\alpha_i, \\beta_i \\geq 0$, 得到\n$$ L(w, b, \\xi, \\alpha, \\beta) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b) - 1 + \\xi_i) - \\sum_{i=1}^n \\beta_i \\xi_i $$对 $w, b, \\xi$ 偏导为 $0$, 得到\n$$ \\begin{aligned} \u0026 w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\ \u0026 0 = \\sum_{i=1}^n \\alpha_i y_i \\\\ \u0026 \\beta_i = C - \\alpha_i \\end{aligned} $$代入 $L(w, b, \\xi, \\alpha, \\beta)$, 得到对偶问题\n线性支持向量机对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 与线性可分支持向量机类似, 只是多了一个 $\\alpha_i \\leq C$ 的约束. 现在考虑 KKT 条件, 有\n$$ \\begin{aligned} \\alpha_i(y_i(w \\cdot x_i + b) - 1 + \\xi_i) \u0026= 0 \\\\ y_i(w \\cdot x_i + b) - 1 + \\xi_i \u0026\\geq 0 \\\\ \\beta_i \\xi_i \u0026= 0 \\\\ \\alpha_i \u0026\\geq 0 \\\\ \\beta_i \u0026\\geq 0 \\\\ \\alpha_i + \\beta_i\u0026=C \\end{aligned} $$则 $\\alpha_i \u003e 0$ 的点 $x_i$ 为支持向量, 满足 $y_i(w \\cdot x_i + b) = 1 - \\xi_i$. 这点与线性可分支持向量机的支持向量不同. 但进一步如果 $\\alpha_i \\lt C$ , 则 $\\beta_i \\gt 0$, 则 $\\xi_i=0$, 从而 $y_i(w \\cdot x_i + b) = 1$, 这样就一致了.\n进一步, 把 $y_i(w \\cdot x_i + b) = 1$ 两边乘 $y_i$, 类似有\n$$ b = y_j - \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x_j $$因而最优分类超平面为\n$$ \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x + b = 0 $$和决策函数\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x + b\\right) $$超平面法向量可以被唯一确定, 但是偏置不唯一.\nSMO 算法 SMO 算法是一种启发式算法, 用于求解支持向量机的对偶问题. SMO 算法的基本思想是: 每次选择两个变量, 固定其他变量, 优化这两个变量. 这样不断迭代, 直到收敛.\n设当前迭代的两个变量为 $\\alpha_i, \\alpha_j$, 则\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = -\\sum_{i=3}^n \\alpha_i y_i $$同乘 $y_1$, 有\n$$ \\alpha_1 + \\alpha_2 y_1y_2= -\\sum_{i=3}^n \\alpha_i y_1y_i $$记右边为 $\\gamma$, $s=y_1y_2 \\in \\{-1, 1\\}$, 则\n$$ \\alpha_1 + s\\alpha_2 = \\gamma $$记$K_{ij} = x_i \\cdot x_j$, $v_i = \\sum_{j=3}^{N} \\alpha_j y_j K_{ij}$, 则对偶问题转化为\n$$ \\begin{aligned} \u0026 \\max_{\\alpha_1, \\alpha_2} \\alpha_1 + \\alpha_2 - \\frac{1}{2} K_{11}\\alpha_1^2 - \\frac{1}{2} K_{22}\\alpha_2^2 - sK_{12}\\alpha_1\\alpha_2 - y_1v_1\\alpha_1 - y_2v_2\\alpha_2 \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\alpha_1 + s\\alpha_2 = \\gamma \\end{aligned} $$再由 $\\alpha_1 = \\gamma - s\\alpha_2$, 代入目标函数, 并对 $\\alpha_2$ 求导为 $0$, 得到\n$$ \\alpha_2 = \\frac{s(K_{11}-K_{12})\\gamma + y_2(v_1 - v_2) - s + 1}{K_{11} + K_{22} - 2K_{12}} $$代入 $v$ 的定义, 随后化简得\n$$ \\alpha_2 = \\alpha_2^* + y_2 \\frac{(y_2 - f(x_2))- (y_1-f(x_1))}{K_{11} + K_{22} - 2K_{12}} $$别忘了约束 $0 \\le \\alpha_1, \\alpha_2 \\le C$, 以及 $\\alpha_1 + s\\alpha_2 = \\gamma$, 对 $\\alpha_2$ 进行裁剪为 $\\alpha_2^{\\text{clip}}$. 相应地,\n$$ \\alpha_1 = \\alpha_1^* + s(\\alpha_2^* - \\alpha_2^{\\text{clip}}) $$最后, 更新 $b$. 假设在 $\\alpha_1, \\alpha_2$ 中, $0 \\lt \\alpha_i \\lt C$, 则\n$$ b = y_i - \\sum_{j=1}^n \\alpha_j y_j K_{ij} $$关于选取 $\\alpha_1, \\alpha_2$, 一般有两个原则:\n选择违反 KKT 条件最严重的两个变量. 选择两个变量使得目标函数有最大变化. 核方法和非线性支持向量机 对于非线性问题, 可以通过核方法将数据映射到高维空间, 从而在高维空间中找到一个线性超平面.\n假设有一个映射 $\\phi: \\mathcal{X} \\mapsto \\mathcal{Z}$, 则在 $\\mathcal{Z}$ 的线性支持向量机变为:\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$对应的对偶问题为\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\phi(x_i) \\cdot \\phi(x_j) \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$相应的分类决策函数为\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) \\cdot \\phi(x) + b\\right) $$然而, 直接计算 $\\phi(x_i) \\cdot \\phi(x_j)$ 的复杂度很高. 为此, 引入核函数\n定义\n设 $\\mathcal{X}$ 是输入空间, $\\mathcal{Z}$ 是特征空间, 如果存在一个从 $\\mathcal{X}$ 到 $\\mathcal{Z}$ 的映射 $\\phi$, 使得对任意 $x, x' \\in \\mathcal{X}$, 都有\n$$ K(x, x') = \\phi(x) \\cdot \\phi(x') $$则称 $K$ 为 核函数.\n注意, 这里我们不再需要显式地计算 $\\phi(x_i)$, 因为结果只与 $K(x_i, x_j)$ 有关.\n非线性支持向量机对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 此时, 分类决策函数为\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\\right) $$ 定义\n$\\mathcal{X}$ 上的函数 $K: \\mathcal{X} \\times \\mathcal{X} \\mapsto \\mathbb{R}$ 称为 正定对称核函数, 如果对任意 $x_1, x_2, \\cdots, x_n \\in \\mathcal{X}$, 核矩阵 (Gram 矩阵) $[K_{ij}]_{m \\times m}$ 是半正定的.\n常见的核函数有:\n线性核函数: $K(x, x') = x \\cdot x'$, 对应线性支持向量机. 多项式核函数: $K(x, x') = (x \\cdot x' + 1)^d, c \\gt 0$ 高斯核函数: $K(x, x') = \\exp\\left(-\\frac{\\|x-x'\\|^2}{2\\sigma^2}\\right), \\sigma \\gt 0$ ","date":"2025-02-28T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/","title":"机器学习基础(2) —— 支持向量机"},{"content":"对象间的运算和结构变量一样, 对象之间可以用 = 进行赋值, 但是不能用 ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;= 进行比较, 除非这些运算符经过了 \u0026ldquo;重载\u0026rdquo;. 运算符重载的实质是函数重载.\n重载为普通函数和成员函数均可. 重载为成员函数时, 重载函数的参数个数比运算符的操作数少一个.\n1 2 3 4 5 6 7 8 9 10 11 12 class Complex { public: double real, imag; Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} Complex operator-(const Complex \u0026amp;c); }; Complex operator+(const Complex \u0026amp;a, const Complex \u0026amp;b) { return Complex(a.real + b.real, a.imag + b.imag); } Complex Complex::operator-(const Complex \u0026amp;c) { return Complex(real - c.real, imag - c.imag); }; 重载 = 有时候希望赋值运算符两边的类型可以不匹配, 比如把一个 char * 类型的字符串赋值给一个字符串对象, 此时就需要重载赋值运算符 =. 赋值运算符 = 只能重载为成员函数, 通常返回类型为 *this 的引用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class String { char *str; public: String() : str(new char[1]) { str[0] = 0; } const char *c_str() { return str; } String \u0026amp;operator=(const char *s); ~String() { delete[] str; } }; String \u0026amp;String::operator=(const char*s) { delete[] str; str = new char[strlen(s) + 1]; strcpy(str, s); return *this; } int main() { String s; s = \u0026#34;Good Luck,\u0026#34;; // s.operator=(\u0026#34;Good Luck,\u0026#34;); cout \u0026lt;\u0026lt; s.c_str() \u0026lt;\u0026lt; endl; // String s2 = \u0026#34;hello!\u0026#34;; // 这是构造函数, 不是赋值 return 0; } 如不定义自己的赋值运算符, 那么 S1 = S2 实际上导致 S1.str 和 S2.str 指向同一地方. 为此要做深拷贝.\n1 2 3 4 5 6 7 8 9 10 11 12 13 String \u0026amp;operator=(const String \u0026amp;s) { // 防止自赋值导致问题 if (this == \u0026amp;s) return *this; delete[] str; str = new char[strlen(s.str) + 1]; strcpy(str, s.str); return *this } String(const String \u0026amp;s) { // 复制构造函数也要深拷贝 str = new char[strlen(s.str) + 1]; strcpy(str, s.str); } 也可以重载为友元函数.\n1 2 3 4 5 6 7 8 9 class Complex{ double real, imag; public: Complex(double r, double i) : real(r), imag(i) {} Complex operator+(double r); }; Complex Complex::operator+(double r) { return Complex(real + r, imag); } 这个可以解决 c = a + 2.5 的问题, 但是不能解决 c = 2.5 + a 的问题. 为此要重载为普通函数, 但普通函数又不能访问私有成员, 用友元函数.\n1 2 3 4 5 6 7 8 Complex operator+(double r, const Complex \u0026amp;c); class Complex { // --skip-- friend Complex operator+(double r, const Complex \u0026amp;c); }; Complex operator+(double r, const Complex \u0026amp;c) { return Complex(c.real + r, c.imag); } 重载流插入运算符 \u0026lt;\u0026lt; 和流提取运算符 \u0026gt;\u0026gt; cout 实际上是一个在 iostream 中定义的, ostream 类的对象.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Complex { double real, imag; public: Complex(double r = 0, double i = 0) : real(r), imag(i) {} friend ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Complex \u0026amp;c); friend istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;is, Complex \u0026amp;c); }; ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Complex \u0026amp;c) { os \u0026lt;\u0026lt; c.real \u0026lt;\u0026lt; \u0026#34;+\u0026#34; \u0026lt;\u0026lt; c.imag \u0026lt;\u0026lt; \u0026#34;i\u0026#34;; // 以\u0026#34;a+bi\u0026#34; 的形式输出 return os; } istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;is, Complex \u0026amp;c) { string s; is \u0026gt;\u0026gt; s; // 将 \u0026#34;a+bi\u0026#34; 作为字符串读入 int pos = s.find(\u0026#34;+\u0026#34;, 0); string sTmp = s.substr(0, pos); // 分离出代表实部的字符串 c.real = atof(sTmp.c_str()); // atof 库函数能将 const char* 指针指向的内容转换成 float sTmp = s.substr(pos + 1, s.length() - pos - 2); // 分离出代表虚部的字符串 c.imag = atof(sTmp.c_str()); return is; } 重载类型转换运算符 没有返回值, 因为转换函数的返回值类型就是要转换的类型.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { double real, imag; public: Complex(double r = 0, double i = 0) : real(r), imag(i) {} operator double() { return real; } }; int main() { Complex c(1.2, 3.4); cout \u0026lt;\u0026lt; (double)c \u0026lt;\u0026lt; endl; // 输出 1.2 double n = 2 + c; // double n = 2 + c.operator double() cout \u0026lt;\u0026lt; n; // 输出 3.2 return 0; } 重载自增自减运算符 ++ 和 -- 自增运算符 ++, 自减运算符 -- 有前置/后置之分, 为了区分所重载的是前置运算符还是后置运算符, C++ 规定前置运算符作为一元运算符重载, 后置运算符作为二元运算符重载, 多写一个没用的参数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 前置, 先加减再返回, 返回引用 // 重载为成员函数 T\u0026amp; operator++(); T\u0026amp; operator--(); // 重载为全局函数 T\u0026amp; operator++(T\u0026amp;); T\u0026amp; operator--(T\u0026amp;) // 后置, 先返回再加减, 返回值 // 重载为成员函数 T operator++(int); T operator--(int); // 重载为全局函数 T operator++(T\u0026amp;, int); T operator--(T\u0026amp;, int); 重载 -\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class A { private: int x; public: A() : x(5) {} int getX() { return x; } A *operator-\u0026gt;() { return this; } }; int main() { A a; cout \u0026lt;\u0026lt; a-\u0026gt;getX() \u0026lt;\u0026lt; endl; // a.operator-\u0026gt;()-\u0026gt;getX() return 0; } 看起来似乎没有什么用, 但是可以用来实现智能指针.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class client { public: int a; client(int x) : a(x) {} }; class proxy { client *target; public: proxy(client *t) : target(t) {} client *operator-\u0026gt;() const { return target; } }; class proxy2 { proxy *target; public: proxy2(proxy *t) : target(t) {} proxy \u0026amp;operator-\u0026gt;() const { return *target; } }; int main() { client x(3); proxy y(\u0026amp;x); proxy2 z(\u0026amp;y); cout \u0026lt;\u0026lt; x.a \u0026lt;\u0026lt; y-\u0026gt;a \u0026lt;\u0026lt; z-\u0026gt;a; // print \u0026#34;333\u0026#34; return 0; } 注意事项 运算符重载不改变运算符的优先级.\n以下运算符不能重载: ., .*(成员函数指针), ::, ?:(三目运算符), sizeof.\n重载运算符 (), [], -\u0026gt; 或者赋值运算符 = 时，运算符重载函数必须声明为类的成员函数.\n重载运算符是为了让它能作用于对象, 因此重载运算符不允许操作数都不是对象 (有一个是枚举类型也可以).\n1 void operator+(int a, char* b); // 错误 ","date":"2025-02-26T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/operator-overload/","title":"程序设计实习(2) —— 运算符重载"},{"content":"面向对象的程序设计 面向对象的程序设计方法:\n将某类客观事物共同特点 (属性) 归纳出来, 形成一个数据结构 (可以用多个变量描述事物的属性); 将这类事物所能进行的行为也归纳出来, 形成一个个函数, 这些函数可以用来操作数据结构. 面向对象的特点有 抽象, 封装, 继承, 多态.\n一般来说, 对象所占用的内存空间的大小, 等于所有成员变量的大小之和.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class CRectangle { public: int w, h; int Area() { return w * h; } int Perimeter() { return 2 * (w + h); } void Init(int w_, int h_) { w = w_; h = h_; } }; // 必须有分号 int main() { int w, h; CRectangle r; // r 是一个对象 cin \u0026gt;\u0026gt; w \u0026gt;\u0026gt; h; r.Init(w, h); cout \u0026lt;\u0026lt; r.Area() \u0026lt;\u0026lt; endl \u0026lt;\u0026lt; r.Perimeter(); return 0; } 和结构变量一样, 对象之间可以用 = 进行赋值, 但是不能用 ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;= 进行比较, 除非这些运算符经过了 重载.\n对象名.成员名 1 2 3 CRectangle r1, r2; r1.w = 5; r2.Init(5, 4); 指针-\u0026gt;成员名 1 2 3 4 5 CRectangle r1, r2; CRectangle *p1 = \u0026amp;r1; CRectangle *p2 = \u0026amp;r2; p1-\u0026gt;w = 5; p2-\u0026gt;Init(5, 4); 引用名.成员名 1 2 3 CRectangle r1; CRectangle \u0026amp;rr = r1; rr.w = 5; 引用 引用名是对象名的别名, 指向同一个对象. 语法: 类名 \u0026amp;引用名 = 对象名; 引用的好处是可以减少指针的使用, 使得代码更加简洁.\n1 2 3 4 5 6 7 8 9 // 要用指针, 否则参数传递会产生拷贝 void swap(int *a, int *b) { int tmp; tmp = *a; *a = *b; *b = tmp; } int n1, n2; swap(\u0026amp;n1, \u0026amp;n2); 可以改写为:\n1 2 3 4 5 6 7 8 void swap(int \u0026amp;a, int \u0026amp;b) { int tmp; tmp = a; a = b; b = tmp; } int n1, n2; swap(n1, n2); 引用还可以作为函数的返回值.\n1 2 3 4 5 6 int \u0026amp;ref(int \u0026amp;a) { return a; } int n = 5; ref(n) = 6; // n = 6 常量, 常引用, 常量指针 定义引用时, 前面加 const 关键字, 表示 常引用. 不能通过常引用修改对象的值, 即只读引用.\n1 2 3 4 int n; const int \u0026amp;r = n; r = 5; // 错误 n = 5; // 正确 const T\u0026amp; 和 T\u0026amp; 是不同的类型. T\u0026amp; 类型的引用或 T 类型的变量可以用来初始化 const T\u0026amp; 类型的引用. const T 类型的常变量和 const T\u0026amp; 类型的引用则不能用来初始化 T\u0026amp; 类型的引用, 除非进行强制类型转换. 不可通过常量指针修改其指向的内容, 但可以修改指针的指向 (引用不可以).\n1 2 3 4 5 int n, m; const int *p = \u0026amp;n; *p = 5; // 错误 n = 5; // 正确 p = \u0026amp;m; // 正确 函数参数为常量指针时, 可避免函数内部不小心改变参数指针所指地方的内.\n1 2 3 4 void myPrintf(const int *p) { *p = 5; // 错误 p = \u0026amp;m; // 正确 } 类成员的访问控制 public: 公有成员, 可以在类的外部访问. private: 私有成员, 只能在类的内部访问, 缺省默认为 private. protected: 保护成员, 只能在类的内部和派生类中访问. 1 2 3 4 5 6 7 8 9 class className { // 这三个关键字可以出现多次, 没有顺序要求 private: // 私有属性和函数 public: // 公有属性和函数 protected: // 保护属性和函数 }; 类内部可以访问当前对象和同类其他对象的私有成员.\n\u0026ldquo;隐藏\u0026rdquo; 的目的是强制对成员变量的访问一定要通过成员函数进行, 那么以后成员变量的类 型等属性修改后, 只需要更改成员函数即可.\nstruct 和 class 的唯一区别是默认的访问控制权限不同, struct 默认为 public, class 默认为 private.\n函数重载和缺省参数 函数名相同, 参数个数或类型不同, 注意没有返回值类型不同.\n1 2 3 double _max(double f1, double f2); int _max(int n1, int n2); int _max(int n1, int n2, int n3); C++ 中, 定义函数的时候可以让 最右边 的连续若干个参数有缺省值, 那么调用函数的时候, 若相应位置不写参数, 参数就是缺省值.\n1 2 3 4 void func(int a, int b = 0, int c = 0); func(1); // a = 1, b = 0, c = 0 func(1, 2); // a = 1, b = 2, c = 0 func(1, , 8); // 错误 成员函数也可以重载或有缺省参数.\n构造函数 构造函数是一种特殊的成员函数: 名字与类名相同, 没有返回值 (void 也不行). 作用是初始化对象的数据成员.\n如果定义类时没有定义构造函数, 编译器会生成一个默认的无参构造函数. 如果定义了, 默认构造函数就不会生成. 对象生成时, 构造函数自动调用. 生成之后不能再执行构造函数. 一个类可以有多个构造函数, 可以重载.\n1 2 3 4 5 6 7 8 9 10 11 class Complex { private: double real; double imag; public: void Set(double r, double i); }; //编译器自动生成默认构造函数 // 两种写法均可. Complex c1; Complex *pc = new Complex; 手动加入构造函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Complex { private: double real; double imag; public: Complex(double r, double i = 0) { // 带缺省参数 real = r; imag = i; } }; Complex *pc1 = new Complex; // error, 没有参数 Complex c2(2); // OK Complex *pc2 = new Complex(3, 4); 构造当然也可以 private, 这样就不能用来生成对象, 但是可以用来实现单例模式.\n构造函数还可以用在数组.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class CSample { int x; public: CSample() {} CSample(int n) { x = n; } CSample(int m, int n) { x = m + n; } }; int main() { CSample array1[2]; // 两次无参 CSample array2[2] = {4, CSample(5, 3)}; // 两次有参 CSample array3[2] = {3}; // 一次有参一次无参 CSample *array4 = new CSample[2]; delete[] array4; return 0; } 复制构造函数只有一个参数, 且参数是本类的引用 (或常量引用). 如果没有定义, 编译器会生成一个默认的复制构造函数. 如果定义了, 默认复制构造函数就不会生成.\n1 2 3 4 5 6 7 8 class Complex { private: double real; double imag; }; Complex c1; Complex c2(c1); // 默认复制构造函数 1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real; double imag; Complex() {} // 必须要写, 否则编译器不会生成默认构造函数 Complex(const Complex \u0026amp;c) { real = c.real; imag = c.imag; cout \u0026lt;\u0026lt; \u0026#34;Copy Constructor called\u0026#34;; } }; Complex c1; Complex c2(c1); // 自定义复制构造函数 复制构造函数的调用时机:\n用一个对象去初始化另一个对象.\n1 2 Complex c1; Complex c2(c1); 一个对象作为函数参数传递给一个非引用类型的参数.\n1 2 3 void func(Complex c); Complex c1; func(c1); 一个对象作为函数返回值返回.\n1 2 Complex func(); Complex c1 = func(); 注意: 对象之间的赋值操作, 不会调用复制构造函数.\n1 2 Complex c1, c2; c1 = c2; // 不会调用复制构造函数 考虑到对象作为函数参数会掉用复制构造函数, 为了避免不必要的开销, 可以使用引用传递.\n手动写复制构造函数的目的一般是为了实现深拷贝.\n转换构造函数的目的是实现类型的自动转换. 不以说明符 explicit 声明 {且可以用单个参数调用 (C++11 前)} 的构造函数被称为转换构造函数.\n1 2 3 4 5 6 7 8 9 10 11 12 class Complex { public: double real, imag; Complex(int i) { // 类型转换构造函数 real = i; imag = 0; } }; int main() { Complex c1 = 9; // 隐式调用, 转换成一个临时 Complex 对象 return 0; } 如果加了 explicit 关键字, 则只能显式调用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real, imag; explicit Complex(int i) { // 类型转换构造函数 real = i; imag = 0; } }; int main() { Complex c1 = 9; // 错误 Complex c2(9); // 正确 return 0; } 析构函数 析构函数是类的一个特殊成员函数, 名字由波浪号 ~ 加类名构成, 没有参数, 也没有返回值, 对象消亡时即自动被调用, 作用是释放对象所占用的资源.\n如果定义类时没写析构函数, 则编译器生成缺省析构函数.缺省析构函数什么也不做. 如果定义了析构函数, 缺省析构函数就不会生成. 一个类只有一个析构函数, 不能重载.\n在数组生命周期结束时, 编译器会自动调用数组中每个元素的析构函数. delete 一个对象时, 会调用对象的析构函数. (注意, new 数组要用 delete[])\n1 2 3 4 5 Ctest *pTest; pTest = new Ctest; // 构造函数调用 delete pTest; // 析构函数调用 pTest = new Ctest[3]; // 构造函数调用 3 次 delete[] pTest; // 析构函数调用 3 次 this 指针 this 是一个指向对象本身的指针. 把 car.foo() 翻译成 C 就是 foo(\u0026amp;car)\n1 2 3 4 5 6 7 8 9 class A { int i; public: void hello() { cout \u0026lt;\u0026lt; \u0026#34;hello\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { A *p = NULL; p-\u0026gt;hello(); } 能运行且输出, 但是是未定义行为. 而且一旦 hello() 中用到了 this, 就会出错.\n非静态成员函数中可以直接使用 this 来代表指向该函数作用的对象的指针.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real, imag; void print() { cout \u0026lt;\u0026lt; real \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; imag; } Complex(double r, double i) : real(r), imag(i) {} Complex addOne() { this-\u0026gt;real++; // 等价于 real++; this-\u0026gt;print(); // 等价于 print() return *this; // 返回对象本身 } }; 静态成员 静态成员即加了 static 关键字的成员.\n静态成员变量是类的所有对象共享的. sizeof 不包括静态成员变量. 本质是全局变量. 静态成员函数是类的所有对象共享的函数, 静态成员函数只能访问静态成员变量和静态成员函数, 不能访问普通成员变量和普通成员函数, 也不可以用 this 指针. 本质是全局函数.\n访问静态成员可以不通过对象访问. 类名::静态成员名.\n1 2 int Rectangle::edges = 4; Rectangle::printTotal(); 即使类的对象不存在, 静态成员变量也存在.\n成员对象和封闭类 有成员对象的类叫封闭类.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CTyre { private: int radius; int width; public: CTyre(int r, int w) : radius(r), width(w) {} }; class CEngine {}; class CCar { private: int price; // 价格 CTyre tyre; CEngine engine; public: CCar(int p, int tr, int tw); }; CCar::CCar(int p, int tr, int w) : price(p), tyre(tr, w) {} 这个例子 CCar 必须有构造函数, 因为 CTyre 和没有默认构造函数.\n封闭类对象生成时, 先执行所有对象成员的构造函数, 然后才执行封闭类的构造函数. 对象成员的构造函数调用次序和对象成员在类中的说明次序一致, 与它们在成员初始化列表中出现的次序无关. 当封闭类的对象消亡时, 先执行封闭类的析构函数, 然后再执行成员对象的析构函数. 次序和构造函数的调用次序相反. 封闭类的对象, 如果是用默认复制构造函数初始化的, 那么它里面包含的成员对象也会用复制构造函数初始化. 友元 友元分为友元函数和友元类两种.\n友元函数: 一个类的友元函数可以访问该类的私有成员.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CCar; // 提前声明 CCar 类, 以便后面的 CDriver 类使用 class CDriver { public: void modifyCar(CCar *pCar); }; class CCar { private: int price; friend int mostExpensiveCar(CCar cars[], int total); // 声明友元 friend void CDriver::modifyCar(CCar *pCar); // 声明友元 // 可以将一个类的成员函数 (包括构造/析构函数) 说明为另一个类的友元。 }; void CDriver::modifyCar(CCar *pCar) { pCar-\u0026gt;price += 1000; } int mostExpensiveCar(CCar cars[], int total) { int tmpMax = -1; for (int i = 0; i \u0026lt; total; ++i) if (cars[i].price \u0026gt; tmpMax) tmpMax = cars[i].price; return tmpMax; } 如果 A 是 B 的友元类, 那么 A 的成员函数可以访问 B 的私有成员.\n1 2 3 4 5 6 7 8 9 10 11 12 class CCar { private: int price; friend class CDriver; // 声明 CDriver 为友元类 }; class CDriver { public: CCar myCar; void modifyCar() { myCar.price += 1000; // 因 CDriver 是 CCar 的友元类, 故此处可以访问其私有成员 } }; 友元类之间的关系不能传递, 不能继承.\n常量对象, 常量成员函数 如果不希望某个对象的值被改变, 则定义该对象的时候可以在前面加 const 关键字变为常量对象.\n在类的成员函数说明后面可以加 const 关键字, 则该成员函数成为常量成员函数. 常量成员函数内部不能改变属性的值, 也不能调用非常量成员函数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Sample { private: int value; public: void func() {}; Sample() {} void SetValue() const { value = 0; // wrong func(); // wrong } }; const Sample Obj; Obj.SetValue(); // 常量对象上可以使用常量成员函数 对于\n1 2 3 4 5 6 int getValue() const { return n; } int getValue() { return 2 * n; } 两个函数, 名字和参数表都一样, 但是一个是 const, 一个不是, 算重载.\n加上 mutable 关键字的成员变量, 即使在常量成员函数中也可以被修改.\n1 2 3 4 5 6 7 8 9 10 class CTest { public: bool getData() const { m_n1++; return m_b2; } private: mutable int m_n1; bool m_b2; }; ","date":"2025-02-19T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/class-and-object/","title":"程序设计实习(1) —— 类和对象"},{"content":"基础数学工具 定义\n随机变量 $X$ 的 期望 $E[X]$ 定义为\n$$ E[X] = \\sum_{x} x \\cdot P(X=x) $$随机变量 $X$ 的 方差 $\\text{Var}(X)$ 定义为\n$$ \\text{Var}(X) = E[(X - E[X])^2] $$标准差 $\\sigma(X)$ 定义为\n$$ \\sigma(X) = \\sqrt{\\text{Var}(X)} $$ 定理Markov 不等式\n设 $X$ 是一个非负随机变量, 期望存在, 那么对于任意 $t \u003e 0$ 有\n$$ P(X \\geq t) \\leq \\frac{E[X]}{t} $$ 定理Chebyshev 不等式\n设 $X$ 是一个随机变量, 期望和方差都存在, 那么对于任意 $t \u003e 0$ 有\n$$ P(|X - E[X]| \\geq t) \\leq \\frac{\\text{Var}(X)}{t^2} $$ 定义\n随机变量 $X$ 和 $Y$ 的 协方差 $\\text{Cov}(X, Y)$ 定义为\n$$ \\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] $$如果 $\\text{Cov}(X, Y) = 0$, 则称 $X$ 和 $Y$ 不相关.\n协方差具有对称性, 双线性.\n定义\n随机向量 $X=(X_1, X_2, \\ldots, X_n)$ 的 协方差矩阵 $C(X)$ 定义为\n$$ C(X) = E[(X - E[X])(X - E[X])^T] = (\\text{Cov}(X_i, X_j))_{ij} $$ 定义\nGauss 分布 (正态分布) 的概率密度函数为\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) $$Laplace 分布 的概率密度函数为\n$$ f(x) = \\frac{1}{2b} \\exp(-\\frac{|x-\\mu|}{b}) $$ 最优化问题\n$$ \\begin{aligned} \u0026 \\min f(x) \\\\ \\text{s.t. } \u0026 c_i(x) \\leq 0, i = 1, 2, \\dots, k \\\\ \u0026 h_j(x) = 0, j = 1, 2, \\dots, l \\end{aligned} $$构造 Lagrange 函数\n$$ L(x, \\alpha, \\beta) = f(x) + \\sum_{i=1}^{k} \\alpha_i c_i(x) + \\sum_{j=1}^{l} \\beta_j h_j(x) $$引入 Karush-Kuhn-Tucker (KKT) 条件\n$$ \\begin{aligned} \u0026 \\nabla_x L(x, \\alpha, \\beta) = 0 \\\\ \u0026 c_i(x) \\leq 0, i = 1, 2, \\dots, k \\\\ \u0026 h_j(x) = 0, j = 1, 2, \\dots, l \\\\ \u0026 \\alpha_i c_i(x) = 0, i = 1, 2, \\dots, k \\\\ \u0026 \\alpha_i \\geq 0, i = 1, 2, \\dots, k \\end{aligned} $$基本概念和术语 定义\n监督学习: 基于标记数据 $T=\\{ (x_i,y_i) \\}_{i=1}^N$, 学习一个从输入空间到输出空间的映射 $f: \\mathcal{X} \\mapsto \\mathcal{Y}$. 利用此对未见数据进行预测. 通常分为 回归 和 分类 两类.\n无监督学习: 基于未标记数据 $T=\\{ x_i \\}_{i=1}^N$, 发现其中隐含的知识模式. 聚类 是典型的无监督学习任务.\n半监督学习: 既有标记数据又有未标记数据 (通常占比较大).\n强化学习: 通过观察环境的反馈, 学习如何选择动作以获得最大的奖励.\n模型评估与选择 损失函数 模型基于算法按照一定策略给出假设 $h \\in \\mathcal{H}$, 通过 损失函数 $L(h(x), y)$ 衡量假设的好坏.\n0-1 损失函数: $$L(h(x), y) = \\mathbb{I}(h(x) \\neq y) = \\begin{cases} 0, \u0026 h(x) = y \\\\ 1, \u0026 h(x) \\neq y \\end{cases}$$ 平方损失函数: $$L(h(x), y) = (h(x) - y)^2$$平均损失 $R(h) = E_{x \\sim D} [L(h(x), y)]$ 称为 泛化误差.\n容易验证, 对于 0-1 损失函数, 准确率 $a = 1-R(h)$.\n二分类 对于二分类问题, 样本预测结果有四种情况:\n真正例 (True Positive, TP): 预测为正例, 实际为正例. 假正例 (False Positive, FP): 预测为正例, 实际为负例. 真负例 (True Negative, TN): 预测为负例, 实际为负例. 假负例 (False Negative, FN): 预测为负例, 实际为正例. 由此引入\n准确率(查准率): $P = \\frac{TP}{TP+FP}$. 召回率(查全率): $R = \\frac{TP}{TP+FN}$. $F_1$ 度量: 考虑到二者抵触, 引入调和均值 $F_1 = \\frac{2PR}{P+R}$. 过拟合和正则化 为了防止由于模型过于复杂而导致的过拟合, 可以通过 正则化 方法来限制模型的复杂度.\n$$ \\min \\sum_{i=1}^{N} L(h(x_i), y_i) + \\lambda J(h) $$其中 $J(h)$ 是随着模型复杂度增加而增加的函数. $\\lambda$ 是正则化参数.\n怎么选取合适的 $\\lambda$ ? 一般是先给出若干候选, 在验证集上进行评估, 选取泛化误差最小的.\n数据集划分 一般将数据集划分为 训练集 $T$ 和 测试(验证)集 $T^\\prime$.\n留出法 (hold-out): 分层无放回地随机采样. 也叫简单交叉验证. $k$ 折交叉验证 ($k$-fold cross validation): 将数据集分为 $k$ 个大小相等的子集, 每次取其中一个作为验证集, 其余作为训练集, 最后以这 $k$ 次的平均误差作为泛化误差的估计. 当 $k=|D|$ 时称为留一 (leave-one-out) 验证法. 自助法 (bootstrapping): 从数据集中有放回地采样 $|D|$ 个数据作为训练集, 没抽中的作为验证集. 因而训练集 $T$ 和原始数据集 $D$ 的分布未必一致, 对数据分布敏感的模型不适用. 偏差-方差分解 为什么泛化误差会随着模型复杂度的增加而先减小后增大?\n定义\n偏差 (bias): 模型预测值的期望与真实值之间的差异. 体现了模型的拟合能力.\n$$\\text{Bias}(x) = E_T[h_T(x)-c(x)] = \\bar{h}(x) - c(x)$$方差 (variance): 模型预测值的方差. 体现了模型的对数据扰动的稳定性.\n$$\\text{Var}(x) = E[(h(x) - \\bar{h}(x))^2]$$ 现在对泛化误差进行分解:\n$$ \\begin{aligned} R(h) \u0026= E_T[(h_T(x) - c(x))^2] \\\\ \u0026= E_T[h_T^2(x) - 2h_T(x)c(x) + c^2(x)] \\\\ \u0026= E_T[h_T^2(x)] - 2c(x)E_T[h_T(x)] + c^2(x) \\\\ \u0026= E_T[h_T^2(x)] - \\bar{h}^2(x) + \\bar{h}^2(x) - 2\\bar{h}(x)c(x) + c^2(x) \\\\ \u0026= E_T[(h_T(x) - \\bar{h}(x))^2] + (\\bar{h}(x) - c(x))^2 \\\\ \u0026= \\text{Var}(x) + \\text{Bias}^2(x) \\end{aligned} $$当然, 由于噪声存在, $y$ 未必一定等于 $c(x)$, 不妨设 $y=c(x)+\\varepsilon$, 其中 $\\varepsilon \\sim \\Epsilon$ 期望为 $0$. 可以证明\n定理偏差-方差分解\n$$ E_{T \\sim D^{|T|}, \\varepsilon \\sim \\Epsilon} [(h_T(x)-y)^2] = \\text{Bias}^2(x) + \\text{Var}(x) + E[\\varepsilon^2] $$即泛化误差可以分解为偏差、方差和噪声三部分.\n起初, 模型较为简单, 偏差在泛化误差起主导作用. 随着模型复杂度的增加, 拟合能力增强, 偏差减小, 但带来过拟合风险, 算法对数据扰动敏感, 方差增大. 方差占比逐渐增大, 最终导致泛化误差增大.\n","date":"2025-02-18T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/intro/","title":"机器学习基础(1) —— 概述"},{"content":"凸优化 凸问题的可行集都是凸集.\n定理\n凸优化问题的任意局部极小点都是全局最优点.\n证明\n假设 $x$ 是局部极小, $y$ 全局最优且 $f(y) \u003c f(x)$.\n考虑 $z = \\theta x + (1-\\theta) y$, 则由于 $z$ 是可行点的凸组合, 也是可行点. 由于 $f$ 是凸函数, 有\n$$ f(z) \\leq \\theta f(x) + (1-\\theta) f(y) \u003c f(x) $$取 $\\theta \\to 1$, 则 $f(z) \\to f(x)$, 与局部最小性矛盾.\n线性规划 所谓 线性规划(LP) 问题是指目标函数和约束条件都是线性的优化问题. 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 Gx \\le e \\end{aligned} $$$\\ell_1$ 和 $\\ell_\\infty$ 范数实际上也是线性的.\n示例: 最大球问题 凸多边形\n$$P = \\{ x \\mid a_i^Tx \\le b_i \\}$$的 Chebyshev 中心是最大半径内切球的中心. 代入得\n$$ \\sup \\{a_i^T (x_c + u) \\mid \\Vert u \\Vert_2 \\le r \\} = a_i^Tx_c + r \\Vert a_i \\Vert_2 \\le b_i $$这也变成了一个线性规划问题.\n二次规划 二次规划问题是指目标函数是二次的的优化问题.\n例如, 对于线性约束条件的问题, 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 \\frac{1}{2} x^T P x + q^T x + r \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 Gx \\le e \\end{aligned} $$也有 带二次约束的二次规划 (QCQP).\n我们归结为 二次锥规划 (SOCP):\n$$ \\begin{aligned} \\min \\quad \u0026 f^T x \\\\ \\text{s.t.} \\quad \u0026 \\Vert A_i x + b_i \\Vert_2 \\le c_i^T x + d_i, \\quad i = 1, \\ldots, m \\\\ \u0026 Fx = g \\end{aligned} $$示例: 最小范数问题 令 $\\bar{v}_i = A_ix+b_i \\in \\mathbb{R}^{n_i}$, 则 $\\min_x \\sum_i \\Vert \\bar{v}_i \\Vert_2$ 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_i v_{i0} \\\\ \\text{s.t.} \\quad \u0026\\bar{v}_i = A_i x + b_i \\\\ \u0026(v_{i0}, \\bar{v}_i) \\succeq_\\mathcal{Q} 0 \\end{aligned} $$其中 $\\mathcal{Q}$ 是二次锥.\n示例: 最小化最大函数和问题 设 $\\theta(x)=(\\theta_1(x), \\theta_2(x), \\cdots, \\theta_m(x))^T$. $\\theta_{[i]}$ 是 $\\theta_i$ 的非递增方式的排序. 则 $\\min_{x\\in Q} \\sum_{i=1}^m \\theta_{[i]}(x)$ 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_{i=1}^m u_i + kt \\\\ \\text{s.t.} \\quad \u0026 x \\in Q \\\\ \u0026 \\theta_i(x)\\le u_i + t \\\\ \u0026 u_i \\ge 0 \\end{aligned} $$半定优化 半定优化 (SDP) 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 c^Tx \\\\ \\text{s.t.} \\quad \u0026 x_1A_1 + \\cdots + x_nA_n + B \\succeq 0 \\\\ \u0026 Gx=h \\end{aligned} $$其实是线性规划在矩阵空间的推广. 仍然考虑标准形式:\n$$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c C, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 \\left\u003c A_i, X \\right\u003e = b_i, \\quad i = 1, \\ldots, m \\\\ \u0026 X \\succeq 0 \\end{aligned} $$和对偶形式:\n$$ \\begin{aligned} \\max \\quad \u0026 \\sum_{i=1}^m b_i y_i \\\\ \\text{s.t.} \\quad \u0026 C - \\sum_{i=1}^m y_i A_i \\succeq 0 \\end{aligned} $$示例: 二次约束二次规划问题 $$ \\begin{aligned} \\min \\quad \u0026 x^T A_0 x + 2b_0^T x + c_0 \\\\ \\text{s.t.} \\quad \u0026 x^T A_i x + 2b_i^T x + c_i \\le 0, \\quad i = 1, \\ldots, m \\end{aligned} $$其中 $A_i$ 是 $n \\times n$ 的对称矩阵, 这个问题在 $A_i$ 不定时实际上是 NP-hard 的. 考虑它的半定松弛, 记 $X=x^Tx$ 注意到有\n$$ x^T A_i x + 2b_i^T x + c_i = \\left\u003c A_i, X \\right\u003e + 2\\left\u003c b_i, x \\right\u003e + c_i = \\left\u003c \\begin{bmatrix} A_i \u0026 b_i \\\\ b_i^T \u0026 c_i \\end{bmatrix}, \\begin{bmatrix} X \u0026 x \\\\ x^T \u0026 1 \\end{bmatrix} \\right\u003e $$我们记作 $\\left\u003c \\bar{A}_i, \\bar{X} \\right\u003e$. 注意到, 现在唯一的非线性部分是约束 $X=xx^T$, 我们将其松弛成半正定约束 $X \\succeq xx^T$. 可以证明, $\\bar{X} \\succeq 0$ 等价于 $X \\succeq xx^T$. 这样我们就得到了一个半定优化问题:\n$$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c A_0, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 \\left\u003c \\bar{A}_i, \\bar{X} \\right\u003e \\le 0, \\quad i = 1, \\ldots, m \\\\ \u0026 \\bar{X} \\succeq 0 \\\\ \u0026 \\bar{X}_{n+1,n+1} = 1 \\end{aligned} $$示例: 最大割问题 令 $G$ 为一个无向图, 节点集为 $V = \\{1, 2, \\cdots, n\\}$, 边集为 $E$. 设 $w_{ij} = w_{ji} \\ge 0$ 为边 $(i, j) \\in E$ 上的权重, 要找 $S \\subseteq V$ 使得 $S$ 与 $\\bar{S}$ 之间相连边的权重之和最大化.\n我们定义 $x_j = 1, j \\in S$ 和 $x_j = -1, j \\in \\bar{S}$, 则\n$$ \\begin{aligned} \\max \\quad \u0026 \\sum_{(i, j) \\in E} \\frac{1}{2} (1-x_i x_j) w_{ij} \\\\ \\text{s.t.} \\quad \u0026 x_i = \\pm 1, \\quad i = 1, \\ldots, n \\end{aligned} $$然而这是一个离散优化问题, 考虑对它做松弛. 令 $W=(w_{ij}) \\in \\mathbb{S}^n$ 为权重矩阵, $C=-\\frac{1}{4}(\\text{Diag}(W\\mathbf{1})-W)$ 是 Laplacian 矩阵的 $-1/4$ 倍. 则\n$$ \\begin{aligned} \\min \\quad \u0026 x^T C x \\\\ \\text{s.t.} \\quad \u0026 x_i^2 = 1, \\quad i = 1, \\ldots, n \\end{aligned} $$仍令 $X=x^Tx$, 则容易看出与下问题等价:\n$$ \\begin{aligned} \\min \\quad \u0026 \\left\u003c C, X \\right\u003e \\\\ \\text{s.t.} \\quad \u0026 X_{ii} = 1, \\quad i = 1, \\ldots, n \\\\ \u0026 X \\succeq 0 \\\\ \u0026 \\text{rank}(X) = 1 \\end{aligned} $$示例: 极小化最大特征值问题 $$ \\min \\quad \\lambda_{\\max}(A_0 + \\sum_{i=1}^m x_i A_i) $$注意到:\n$$\\lambda_{\\max}(A) \\le t \\Leftrightarrow A \\preceq tI$$于是我们有 SDP 形式:\n$$ \\begin{aligned} \\min \\quad \u0026 z \\\\ \\text{s.t.} \\quad \u0026 A_0 + \\sum_{i=1}^m x_i A_i \\preceq zI \\end{aligned} $$示例: 极小化二范数问题 $$ \\min \\quad \\Vert A_0 + \\sum_{i=1}^m x_i A_i \\Vert_2 $$记 $A = A_0 + \\sum_{i=1}^m x_i A_i$. 注意到:\n$$ \\Vert A \\Vert_2 \\le t \\Leftrightarrow A^TA \\preceq t^2I \\Leftrightarrow \\begin{bmatrix} tI \u0026 A \\\\ A^T \u0026 tI \\end{bmatrix} \\succeq 0 $$于是我们有 SDP 形式:\n$$ \\begin{aligned} \\min \\quad \u0026 t \\\\ \\text{s.t.} \\quad \u0026 \\begin{bmatrix} tI \u0026 A \\\\ A \u0026 tI \\end{bmatrix} \\succeq 0 \\end{aligned} $$示例: 特征值优化问题 $$ \\min \\quad \\sum_{i=1}^n \\lambda_{[i]}(A_0 + \\sum_{j=1}^m x_j A_j) $$其中 $\\lambda_{[i]}(A)$ 表示 $A$ 的第 $i$ 大特征值. 前面的极小最大函数和提到它等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_{i=1}^n u_i + kt \\\\ \\text{s.t.} \\quad \u0026 u_i+t \\ge \\lambda_i(A_0 + \\sum_{j=1}^m x_j A_j), \\quad i = 1, \\ldots, n \\\\ \u0026 u_i \\ge 0 \\end{aligned} $$设 $u_i = \\lambda_i(X)$, 则写成 SDP 形式:\n$$ \\begin{aligned} \\min \\quad \u0026 kt + \\text{Tr}(X) \\\\ \\text{s.t.} \\quad \u0026 tI + X \\succeq A_0 + \\sum_{j=1}^m z_j A_j \\\\ \u0026 X \\succeq 0 \\end{aligned} $$","date":"2025-02-15T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/opt-problem/","title":"最优化方法(4) —— 优化问题"},{"content":"基本线性代数知识 定义\n给定函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$, 且 $f$ 在 $x$ 一个邻域内有定义, 若存在 $g \\in \\mathbb{R}^n$, 使得\n$$ \\lim_{p \\to 0} \\frac{f(x+p)-f(x)-g^Tp}{\\Vert p \\Vert} = 0 $$其中 $\\Vert \\cdot \\Vert$ 是向量范数, 则称 $f$ 在 $x$ 处 可微. 此时, $g$ 称为 $f$ 在 $x$ 处的 梯度, 记为 $\\nabla f(x)$.\n显然, 如果梯度存在, 令 $p = \\varepsilon e_i$, 易得\n$$ \\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right) $$ 定义\n如果函数 $f(x): \\mathbb{R}^n \\mapsto \\mathbb{R}$ 在点 $x$ 处的二阶偏导数 $\\dfrac{\\partial^2 f}{\\partial x_i \\partial x_j}$ 存在, 则称 $f$ 在 $x$ 处 二次可微. 此时, $n \\times n$ 矩阵\n$$ \\nabla^2 f(x) = \\begin{pmatrix} \\dfrac{\\partial^2 f}{\\partial x_1^2} \u0026 \\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\dfrac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026 \\dfrac{\\partial^2 f}{\\partial x_2^2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\dfrac{\\partial^2 f}{\\partial x_n \\partial x_1} \u0026 \\dfrac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_n^2} \\end{pmatrix} $$称为 $f$ 在 $x$ 处的 Hessian 矩阵. 若 $\\nabla^2 f(x)$ 在 $D$ 上连续, 则称 $f$ 在 $D$ 上 二次连续可微.\n可以证明, 若 $f$ 在 $D$ 上二次连续可微, 则 $\\nabla^2 f(x)$ 为对称矩阵.\n多元函数的梯度可以推广到变量是矩阵的情形.\n定义\n给定函数 $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$, 且 $f$ 在 $X$ 一个邻域内有定义, 若存在 $G \\in \\mathbb{R}^{m \\times n}$, 使得\n$$ \\lim_{V \\to 0} \\frac{f(X+V)-f(X)-\\left\u003c G, V \\right\u003e}{\\Vert V \\Vert} = 0 $$其中 $\\Vert \\cdot \\Vert$ 是矩阵范数, 则称 $f$ 在 $X$ 处 (Fréchet)可微. 此时, $G$ 称为 $f$ 在 $X$ 处的 梯度, 记为 $\\nabla f(X)$.\n矩阵的可微有另一种较为简单常用的定义.\n定义\n给定函数 $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$, 若存在矩阵 $G \\in \\mathbb{R}^{m \\times n}$, 使得\n$$ \\lim_{t \\to 0} \\frac{f(X+tV)-f(X)}{t} = \\left\u003c G, V \\right\u003e $$则称 $f$ 在 $X$ 处 (Gâteaux)可微.\n例如:\n$f(X) = \\text{tr}(AX^TB)$, 此时 $\\nabla f(X) = BA$.\n$f(X, Y)=\\frac{1}{2} \\Vert XY-A \\Vert_F^2$. 此时\n$$ \\begin{aligned} \u0026f(X,Y+tV)-f(X,Y) \\\\ \u0026= \\frac{1}{2} \\Vert X(Y+tV)-A \\Vert_F^2 - \\frac{1}{2} \\Vert XY-A \\Vert_F^2 \\\\ \u0026= \\frac{1}{2} \\Vert XY - A + tVX \\Vert_F^2 - \\frac{1}{2} \\Vert XY - A \\Vert_F^2 \\\\ \u0026= \\frac{1}{2} \\Vert tVX \\Vert_F^2 + \\left\u003c XY-A, tVX \\right\u003e \\\\ \u0026= t \\left\u003c X^T(XY-A), V \\right\u003e + o(t) \\end{aligned} $$所以 $\\frac{\\partial f}{\\partial Y} = X^T(XY-A)$, 类似地, $\\frac{\\partial f}{\\partial X} = (XY-A)Y^T$.\n$f(X)=\\ln\\text{det}(X)$, $X$ 为正定矩阵. 此时\n$$ \\begin{aligned} \u0026f(X+tV)-f(X) \\\\ \u0026= \\ln\\text{det}(X+tV) - \\ln\\text{det}(X) \\\\ \u0026= \\ln\\text{det}(I+tX^{-1/2}VX^{-1/2}) \\end{aligned} $$考虑 $X^{-1/2}VX^{-1/2}$ 的特征值 $\\lambda_i$, 则由特征值之和为迹, 有\n$$ \\begin{aligned} \u0026= \\ln\\text{det}\\prod_{i=1}^n (1+t\\lambda_i) \\\\ \u0026= \\sum_{i=1}^n \\ln(1+t\\lambda_i) \\\\ \u0026= \\sum_{i=1}^n t\\lambda_i + o(t) \\\\ \u0026= t\\text{tr}(X^{-1/2}VX^{-1/2}) + o(t) \\\\ \u0026= t\\text{tr}(X^{-1}V) + o(t) \\\\ \u0026= t\\left\u003c X^{-T}, V \\right\u003e + o(t) \\end{aligned} $$所以 $\\nabla f(X) = X^{-T}$.\n定义\n广义实数 是一种扩充实数域的数, 记为 $\\bar{\\mathbb{R}} = \\mathbb{R} \\cup \\{ \\pm \\infty \\}$. 映射 $f: \\mathbb{R}^n \\mapsto \\bar{\\mathbb{R}}$ 称为 广义实值函数.\n定义\n给定广义实值函数 $f$ 和非空集合 $X$. 如果存在 $x \\in X$ 使得 $f(x) \u003c +\\infty$, 并且对任意的 $x \\in X$, 都有 $f(x) \u003e -\\infty$, 那么称函数 $f$ 关于集合 $X$ 是 适当的．\n定义\n对于广义实值函数 $f: \\mathbb{R}^n \\mapsto \\bar{\\mathbb{R}}$,\n$C_\\alpha = \\{x \\mid f(x) \\le \\alpha \\}$ 称为 $f$ 的 $\\alpha$-下水平集. $\\text{epi} f = \\{ (x, t) \\mid f(x) \\le t \\}$ 称为 $f$ 的 上方图. 若 $\\text{epi} f$ 为闭集, 则称 $f$ 为闭函数. 若对任意的 $x \\in \\mathbb{R}^n$, 有 $\\liminf_{y \\to x} f(y) \\ge f(x)$, 则称 $f$ 为 下半连续函数. 定理\n对于广义实值函数 $f$, 以下命题等价:\n$f(x)$ 的任意 $\\alpha$-下水平集都是闭集; $f(x)$ 是下半连续的; $f(x)$ 是闭函数. 证明\n(1) $\\Rightarrow$ (2): 反证, 假设 $x_k \\to \\bar{x}$ 但 $\\liminf_{k \\to \\infty} f(x_k) \u003c f(\\bar{x})$. 取 $t$ 介于二者之间.\n考虑到 $\\liminf_{k \\to \\infty} f(x_k) \u003c t$, 则有无穷多 $x_k$ 使得 $f(x_k) \\le t$, 即这些 $x_k$ 在 $C_t$ 中. 由于 $C_t$ 是闭集, 则 $\\bar{x} \\in C_t$, 即 $f(\\bar{x}) \\le t$, 矛盾.\n(2) $\\Rightarrow$ (3): 考虑 $(x_k,y_k) \\in \\text{epi} f \\to (\\bar{x},\\bar{y})$, 由于 $f$ 下半连续, 则\n$$ f(\\bar{x}) \\le \\liminf_{k \\to \\infty} f(x_k) = \\liminf_{k \\to \\infty} y_k = \\bar{y} $$即 $(\\bar{x}, \\bar{y}) \\in \\text{epi} f$.\n(3) $\\Rightarrow$ (1): 考虑 $x_k \\in C_\\alpha \\to \\bar{x}$, 则 $(x_k, \\alpha) \\in \\text{epi} f \\to (\\bar{x}, \\alpha)$, 所以 $(\\bar{x}, \\alpha) \\in \\text{epi} f$, 即 $f(\\bar{x}) \\le \\alpha$, 所以 $\\bar{x} \\in C_\\alpha$.\n适当闭函数的和, 复合, 逐点上确界仍然是闭函数.\n凸函数 定义\n适当函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 凸函数, 如果 $\\text{dom} f$ 是凸集, 且对任意的 $x, y \\in \\text{dom} f$ 和 $\\theta \\in [0,1]$, 有\n$$ f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y) $$ 易知仿射函数既是凸函数又是凹函数. 所有的范数都是凸函数.\n定义\n若存在常数 $m \u003e 0$, 使得 $g(x) = f(x) - \\frac{m}{2} \\Vert x \\Vert^2$ 是凸函数, 则称 $f$ 是 强凸函数, $m$ 称为 强凸参数.\n定理凸函数判定定理\n适当函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 是凸函数的充要条件是, 对任意的 $x \\in \\text{dom} f$, 函数 $g: \\mathbb{R} \\mapsto \\mathbb{R}$ 是凸函数, 其中\n$$g(t) = f(x+tv), \\quad \\text{dom}g = \\{ t \\mid x + tv \\in \\text{dom} f \\}$$ 定理一阶条件\n对于定义在凸集上的可微函数 $f$, $f$ 是凸函数当且仅当\n$$ f(y) \\ge f(x) + \\nabla f(x)^T(y-x), \\quad \\forall x, y \\in \\text{dom} f $$证明\n必要性: 设 $f$ 凸, 则 $\\forall x, y \\in \\text{dom} f, t \\in [0,1]$, 有\n$$tf(y)+(1-t)f(x) \\ge f(x+t(y-x))$$令 $t \\to 0$, 即\n$$f(y)-f(x) \\ge \\frac{f(x+t(y-x))-f(x)}{t} \\to \\nabla f(x)^T(y-x)$$充分性: $\\forall x, y \\in \\text{dom}f, t\\in (0,1)$, 取 $z = tx+(1-t)y$, 则\n$$ \\begin{aligned} f(x) \u0026\\ge f(z) + \\nabla f(z)^T(x-z) \\\\ f(y) \u0026\\ge f(z) + \\nabla f(z)^T(y-z) \\end{aligned} $$一式乘以 $t$, 二式乘以 $1-t$, 相加即得.\n定理梯度单调性\n设 $f$ 为可微函数, 则 $f$ 为凸函数当且仅当 $\\text{dom} f$ 为凸集且 $\\nabla f$ 为单调映射.\n$$(\\nabla f(x) - \\nabla f(y))^T(x-y) \\ge 0$$证明\n必要性: 根据一阶条件, 有\n$$ \\begin{aligned} f(y) \u0026\\ge f(x) + \\nabla f(x)^T(y-x) \\\\ f(x) \u0026\\ge f(y) + \\nabla f(y)^T(x-y) \\end{aligned} $$相加即可.\n充分性: 考虑 $g(t)=f(x+t(y-x))$, 则 $g^\\prime(t)=\\nabla f(x+t(y-x))^T (y-x)$, 从而 $g^\\prime (t) \\ge g^\\prime (0)$.\n$$ \\begin{aligned} f(y) \u0026= g(1) = g(0) + \\int_{0}^1 g^\\prime(t) dt \\\\ \u0026\\ge g(0) + \\int_{0}^1 g^\\prime(0) dt = g(0) + g^\\prime(0) \\\\ \u0026= f(x) + \\nabla f(x)^T(y-x) \\end{aligned} $$ 定理\n函数 $f(x)$ 是凸函数当且仅当 $\\text{epi}f$ 是凸集.\n定理二阶条件\n设 $f$ 为定义在凸集上的二阶连续可微函数, $f$ 是凸函数当且仅当 $\\nabla^2 f(x) \\succeq 0, \\forall x \\in \\text{dom} f$. 若不取等, 则为严格凸函数.\n证明\n必要性: 反设 $f(x)$ 在 $x$ 处 $\\nabla^2 f(x) \\prec 0$, 则存在 $v \\in \\mathbb{R}^n$, 使得 $v^T \\nabla^2 f(x) v \u003c 0$, 考虑 Peano 余项\n$$ f(x+tv)=f(x)+t\\nabla f(x)^Tv+\\frac{t^2}{2}v^T\\nabla^2 f(x+tv)v + o(t^2) $$取 $t$ 充分小,\n$$ \\frac{f(x+tv)-f(x)-t\\nabla f(x)^T v}{t^2}=\\frac{1}{2}v^T\\nabla^2 f(x+tv)v + o(1) \u003c 0 $$这和一阶条件矛盾.\n充分性: 对于任意的 $x, y \\in \\text{dom} f$, 有\n$$ \\begin{aligned} f(y) \u0026= f(x)+\\nabla f(x)^T(y-x)+\\frac{1}{2}(y-x)^T\\nabla^2 f(z)(y-x) \\\\ \u0026\\ge f(x)+\\nabla f(x)^T(y-x) \\end{aligned} $$由一阶条件, $f$ 为凸函数.\n保凸运算 下面举一些重要的例子.\n逐点取上界: 若对每个 $y \\in A$, $f(x,y)$ 都是关于 $x$ 的凸函数, 则\n$$g(x)=\\sup_{y \\in A} f(x,y)$$也是凸函数.\n$C$ 的支撑函数 $f(x)=\\sup_{y \\in C} y^Tx$ 是凸函数. $C$ 到 $x$ 的最远距离 $f(x)=\\sup_{y \\in C} \\Vert x-y \\Vert$ 是凸函数. 对称阵 $X \\in \\mathbb{S}^n$ 的最大特征值 $\\lambda_{\\max}(X)=\\sup_{\\Vert x \\Vert=1} x^TXx$ 是凸函数. 标量函数的复合: 若 $g: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 是凸函数, $h: \\mathbb{R} \\mapsto \\mathbb{R}$ 是单调不减的凸函数, 则\n$$f(x) = h(g(x))$$也是凸函数. 凹同理.\n如果 $g$ 凸, 则 $f(x) = \\exp(g(x))$ 也是凸函数. 如果 $g$ 凹, 则 $f(x) = 1/g(x)$ 也是凸函数. 取下确界: 若 $f(x, y)$ 关于 $(x, y)$ 整体是凸函数, $C$ 是凸集, 则\n$$g(x) = \\inf_{y \\in C} f(x, y)$$也是凸函数.\n凸集 $C$ 到 $x$ 的距离 $f(x)=\\inf_{y \\in C} \\Vert x-y \\Vert$ 是凸函数. 透视函数: 若 $f: \\mathbb{R}^{n} \\mapsto \\mathbb{R}$ 是凸函数, 则\n$$g(x, t) = tf(x/t), \\quad \\text{dom} g = \\{ (x, t) \\mid x / t \\in \\text{dom} f, t \u003e 0 \\}$$也是凸函数.\n相对熵函数 $g(x,t)=t\\log t-t\\log x$ 是凸函数. 若 $f$ 凸, 则 $g(x)=(c^T+d)f((Ax+b)/(c^T+d))$ 也是凸函数. 共轭函数: 任意适当函数 $f$ 的共轭函数\n$$f^\\ast(y)=\\sup_{x \\in \\text{dom} f} (\\left\u003c x, y \\right\u003e - f(x))$$是凸函数.\n凸函数的推广 定义\n$f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 拟凸的, 如果 $\\text{dom} f$ 是凸集, 且对任意 $\\alpha$, 下水平集 $C_\\alpha$ 是凸集.\n若 $f$ 是拟凸的, 则称 $-f$ 是 拟凹的. 若 $f$ 既是拟凸又是拟凹的, 则称 $f$ 是 拟线性的.\n注意: 拟凸函数的和不一定是拟凸函数.\n定理\n拟凸函数满足类 Jenson 不等式: 对拟凸函数 $f$ 和 $\\forall x, y \\in \\text{dom} f, \\theta \\in [0,1]$, 有 $$f(\\theta x + (1-\\theta)y) \\le \\max\\left\\{f(x),f(y)\\right\\}$$ 拟凸函数满足一阶条件: 定义在凸集上的可微函数 $f$ 拟凸当且仅当 $$f(y) \\le f(x) \\Rightarrow \\nabla f(x)^T(y-x) \\le 0$$ 定义\n如果正值函数 $f$ 满足 $\\log f$ 是凸函数, 则 $f$ 称为 对数凸函数; 若为凹函数, 则 $f$ 称为 对数凹函数.\n例如, 正态分布\n$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\text{det} \\Sigma}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$是对数凹函数.\n对数凹函数的乘积, 积分都是对数凹的, 但加和不一定是对数凹的.\n在广义不等式下, 也可以定义凸凹性.\n定义\n$$f(\\theta x+(1-\\theta)y \\preceq_K \\theta f(x)+(1-\\theta)f(y))$$ 对任意 $x,y \\in \\text{dom} f, 0 \\le \\theta \\le 1$ 成立.\n例如, $f: \\mathbb{S}^m \\mapsto \\mathbb{S}^m$, $f(X)=X^2$ 是 $\\mathbb{S}^m_+$-凸函数. 这点利用 $z^TX^2z=\\Vert Xz \\Vert^2$ 是关于 $X$ 的凸函数即可得知.\n","date":"2025-01-25T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-function/","title":"最优化方法(3) —— 凸函数"},{"content":"范数 定义\n记号 $\\Vert \\cdot \\Vert: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 向量范数, 若满足:\n正定性: $\\Vert x \\Vert \\geq 0$, 且 $\\Vert x \\Vert = 0 \\Leftrightarrow x = 0$; 齐次性: $\\Vert \\alpha x \\Vert = \\vert \\alpha \\vert \\Vert x \\Vert$; 三角不等式: $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$. $\\ell_p$ 范数是最常见的向量范数\n$$ \\Vert x \\Vert_p = \\left( \\sum_{i=1}^n \\vert x_i \\vert^p \\right) ^{\\frac{1}{p}} $$特别地, 当 $p = \\infty$ 时, $\\Vert x \\Vert_\\infty = \\max_i \\vert x_i \\vert$.\n向量范数可以自然地推广到矩阵范数. 常见的矩阵范数有:\n和范数: $\\Vert A \\Vert_1 = \\sum_{i,j} \\vert A_{ij} \\vert$; Frobenius 范数: $\\Vert A \\Vert_F = \\sqrt{\\sum_{i,j} A_{ij} ^2} = \\sqrt{\\text{tr}(A^T A)}$; 算子范数: $\\Vert A \\Vert_{(m,n)}=\\max_{\\Vert x \\Vert_n = 1} \\Vert Ax \\Vert_m$. 特别地, 当 $m = n = p$ 时: $p=1$ 时, $\\Vert A \\Vert_{p=1} = \\max_j \\sum_i \\vert A_{ij} \\vert$; $p=2$ 时, $\\Vert A \\Vert_{p=2} = \\sqrt{\\lambda_{\\max}(A^T A)}$, 亦称为 谱范数. $p=\\infty$ 时, $\\Vert A \\Vert_{p=\\infty} = \\max_i \\sum_j \\vert A_{ij} \\vert$. 核范数: $\\Vert A \\Vert_\\ast = \\sum_i \\sigma_i$, 其中 $\\sigma_i$ 为 $A$ 的奇异值. 定理Cauchy 不等式\n$$\\vert \\left\u003c X, Y \\right\u003e \\vert \\leq \\Vert X \\Vert \\Vert Y \\Vert$$等号成立当且仅当 $X$ 与 $Y$ 线性相关.\n凸集 定义\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in \\mathbb{R}$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 仿射集.\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in [0, 1]$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 凸集.\n换言之, 仿射集要求过任意两点的直线都在集合内, 而凸集要求过任意两点的线段都在集合内. 显然, 仿射集都是凸集. 线性方程组的解集是一个仿射集, 而线性规划问题的可行域是一个凸集. 可以证明, 仿射集均可表示为某个线性方程组的解集.\n定理\n若 $S$ 是凸集, 则 $kS = \\left\\{ ks \\mid k \\in \\mathbb{R}, s \\in S \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S + T = \\left\\{ s + t \\mid s \\in S, t \\in T \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S \\cap T$ 也是凸集. 凸集的内部和闭包均为凸集. 可以证明, 任意多个凸集的交集仍为凸集.\n定义\n形如 $x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_kx_k$, 其中 $\\theta_i \\geq 0$ 且 $\\sum_i \\theta_i = 1$, 的表达式称为 $x$ 的 凸组合. 集合 $S$ 的所有点的凸组合构成的集合称为 $S$ 的 凸包, 记为 $\\text{conv}(S)$.\n定理\n若 $\\text{conv} S \\subseteq S$, 则 $S$ 为凸集, 反之亦然.\n证明\n先证明正方向. 对任意 $x,y \\in S, \\theta \\in [0,1]$, 有 $\\theta x + (1-\\theta) y \\in \\text{conv} S \\subseteq S$, 故 $S$ 为凸集.\n再证明反方向, 对凸组合的维数 $k$ 采用数学归纳法证明之.\n若 $k=1$, 显然成立. 假设对于 $k-1$ 成立, 则对于 $k$, 考虑\n$$ \\begin{aligned} x \u0026= \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_k x_k \\\\ \u0026= (1-\\theta_k)\\left(\\frac{\\theta_1}{1-\\theta_k} x_1 + \\frac{\\theta_2}{1-\\theta_k} x_2 + \\cdots + \\frac{\\theta_{k-1}}{1-\\theta_k} x_{k-1}\\right) + \\theta_k x_k \\end{aligned} $$前面大括号内的表达式为 $k-1$ 个凸组合, 故在 $S$ 中. 于是 $x$ 又成为两个点的凸组合, 由于 $S$ 为凸集, 故 $x \\in S$. 则 $\\text{conv} S \\subseteq S$.\n定理\n$\\text{conv}S$ 是包含 $S$ 的最小凸集; $\\text{conv}S$ 是所有包含 $S$ 的凸集的交集. 证明\n显然第一个是第二个的推论, 只证明第二个.\n已知凸集的交是凸集, 从而所有包含 $S$ 的凸集的交集 $X$ 是凸集. 且 $\\text{conv} S$ 是包含 $S$ 的凸集, 则 $X \\subseteq \\text{conv} S$.\n另一方面, $S \\subseteq X$, 则 $\\text{conv} S \\subseteq \\text{conv}X$, 而 $X$ 是凸集, 则 $\\text{conv}X = X$, 即 $\\text{conv}S \\subseteq X$. 综上, $\\text{conv}S = X$.\n仿照凸组合和凸包, 也可以定义仿射组合和仿射包 $\\text{affine} S$, 不再赘述.\n定义\n形如 $x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_kx_k$, 其中 $\\theta_i \\geq 0$ 的表达式称为 $x$ 的 锥组合. 若集合 $S$ 中任意点的锥组合都在 $S$ 中, 则称 $S$ 为凸锥.\n常见凸集 定义\n任取非零向量 $a\\in \\mathbb{R}^n$, 形如\n$$ \\left\\{ x \\mid a^Tx =b \\right\\} $$的集合称为 超平面, 形如\n$$ \\left\\{ x \\mid a^Tx \\le b \\right\\} $$的集合称为 半空间.\n定义\n满足线性等式和不等式组的点的集合称为 多面体, 即\n$$ \\left\\{x \\mid Ax \\le b, Cx = d\\right\\} $$其中 $A \\in \\mathbb{R}^{m \\times n}, C \\in \\mathbb{R}^{p \\times n}$.\n定义\n对中心 $x_c$ 和半径 $r$, 形如\n$$ B(x_c, r) = \\left\\{ x \\mid \\Vert x - x_c \\Vert \\le r \\right\\} = \\left\\{ x_c + ru \\mid \\Vert u \\Vert \\le 1 \\right\\} $$的集合称为 球.\n对中心 $x_c$ 和对称正定矩阵 $P$, 非奇异矩阵 $A$, 形如\n$$ \\left\\{ x \\mid (x-x_c)^TP(x-x_c) \\le 1 \\right\\} = \\left\\{ x_c + Au \\mid \\Vert u \\Vert \\le 1 \\right\\} $$的集合称为 椭球.\n定义\n形如\n$$ \\left\\{(x,t) \\mid \\Vert x \\Vert \\le t \\right\\} $$的集合称为 (范数)锥.\n保凸运算 定理\n仿射运算保凸, 即对 $f(x)=Ax+b$, 则凸集在 $f$ 下的像是凸集, 凸集在 $f$ 下的原像是凸集.\n考虑双曲锥\n$$ \\left\\{ x \\mid x^TPx \\le \\left( c^Tx \\right)^2, c^Tx \\ge 0, P \\in \\mathbb{S}_+^n \\right\\} $$$\\mathbb{S}_+^n$ 表示半正定矩阵. 双曲锥可以表示为二阶锥\n$$ \\left\\{ x \\mid \\Vert Ax \\Vert_2 \\le c^Tx, c^Tx \\ge 0, A^TA = P \\right\\} $$这个可以由二次范数锥得到.\n透视变换 $P: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n$:\n$$ P(x,t) = \\frac{x}{t}, \\quad \\text{dom} P = \\left\\{ (x,t) \\mid t \u003e 0 \\right\\} $$保凸.\n分式线性变换 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}^m$:\n$$ f(x) = \\frac{Ax+b}{c^Tx+d}, \\quad \\text{dom} f = \\left\\{ x \\mid c^Tx+d \u003e 0 \\right\\} $$保凸.\n广义不等式和对偶锥 定义\n我们称一个凸锥 $K \\subseteq \\mathbb{R}^n$ 为 适当锥, 当其还满足\n$K$ 是闭集; $K$ 是实心的, 即 $\\text{int} K \\neq \\emptyset$; $K$ 是尖的, 即内部不包含直线: 若 $x \\in \\text{int} K, -x \\in \\text{int} K$. 则一定有 $x = 0$. 例如\n非负卦限 $K=\\mathbb{R}_+^n=\\left\\{ x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\right\\}$ 是适当锥. 半正定锥 $K=\\mathbb{S}_+^n$ 是适当锥. $[0,1]$ 上的有限非负多项式 $K=\\left\\{ x \\in \\mathbb{R}^n \\mid x_1 + x_2t + \\cdots + x_nt^{n-1} \\ge 0, t \\in [0,1] \\right\\}$ 是适当锥. 可以在 适当锥 上定义广义不等式.\n定义\n对于适当锥 $K$ , 定义偏序 广义不等式 为\n$$x \\preceq_K y \\Leftrightarrow y - x \\in K$$严格版本:\n$$x \\prec_K y \\Leftrightarrow y - x \\in \\text{int} K$$ 广义不等式是一个偏序关系, 具有自反性, 反对称性, 传递性, 可加性, 非负缩放性, 不再赘述.\n定义\n令锥 $K$ 为全空间 $\\Omega$ 的子集, 则 $K$ 的 对偶锥 为\n$$ K^\\ast = \\left\\{ y \\mid \\left\u003c x, y \\right\u003e \\ge 0, \\forall x \\in K \\right\\} $$ 例如\n非负卦限是自对偶锥. 半正定锥是自对偶锥. 定理\n设 $K$ 是一锥, $K^\\ast$ 是其对偶锥, 则满足:\n$K^\\ast$ 是锥 (即使 $K$ 不是锥); $K^\\ast$ 是凸且闭的; 若 $\\text{int} \\neq \\emptyset$, 则 $K^\\ast$ 是尖的. 若 $K$ 是尖的, 则 $\\text{int} K^\\ast \\neq \\emptyset$. 若 $K$ 是适当锥, 则 $K^\\ast$ 是适当锥. $K^{\\ast\\ast}$ 是 $K$ 的凸包. 特别地, 若 $K$ 是凸且闭的, 则 $K^\\ast=K$. 适当锥的对偶锥仍是适当锥, 则适当锥 $K$ 的对偶锥 $K^\\ast$ 也可以诱导广义不等式.\n定义\n对于适当锥 $K$, 定义其对偶锥 $K^\\ast$ 上的 对偶广义不等式 为:\n$$x \\preceq_{K^\\ast} y \\Leftrightarrow y - x \\in K^\\ast$$其满足\n$x \\preceq_{K} y \\Leftrightarrow \\lambda^Tx \\le \\lambda^Ty, \\forall \\lambda \\succeq_{K^\\ast} K^\\ast$. $y \\succeq_{K^\\ast} 0 \\Leftrightarrow y^Tx \\ge 0, \\forall x \\succeq_K 0$. 分离超平面定理 定理分离超平面定理\n如果 $C$ 和 $D$ 是不相交的凸集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 分开, 即存在 $a \\neq 0, b$ 使得\n$$ \\begin{aligned} a^Tx \u0026\\le b, \\quad \\forall x \\in C \\\\ a^Tx \u0026\\ge b, \\quad \\forall x \\in D \\end{aligned} $$ 简要想法是找距离最近的一对点, 以这两点的中点为中心, 以两点的连线为法向量构造超平面.\n定理严格分离定理\n如果 $C$ 和 $D$ 是不相交的凸集, 且 $C$ 是闭集, $D$ 是紧集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 严格分开, 即存在 $a \\neq 0, b$ 使得\n$$ \\begin{aligned} a^Tx \u0026\\lt b, \\quad \\forall x \\in C \\\\ a^Tx \u0026\\gt b, \\quad \\forall x \\in D \\end{aligned} $$ 定义\n给定集合 $C$ 和边界点 $x_0$, 如果 $a\\ne 0$ 满足 $a^Tx \\le a^T x_0, \\forall x \\in C$, 则称\n$$ \\left\\{ x \\mid a^Tx = a^T x_0 \\right\\} $$为 $C$ 的 支撑超平面.\n由分离超平面的特殊情况 ($D$ 为单点集) 可以得到支撑超平面的存在性.\n定理支撑超平面定理\n若 $C$ 是凸集, 则 $C$ 的任意边界点处存在支撑超平面.\n","date":"2025-01-16T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-set/","title":"最优化方法(2) —— 凸集"},{"content":"概要 最优化问题的一般形式:\n$$ \\begin{aligned} \\min_{x} \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 g_i(x) \\leq 0, \\quad i = 1, 2, \\ldots, m \\\\ \u0026 h_j(x) = 0, \\quad j = 1, 2, \\ldots, p \\end{aligned} $$稀疏优化 考虑线性方程组 $Ax = b$, 优化函数 $\\min_{x \\in R^n} {\\Vert x \\Vert}_0, {\\Vert x \\Vert}_1, {\\Vert x \\Vert}_2$, 分别指代 $x$ 的非零元个数, $l_1, l_2$ 范数. LASSO(least absolute shrinkage and selection operator) 问题:\n$$ \\min_{x \\in \\mathbb{R}^n} \\mu {\\Vert x \\Vert}_1 + \\frac{1}{2} {\\Vert Ax - b \\Vert}_2^2 $$低秩矩阵优化 考虑矩阵 $M$, 希望 $X$ 在描述 $M$ 有效特征元素的同时, 尽可能保证 $X$ 的低秩性质. 低秩矩阵问题:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\text{rank}(X) \\quad \\text{s.t.} \\quad X_{ij} = M_{ij}, \\quad (i, j) \\in \\Omega $$核范数 ${\\Vert X \\Vert}_*$ 为所有奇异值的和. 也有二次罚函数的形式:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\mu {\\Vert X \\Vert}_* + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} (X_{ij} - M_{ij})^2 $$对于低秩情形, $X=LR^T$, 其中 $L \\in \\mathbb{R}^{m \\times r}, R \\in \\mathbb{R}^{n \\times r}$, $r \\ll m,n$ 为秩. 优化问题可写为:\n$$ \\min_{L,R} \\alpha {\\Vert L \\Vert}^2_F + \\beta {\\Vert R \\Vert}^2_F + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} ([LR^T]_{ij} - M_{ij})^2 $$引入正则化系数 $\\alpha, \\beta$ 来消除 $L,R$ 在常数缩放下的不确定性.\n深度学习 机器学习的问题通常形如\n$$ \\min_{x \\in W} \\frac{1}{N} \\sum_{i=1}^N \\ell(f(a_i, x), b_i) + \\lambda R(x) $$ 基本概念 定义\n设 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$, $x \\in \\mathbb{R}^n$ 的可行区域为 $S$. 若存在一个邻域 $N(x)$, 使得 $\\forall x \\in N(x) \\cap S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的局部极小点. 若 $\\forall x \\in S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的全局极小点.\n大多数的问题是不能显式求解的, 通常要使用迭代算法.\n定义\n称算法是 Q-线性收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} \\le a, \\quad a \\in (0, 1) $$称算法是 Q-超线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 0 $$称算法是 Q-次线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 1 $$称算法是 Q-二次收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert^2}} \\le a, \\quad a \u003e 0 $$ 定义\n设 $x_k$ 是迭代算法产生的序列且收敛到 $x^\\ast$, 如果存在 Q-线性收敛于 $0$ 的非负序列 $t_k$, 且\n$$ \\Vert x_k - x^\\ast \\Vert \\le t_k $$则称 $x_k$ 是 R-线性收敛 的.\n一般来说, 收敛准则可以是\n$$ \\frac{f(x_k) - f^\\ast}{\\max\\left\\{\\left|f^\\ast \\right|, 1\\right\\}} \\le \\varepsilon $$也可以是\n$$ \\nabla f(x_k) \\le \\varepsilon $$如果有约束要求, 还要同时考虑到约束违反度. 对于实际的计算机算法, 会设计适当的停机准则, 例如\n$$ \\frac{{\\Vert x_{k+1} - x_k \\Vert}}{\\max\\left\\{\\Vert x_k \\Vert, 1\\right\\}} \\le \\varepsilon $$","date":"2025-01-12T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/intro/","title":"最优化方法(1) —— 简介"}]