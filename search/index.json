[{"content":"本节课件链接\n特征的分类能力评估 定义\n给定数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 其中 $x_i=\\left(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(m)}\\right) \\in \\mathcal{X}$ 是第 $i$ 个样本的特征向量, $y_i \\in \\mathcal{Y}=\\{c_1,c_2,\\cdots,c_K\\}$ 是第 $i$ 个样本的标签. 假设数据集 $D$ 根据特征分成了 $K$ 个子集 $D_1,D_2,\\cdots,D_K$, 定义 经验熵 为\n$$ H(D) = -\\sum_{k=1}^K \\frac{|D_k|}{|D|} \\log_2 \\frac{|D_k|}{|D|} $$现在给定某维特征 $A$ 和其取值集合 $\\{a_1,a_2,\\cdots,a_m\\}$, 根据 $A$ 的取值将数据集 $D$ 分成了 $m$ 个子集 $D_1^A,D_2^A,\\cdots,D_m^A$, 并进一步考虑 $D_i^A$ 中的标签分布, 定义 条件经验熵 为\n$$ H(D|A) = \\sum_{i=1}^m \\frac{|D_i^A|}{|D|} H(D_i^A) $$ 如果条件经验熵和经验熵之差越大, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.\n定义\n属性 $A$ 对数据集 $D$ 的 信息增益 $g(D,A)$ 定义为\n$$ g(D,A) = H(D) - H(D|A) $$ 考虑到信息增益的计算会偏向于选择取值较多的特征, 为了避免这种情况, 引入信息增益率来评估特征的分类能力.\n定义\n特征 $A$ 的 分裂信息 $IV(A)$ 定义为\n$$ IV(A) = -\\sum_{i=1}^m \\frac{|D_i^A|}{|D|} \\log_2 \\frac{|D_i^A|}{|D|} $$特征 $A$ 的 信息增益率 $g_R(D,A)$ 定义为\n$$ g_R(D,A) = \\frac{g(D,A)}{IV(A)} $$ 分裂信息其实就是按照 $A$ 取值作划分的经验熵.\n除了信息增益和信息增益率, 还有 Gini 指数可以用来评估特征的分类能力.\n定义\n数据集 $D$ 的 Gini 指数 $\\text{Gini}(D)$ 定义为\n$$ \\text{Gini}(D) = 1 - \\sum_{k=1}^K \\left(\\frac{|D_k|}{|D|}\\right)^2 $$特征 $A$ 的 Gini 指数 $\\text{Gini}(D,A)$ 定义为\n$$ \\text{Gini}(D,A) = \\sum_{i=1}^m \\frac{|D_i^A|}{|D|} \\text{Gini}(D_i^A) $$如果按照特征 $A$ 是否取值为 $a_i$ 对数据集 $D$ 进行划分 $D=D_i^A \\cup (D-D_i^A)$, 则 $A=a_i$ 的 Gini 指数 $\\text{Gini}_d(D,A=a_i)$ 定义为\n$$ \\text{Gini}_d(D,A=a_i) = \\frac{|D_i^A|}{|D|} \\text{Gini}(D_i^A) + \\frac{|D-D_i^A|}{|D|} \\text{Gini}(D-D_i^A) $$ Gini 指数可以看作任取两个样本, 它们的标签不一致的概率. 如果 Gini 指数越小, 则说明特征 $A$ 对数据集 $D$ 的分类能力越强.\n决策树模型 生成决策树算法\n输入: 训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^N$, 特征集 $\\mathcal{A}=\\{A_1,A_2,\\cdots,A_m\\}$, 最优特征选择函数 $F$.\n输出: 决策树 $T$.\n若数据集 $D$ 中所有样本的标签都是 $c_k$, 则生成一个类标记为 $c_k$ 的叶结点, 返回 $T$; 若 $A=\\emptyset$, 且 $D$ 非空, 则生成一个单节点树, 并以 $D$ 中样本数最多的类标记作为该节点的类标记, 返回 $T$; 计算 $A^\\ast=F(D,\\mathcal{A})$; 对 $A^\\ast$ 的每一个取值 $a_i$, 构造一个对应于 $D_i$ 的子节点; 若 $D_i=\\emptyset$, 则将子节点标记为叶结点, 类标记为 $D$ 中样本数最多的类标记; 否则, 将 $D_i$ 中样本数最多的类标记作为该节点的类标记 对每个 $D_i$ 对应的非叶子节点, 以 $D_i$ 为训练集, 以 $\\mathcal{A}-\\{A^\\ast\\}$ 为特征集, 递归调用 1-6 步, 构建决策树 $T$. 如果以信息增益为特征选择函数, 即 $A^\\ast = \\arg\\max_{A \\in \\mathcal{A}} g(D,A)$, 则算法对应于 ID3 算法; 如果以信息增益率为特征选择函数, 即 $A^\\ast = \\arg\\max_{A \\in \\mathcal{A}} g_R(D,A)$, 则算法对应于 C4.5 算法.\n二路划分会采用以特征的可能取值为切分点的二分法划分当前数据集, 例如与选择 Gini 指数最小的特征和切分点对应的特征值, 即 $(A^\\ast,a^\\ast) = \\arg\\min_{A \\in \\mathcal{A},a \\in V(A)} \\text{Gini}_d(D,A=a)$, 则算法对应于 CART 算法.\n为了降低过拟合风险, 可以对决策树进行剪枝. 常用的是后剪枝, 即先生成一棵完全生长的决策树, 然后根据泛化性能决定是否剪枝. 也可以采用正则化方法, 例如, 定义决策树 $T$ 的损失或代价函数:\n$$ C_\\alpha(T) = C(T) + \\alpha |T| $$其中 $C(T)$ 用于衡量 $T$ 对 $D$ 的拟合程度, $|T|$ 表示 $T$ 的叶结点个数, $\\alpha \\geq 0$ 用于权衡拟合程度和模型复杂度.\nCART 算法有特别的剪枝处理: 从 CART 算法生成得到完整决策树 $T_0$ 开始, 产生一个递增的权衡系数序列 $0=\\alpha_0 \u003c \\alpha_1 \u003c \\cdots \u003c \\alpha_n \u003c +\\infty$ 和一个嵌套的子树序列 $\\{T_0, T_1, \\cdots, T_n\\}$, $T_i$ 为 $\\alpha \\in [\\alpha_i, \\alpha_{i+1})$ 时的最优子树, $T_n$ 是根节点单独构成的树.\n如果是连续特征, 则可以考虑将其离散化, 例如, 通过二分法将其划分为两个区间, 选择最优划分点.\n现在继续从经验风险的角度来看决策树模型.采用 $0-1$ 损失函数, 设节点 $t$ 设置的标记是 $c_k$, 则在 $t$ 对应的数据集上的经验风险为\n$$ \\frac{1}{|D_t|} \\sum_{i=1}^{|D_t|} I(y_i \\neq c_k) $$显见, 等价于\n$$ \\max_{c_k \\in \\mathcal{Y}} \\frac{1}{|D_t|} \\sum_{i=1}^{|D_t|} I(y_i = c_k) $$从现在来看, 决策树构造过程中划分的单元都是矩形的, 即分类边界是若干与特征坐标轴平行的边界组成. 多变量决策树模型允许用若干特征的线性组合来划分数据集, 对每个非叶结点学习一个线性分类器.\n","date":"2025-03-18T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/decision-tree/","title":"机器学习基础(5) —— 决策树模型"},{"content":"本节课件链接\nk-近邻算法 k-近邻算法的主要思想是, 对于一个给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点, 然后根据这 $k$ 个样本点的类别进行多数占优的投票方式来预测 $x$ 的类别.\n在 $n$ 维实数空间 $\\mathbb{R}$ 中, 通常用 Minkowski 距离来度量两个点 $x_i, x_j$ 的相似性:\n定义\n设 $x_i, x_j \\in \\mathbb{R}^n$, 则 $x_i, x_j$ 之间的 Minkowski 距离 $\\text{dist}_p(x_i,x_j)$ 定义为\n$$ \\text{dist}_p(x_i,x_j) = \\left( \\sum_{l=1}^n |x_i^l - x_j^l|^p \\right)^{1/p} $$ $p=1$ 时, 就是 Manhattan 距离; $p=2$ 时, 就是 Euclidean 距离; $p=\\infty$ 时, 就是 Chebyshev 距离. 在必要时, 还可以给每个维度的特征值加权.\n定义\n给定训练样本集 $D = \\{(x_i, y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_k\\}$, 以及距离度量 $\\text{dist}$, k-近邻算法 的基本步骤如下:\n基于度量 $\\text{dist}$, 对于给定的样本点 $x$, 找到训练集中与 $x$ 最近的 $k$ 个样本点所构成的邻域 $N_k^{\\text{dist}}(x)$;\n采用如下的多数投票规则来预测 $x$ 的类别:\n$$ y = \\arg\\max_{c_i} \\sum_{x_j \\in N_k^{\\text{dist}}(x)} I(y_j = c_i) $$ 如果把 0-1 作为损失函数, 那么 k-近邻算法实际上就是让经验风险最小化.\n最近邻算法 在 k-近邻算法中, 当 $k=1$ 时, 称为最近邻算法. 因此, 特点是偏差小, 方差大. 这其实是特征空间的一个划分 $\\mathcal{X}=\\bigcup_{i=1}^n \\{R_i\\}$. 对每个划分单元 $R_i$, 该单元的数据点到其他样本的距离都不会小于到 $x_i$ 的距离.\n最近邻算法的扩展 给定样本集 $D = \\{(x_i,y_i)\\}_{i=1}^n$, 以 $D_i$ 表示属于类 $c_i$ 的样本集, 希望找一个方式把每个 $D_i$ 分成 $k$ 个簇 $(D_{i1}, D_{i2}, \\cdots, D_{ik})$, 使得数据分布的方差最小, 即\n$$ (D^\\ast_{i1}, D^\\ast_{i2}, \\cdots, D^\\ast_{il}) = \\arg\\min_{D_{i1}, D_{i2}, \\cdots, D_{ik}} \\sum_{j=1}^k \\sum_{(x_t,y_t) \\in D_{ij}} \\Vert x_t-c_{ij} \\Vert_2^2 $$然而很难找到最优解, 因此采用迭代的方式来近似求解:\n定义\nK-means 算法 的基本步骤如下:\n初始化 $k$ 个簇的中心 $c_{ij}$; 对每个 $(x_t),(y_t) \\in D_i$ (即 $y_t=c_i$), 令 $$I_{x_t}= \\arg\\min_{j} \\Vert x_t-c_{ij} \\Vert_2^2$$ 即将 $x_t$ 分配到最近的簇; 对每个 $D_{ij}$, 更新均值 $$c_{ij} = \\frac{1}{|D_{ij}|} \\sum_{(x_t,y_t) \\in D_{ij}} x_t$$ 重复 2, 3 直到收敛. 有可能会使得某些离分类边界很近的点被错误分类. 引入学习向量量化方法 (LVQ 算法). 让同类和异类的点在构建过程中都能起作用.\n定义\nLVQ 算法 的基本步骤如下:\n对每个类 $c_m$ 随机选择 $k$ 个点 $I_{mi}$ 作为代表; 对每个样本点 $x_t$, 找到最近的代表元 $I_{m^\\ast i^\\ast}$, 即 $$I_{m^\\ast i^\\ast} = \\arg\\min_{m,i} \\Vert x_t - I_{mi} \\Vert_2^2$$ 如果 $y_t=c_{m^\\ast}$, 则 $$I_{m^\\ast i^\\ast} \\gets I_{m^\\ast i^\\ast} + \\eta(x_t - I_{m^\\ast i^\\ast})$$ 否则 $$I_{m^\\ast i^\\ast} \\gets I_{m^\\ast i^\\ast} - \\eta(x_t - I_{m^\\ast i^\\ast})$$ 重复 2, 3 直到收敛. 这里 $\\eta$ 是学习率.\n在 $\\eta=1$ 时, LVQ 算法相当于逐步地进行 k-means 算法.\n在最近邻算法和其扩展方法中, 每个簇的代表点也称为相应单元的原型. 这种方法也常被称作原型方法或免模型方法.\n","date":"2025-03-14T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/knn/","title":"机器学习基础(4) —— 基于近邻的分类方法"},{"content":"本节课件链接\n后验概率最大化准则 定义\n对训练样本集 $D=\\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathcal{X}$, $y_i \\in \\mathcal{Y} = \\{c_1, c_2, \\cdots, c_K\\}$, 将 $x$ 的类别预测为 $c_i$ 的 风险 为\n$$ R(Y=c_i | x) = \\sum_{j=1}^K \\lambda_{ij} P(Y=c_j | x) $$其中 $\\lambda_{ij}$ 是将属于 $c_j$ 的样本预测为 $c_i$ 的损失. 最优预测 $\\hat{y}$ 是使得风险最小的类别, 即\n$$ \\hat{y} = \\arg\\min_{c_i} R(Y=c_i | x) $$ 假设采用 $0-1$ 损失函数, 易知\n$$ R(Y=c_i | x) = 1 - P(Y=c_i | x) $$即输入 $x$ 的最优预测 $\\hat{y}$ 为使得后验概率 $P(y | x)$ 最大的类别.\n逻辑斯蒂回归模型 定义\n设 $\\mathcal{X}=\\mathbb{R}^n, \\mathcal{Y}=\\{c_1,c_2\\}$, 逻辑斯蒂回归模型是如下的后验概率分布:\n$$ \\begin{aligned} P(Y=c_1 | x) \u0026= \\frac{\\exp(w \\cdot x + b)}{1+\\exp(w \\cdot x + b)} \\\\ P(Y=c_2 | x) \u0026= \\frac{1}{1+\\exp(w \\cdot x + b) } \\end{aligned} $$其中 $w,b$ 是模型参数.\n按照后验概率最大化准则, 显然当 $w \\cdot x + b \u003e 0$ 时, 预测为 $c_1$, 否则预测为 $c_2$.\n对于多类分类任务, 仍然可以使用逻辑斯蒂回归模型:\n$$ \\begin{aligned} p(y=c_i | x) \u0026= \\frac{\\exp(w_i \\cdot x + b_i)}{\\sum_{j=1}^{K-1} \\exp(w_j \\cdot x + b_j)}, \\quad i=1,2,\\cdots,K-1 \\\\ p(y=c_K | x) \u0026= \\frac{1}{\\sum_{j=1}^{K-1} \\exp(w_j \\cdot x + b_j)} \\end{aligned} $$给定 $D=\\{(x_i,y_i)\\}_{i=1}^n$, 其中 $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathcal{Y} = \\{0,1\\}$, 用 $\\theta=(w,b)$ 表示二项逻辑斯蒂回归模型的参数, 令\n$$ p(x;\\theta) = p(Y=1 | x;\\theta) $$则考虑似然函数为\n$$ \\begin{aligned} L(\\theta) \u0026= \\prod_{i=1}^n p(x_i;\\theta)^{y_i} (1-p(x_i;\\theta))^{1-y_i} \\\\ \\log L(\\theta) \u0026= \\sum_{i=1}^n y_i \\log p(x_i;\\theta) + (1-y_i) \\log (1-p(x_i;\\theta)) \\\\ \u0026= \\sum_{i=1}^N y_i(w \\cdot x_i + b) - \\log(1+\\exp(w \\cdot x_i + b)) \\end{aligned} $$对 $w,b$ 求偏导为 $0$, 得到\n$$ \\begin{aligned} \\frac{\\partial \\log L(\\theta)}{\\partial w} \u0026= \\sum_{i=1}^n x_i(y_i - p(x_i;\\theta)) = 0\\\\ \\frac{\\partial \\log L(\\theta)}{\\partial b} \u0026= \\sum_{i=1}^n (y_i - p(x_i;\\theta)) = 0 \\end{aligned} $$朴素 Bayers 分类器 定理Bayers 公式\n$$ \\begin{aligned} P(Y=c_i | x) \u0026= \\frac{P(x | Y=c_i) P(Y=c_i)}{P(x)} \\\\ \u0026= \\frac{P(x | Y=c_i) P(Y=c_i)}{\\sum_{j=1}^K P(x | Y=c_j) P(Y=c_j)} \\end{aligned} $$ 朴素 Bayers 假定特征之间相互独立, 即\n$$ p(X^1=x^1, X^2=x^2, \\cdots, X^n=x^n | Y=c_k) = \\prod_{j=1}^n p(X^j=x^j | Y=c_k) $$对于输入实例 $x=(x^1,x^2,\\cdots,x^n)$, 则后验概率\n$$ p(Y=c_k|x)=\\frac{\\left( \\prod_{i=1}^n p(X^i=x^i | Y=c_k) \\right) P(Y=c_k)}{\\sum_{j=1}^K \\left( \\prod_{i=1}^n p(X^i=x^i | Y=c_j) \\right) P(Y=c_j)} $$分母是固定的, 只需比较分子的大小即可. 但是一旦某个特征取值和分类没有同时出现, 后验概率直接为 $0$, 为了避免这种情况, 通常引入一些平滑技术:\n$$ p_{\\lambda}(Y=c_k) = \\frac{\\sum_{j=1}^NI(y_j=c_k)+\\lambda}{N+K\\lambda} $$$\\lambda=1$ 时称为 Laplace 平滑.\n","date":"2025-03-11T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/bayers/","title":"机器学习基础(3) —— 基于后验概率最大化准则的分类模型"},{"content":"本节课件链接\n虚函数 在类的定义中, 前面有 virtual 关键字的成员函数就是 虚函数. virtual 只用在类定义里的函数声明中, 写函数体时不用.\n派生类的指针可以赋给基类指针. 通过基类指针调用基类和派生类中的同名同参 虚 函数时:\n若该指针指向一个基类的对象, 那么被调用是基类的虚函数; 若该指针指向一个派生类的对象, 那么被调用的是派生类的虚函数. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class CBase { public: virtual void someVirtualFunction() { cout \u0026lt;\u0026lt; \u0026#34;base function\u0026#34; \u0026lt;\u0026lt; endl; } }; class CDerived : public CBase { public: virtual void someVirtualFunction() { cout \u0026lt;\u0026lt; \u0026#34;derived function\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { CDerived ODerived; CBase *p = \u0026amp;ODerived; p-\u0026gt;someVirtualFunction(); // derived function return 0; } 派生类的对象可以赋给基类引用. 类似于指针, 通过基类引用调用基类和派生类中的同名同参虚函数也是多态的.\n1 2 3 4 5 6 int main() { CDerived ODerived; CBase \u0026amp;r = ODerived; r.someVirtualFunction(); // derived function return 0; } 多态不能针对对象. 派生类中的虚函数的访问权限可以是 public, protected, private. 但是, 基类中的虚函数的访问权限不能是 private, 即使派生类中的虚函数的访问权限是 public. 反过来是可以的, 而且可以正常多态.\n实现原理 采用了动态联编的技巧. 每一个有虚函数的类 (或其派生类) 都有一个虚函数表，该类的任何对象中都放着虚函数表的指针. 虚函数表中列出了该类的虚函数地址. 多出来的 4 个字节就是用来放虚函数表的地址的. 在编译时, 调用语句被编译成对虚函数表的索引, 而不是对函数的直接调用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Base { public: int i; virtual void Print() { cout \u0026lt;\u0026lt; \u0026#34;Base:Print\u0026#34; \u0026lt;\u0026lt; endl; } }; class Derived : public Base { public: int n; virtual void Print() { cout \u0026lt;\u0026lt; \u0026#34;Drived:Print\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { Derived d; cout \u0026lt;\u0026lt; sizeof(Base) \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; sizeof(Derived); return 0; } 32 位系统下, 这个程序的输出是 8,12.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class A { public: virtual void func() { cout \u0026lt;\u0026lt; \u0026#34;A::func \u0026#34;; } }; class B : public A { public: virtual void func() { cout \u0026lt;\u0026lt; \u0026#34;B::func \u0026#34;; } }; int main() { A a; A *pa = new B(); pa-\u0026gt;func(); // 多态, B::func // 64 位程序 long long *p1 = (long long *)\u0026amp;a; long long *p2 = (long long *)pa; // 篡改了虚函数表指向 *p2 = *p1; pa-\u0026gt;func(); // A::func return 0; } 虚析构函数 在非构造/析构函数中调用虚函数时, 调用的是当前对象的虚函数, 而不是基类的虚函数, 是多态. 在构造/析构函数中调用虚函数时, 调用的是当前类的虚函数, 编译时确定, 不是多态.\n通过基类的指针删除派生类对象时，通常情况下只调用基类的析构函数. 为了解决这个问题, 可以把基类的析构函数声明为虚函数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class son { public: virtual ~son() { cout \u0026lt;\u0026lt; \u0026#34;bye from son\u0026#34; \u0026lt;\u0026lt; endl;} }; class grandson : public son { public: ~grandson() { cout \u0026lt;\u0026lt; \u0026#34;bye from grandson\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { son *pson; pson = new grandson(); delete pson; return 0; } 此时, 会先调用派生类的析构函数, 再调用基类的析构函数. 另外注意, 构造函数不能是虚函数.\n纯虚函数和抽象类 如果在虚函数后面加上 = 0, 则该虚函数是纯虚函数. 纯虚函数不可以有函数体, 只有声明.\n1 2 3 4 5 class A { public: virtual void print() = 0; // 纯虚函数 void fun() { cout \u0026lt;\u0026lt; \u0026#34;fun\u0026#34;; } } 一个类中有纯虚函数的类叫 抽象类. 抽象类不能实例化, 只能作为基类, 不过可以作为指针或引用类型. 在抽象类的成员函数内可以调用纯虚函数, 但是在构造函数或析构函数内部不能调用纯虚函数.\n","date":"2025-03-07T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/polymorphism/","title":"程序设计实习(4) —— 多态"},{"content":"本节课件链接\n基本概念 在定义一个新的类 B 时, 如果 B 类与拥有已有类 A 的全部特点, 那么就可以把 A 作为一个基类, 而把 B 作为基类的一个 派生类 (也称 子类). 派生类可以对基类:\n扩充: 在派生类中, 可以添加新的成员变量和成员函数 修改: 在派生类中, 可以重新编写从基类继承得到的成员 派生类一经定义后, 可以独立使用, 不依赖于基类.\n派生方式说明符：public, private, protected. 派生类的写法:\n1 2 3 class Derived : public Base { // code }; 关于内存上, 派生类对象的大小等于基类对象的大小加上派生类对象自己的成员变量的大小. 在派生类对象中包含着基类对象, 且基类对象的存储位置位于派生类对象新增的成员变量之前.\n覆盖 派生类可以定义一个和基类成员同名的成员, 这叫覆盖. 在派生类中访问这类成员时, 缺省的情况是访问派生类中定义的成员. 要在派生类中访问由基类定义的同名成员时, 要使用作用域符号::.\n注意, 对于成员变量, 在内存中, 两个变量占用不同的空间. 并不建议覆盖成员变量.\n关于权限及派生方式的访问权限, 可以参考下表:\n派生方式\\成员访问 public protected private public public protected 不可访问 protected protected protected 不可访问 private private private 不可访问 为此, 可以用基类构造函数初始化派生类对象的基类部分. 例如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Bug { private: int nLegs; int nColor; protected: int nType; public: Bug(int legs, int color); void PrintBug() {} }; class FlyBug : public Bug { int nWings; public: FlyBug(int legs, int color, int wings); }; Bug::Bug(int legs, int color) { nLegs = legs; nColor = color; } FlyBug::FlyBug(int legs, int color, int wings) : Bug(legs, color) { nType = 1; nWings = wings; } 派生类的对象生命周期 在创建派生类的对象时, 需要调用基类的构造函数：初始化派生类对象中从基类继承的成员. 在执行一个派生类的构造函数之前, 总是先执行基类的构造函数. 派生类的析构函数被执行时, 执行完派生类的析构函数后, 自动调用基类的析构函数.\n在创建派生类的对象时:\n先执行基类的构造函数, 用以初始化派生类对象中从基类继承的成员; 再执行成员对象类的构造函数, 用以初始化派生类对象中成员对象; 最后执行派生类自己的构造函数. 在派生类对象消亡时：\n先执行派生类自己的析构函数; 再依次执行各成员对象类的析构函数; 最后执行基类的析构函数. 总之, 析构函数的调用顺序与构造函数的调用顺序相反.\n对于 public 继承具有赋值兼容规则, 即\n1 2 3 4 class Base {}; class Derived : public Base {}; Base b; Derived d; 派生类对象可以赋值给基类对象\n1 b = d; 派生类对象的地址可以赋值给基类指针\n1 Base *pb = \u0026amp;d; 派生类对象的引用可以赋值给基类引用\n1 Base \u0026amp;rb = d; 注意, 只有 public 继承才具有赋值兼容规则, protected 和 private 不允许这种赋值方式.\n指针强转成基类后, 就不再能访问派生类的成员了. 如果需要, 可以再利用指针强转回来.\n1 2 3 Derived objDerived; Base *ptrBase = \u0026amp;objDerived; Derived *ptrDerived = (Derived *)ptrBase; 在声明派生类时，只需要列出它的直接基类, 派生类沿着类的层次自动向上继承它的间接基类. 构造的顺序即为派生类的继承顺序, 析构的顺序则相反.\n","date":"2025-03-05T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/inherit/","title":"程序设计实习(3) —— 继承与派生"},{"content":"本节课件链接\n线性可分支持向量机 定义\n对于一个数据集 $D$, 如果能找到一个超平面 $H: w^Tx + b = 0$, 将数据分为两类. 即对任意 $(x_i, y_i) \\in D$, 若 $y_i = 1$, 则 $w^Tx_i + b \\geq 0$; 若 $y_i = -1$, 则 $w^Tx_i + b \u003c 0$. 则称 $D$ 是 线性可分的 , 超平面 $H$ 是 $D$ 的一个 分离超平面.\n最优超平面不仅要能够将数据分开, 还要使得两类数据点到超平面的距离尽可能远.\n考虑到 $w,b$ 任意缩放都不影响超平面的位置, 我们可以规定 $w^Tx + b = 1$ 为最近的正类数据点满足的方程. 此时距离为 $1/{\\|w\\|}$, 要最大化这个量, 即化归成凸二次规划问题:\n$$ \\begin{aligned} \u0026 \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1, \\quad i = 1, 2, \\cdots, n \\end{aligned} $$只要 $D$ 是线性可分的, 上述问题一定有解且唯一. 对应的分类决策函数\n$$ f(x) = \\text{sign}(w^Tx + b) $$称为 线性可分支持向量机.\n引入 Lagrange 乘子 $\\alpha_i \\geq 0$:\n$$ L(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b) - 1) $$对 $w, b$ 求偏导为 $0$, 得到\n$$ \\begin{aligned} \u0026 w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\ \u0026 0 = \\sum_{i=1}^n \\alpha_i y_i \\end{aligned} $$代入 $L(w, b, \\alpha)$, 得到对偶问题:\n线性可分对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j \\\\ \u0026 \\text{s.t.} \\quad \\alpha_i \\geq 0, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 由 KKT 条件, 最优解一定满足\n$$ \\begin{aligned} \\alpha_i(y_i(w \\cdot x_i + b) - 1) \u0026= 0 \\\\ y_i(w \\cdot x_i + b) - 1 \u0026\\geq 0 \\\\ \\alpha_i \u0026\\geq 0 \\\\ \\end{aligned} $$由于 $\\alpha_i$ 不全为 $0$, 存在 $j$ 使得 $y_j(w \\cdot x_j + b) = 1$, 由此\n$$ b = y_j - w \\cdot x_j = y_j - \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x_j $$乘上 $\\alpha_jy_j$ 做累和, 有\n$$ 0=\\sum_{j=1}^n \\alpha_jy_jb = \\sum_{j=1}^n \\alpha_j - \\| w \\|^2 $$上式中 $\\alpha_i=0$ 的 $i$ 也成立, 因为都是 $0$ 不影响结果. 注意到 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$ 也只收到 $\\alpha_i \u003e 0$ 的影响, 而这些项的点都落在间隔边界\n$$ H_1: w \\cdot x + b = 1, \\quad H_2: w \\cdot x + b = -1 $$上, 称这些点 $x_i$ 为 支持向量.\n支持向量机的留一误差\n$$ \\hat{R}_{\\text{loo}} = \\frac{1}{n} \\sum_{i=1}^n I(f_{D-\\{x_i\\}}(x_i) \\neq y_i) $$则 $\\hat{R}_{\\text{loo}} \\le N_{SV}/n$, 其中 $N_{SV}$ 为支持向量的个数.\n线性支持向量机 要求 $D$ 线性可分有点苛刻. 容忍一些误差, 引入松弛变量 $\\xi_i \\geq 0$, 使得约束条件变为\n$$ y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i $$对于被错误分类的点, $\\xi_i$ 可以大于 $1$. 把 $\\xi_i \\ne 0$ 的点视为特异点, 那么希望特异点尽可能少, 于是优化目标变为\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n I(\\xi_i \\ne 0) \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$直接用 $\\xi_i$ 代替 $I(\\xi_i \\ne 0)$, 问题变为\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$既然要 $\\xi_i$ 尽可能小, 不妨取 $\\xi_i = 1 - y_i(w \\cdot x_i + b)$, 引入合页损失函数 $h(z) = \\max(0, 1-z)$, 即\n$$\\xi_i = h(y_i(w \\cdot x_i + b))$$则提出一个 $C$ 后, 优化目标变为\n$$ \\min_{w, b} \\frac{1}{2C} \\|w\\|^2 + \\sum_{i=1}^n h(y_i(w \\cdot x_i + b)) $$做了这么多, 只是相当于把 0-1 损失函数换成了合页损失函数.\n回到原问题, 引入 Lagrange 乘子 $\\alpha_i, \\beta_i \\geq 0$, 得到\n$$ L(w, b, \\xi, \\alpha, \\beta) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b) - 1 + \\xi_i) - \\sum_{i=1}^n \\beta_i \\xi_i $$对 $w, b, \\xi$ 偏导为 $0$, 得到\n$$ \\begin{aligned} \u0026 w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\ \u0026 0 = \\sum_{i=1}^n \\alpha_i y_i \\\\ \u0026 \\beta_i = C - \\alpha_i \\end{aligned} $$代入 $L(w, b, \\xi, \\alpha, \\beta)$, 得到对偶问题\n线性支持向量机对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 与线性可分支持向量机类似, 只是多了一个 $\\alpha_i \\leq C$ 的约束. 现在考虑 KKT 条件, 有\n$$ \\begin{aligned} \\alpha_i(y_i(w \\cdot x_i + b) - 1 + \\xi_i) \u0026= 0 \\\\ y_i(w \\cdot x_i + b) - 1 + \\xi_i \u0026\\geq 0 \\\\ \\beta_i \\xi_i \u0026= 0 \\\\ \\alpha_i \u0026\\geq 0 \\\\ \\beta_i \u0026\\geq 0 \\\\ \\alpha_i + \\beta_i\u0026=C \\end{aligned} $$则 $\\alpha_i \u003e 0$ 的点 $x_i$ 为支持向量, 满足 $y_i(w \\cdot x_i + b) = 1 - \\xi_i$. 这点与线性可分支持向量机的支持向量不同. 但进一步如果 $\\alpha_i \\lt C$ , 则 $\\beta_i \\gt 0$, 则 $\\xi_i=0$, 从而 $y_i(w \\cdot x_i + b) = 1$, 这样就一致了.\n进一步, 把 $y_i(w \\cdot x_i + b) = 1$ 两边乘 $y_i$, 类似有\n$$ b = y_j - \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x_j $$因而最优分类超平面为\n$$ \\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x + b = 0 $$和决策函数\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i x_i \\cdot x + b\\right) $$超平面法向量可以被唯一确定, 但是偏置不唯一.\nSMO 算法 SMO 算法是一种启发式算法, 用于求解支持向量机的对偶问题. SMO 算法的基本思想是: 每次选择两个变量, 固定其他变量, 优化这两个变量. 这样不断迭代, 直到收敛.\n设当前迭代的两个变量为 $\\alpha_i, \\alpha_j$, 则\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = -\\sum_{i=3}^n \\alpha_i y_i $$同乘 $y_1$, 有\n$$ \\alpha_1 + \\alpha_2 y_1y_2= -\\sum_{i=3}^n \\alpha_i y_1y_i $$记右边为 $\\gamma$, $s=y_1y_2 \\in \\{-1, 1\\}$, 则\n$$ \\alpha_1 + s\\alpha_2 = \\gamma $$记$K_{ij} = x_i \\cdot x_j$, $v_i = \\sum_{j=3}^{N} \\alpha_j y_j K_{ij}$, 则对偶问题转化为\n$$ \\begin{aligned} \u0026 \\max_{\\alpha_1, \\alpha_2} \\alpha_1 + \\alpha_2 - \\frac{1}{2} K_{11}\\alpha_1^2 - \\frac{1}{2} K_{22}\\alpha_2^2 - sK_{12}\\alpha_1\\alpha_2 - y_1v_1\\alpha_1 - y_2v_2\\alpha_2 \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\alpha_1 + s\\alpha_2 = \\gamma \\end{aligned} $$再由 $\\alpha_1 = \\gamma - s\\alpha_2$, 代入目标函数, 并对 $\\alpha_2$ 求导为 $0$, 得到\n$$ \\alpha_2 = \\frac{s(K_{11}-K_{12})\\gamma + y_2(v_1 - v_2) - s + 1}{K_{11} + K_{22} - 2K_{12}} $$代入 $v$ 的定义, 随后化简得\n$$ \\alpha_2 = \\alpha_2^* + y_2 \\frac{(y_2 - f(x_2))- (y_1-f(x_1))}{K_{11} + K_{22} - 2K_{12}} $$别忘了约束 $0 \\le \\alpha_1, \\alpha_2 \\le C$, 以及 $\\alpha_1 + s\\alpha_2 = \\gamma$, 对 $\\alpha_2$ 进行裁剪为 $\\alpha_2^{\\text{clip}}$. 相应地,\n$$ \\alpha_1 = \\alpha_1^* + s(\\alpha_2^* - \\alpha_2^{\\text{clip}}) $$最后, 更新 $b$. 假设在 $\\alpha_1, \\alpha_2$ 中, $0 \\lt \\alpha_i \\lt C$, 则\n$$ b = y_i - \\sum_{j=1}^n \\alpha_j y_j K_{ij} $$关于选取 $\\alpha_1, \\alpha_2$, 一般有两个原则:\n选择违反 KKT 条件最严重的两个变量. 选择两个变量使得目标函数有最大变化. 核方法和非线性支持向量机 对于非线性问题, 可以通过核方法将数据映射到高维空间, 从而在高维空间中找到一个线性超平面.\n假设有一个映射 $\\phi: \\mathcal{X} \\mapsto \\mathcal{Z}$, 则在 $\\mathcal{Z}$ 的线性支持向量机变为:\n$$ \\begin{aligned} \u0026 \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\ \u0026 \\text{s.t.} \\quad y_i(w \\cdot \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\end{aligned} $$对应的对偶问题为\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\phi(x_i) \\cdot \\phi(x_j) \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$相应的分类决策函数为\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) \\cdot \\phi(x) + b\\right) $$然而, 直接计算 $\\phi(x_i) \\cdot \\phi(x_j)$ 的复杂度很高. 为此, 引入核函数\n定义\n设 $\\mathcal{X}$ 是输入空间, $\\mathcal{Z}$ 是特征空间, 如果存在一个从 $\\mathcal{X}$ 到 $\\mathcal{Z}$ 的映射 $\\phi$, 使得对任意 $x, x' \\in \\mathcal{X}$, 都有\n$$ K(x, x') = \\phi(x) \\cdot \\phi(x') $$则称 $K$ 为 核函数.\n注意, 这里我们不再需要显式地计算 $\\phi(x_i)$, 因为结果只与 $K(x_i, x_j)$ 有关.\n非线性支持向量机对偶问题\n$$ \\begin{aligned} \u0026 \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) \\\\ \u0026 \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{aligned} $$ 此时, 分类决策函数为\n$$ f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\\right) $$ 定义\n$\\mathcal{X}$ 上的函数 $K: \\mathcal{X} \\times \\mathcal{X} \\mapsto \\mathbb{R}$ 称为 正定对称核函数, 如果对任意 $x_1, x_2, \\cdots, x_n \\in \\mathcal{X}$, 核矩阵 (Gram 矩阵) $[K_{ij}]_{m \\times m}$ 是半正定的.\n常见的核函数有:\n线性核函数: $K(x, x') = x \\cdot x'$, 对应线性支持向量机. 多项式核函数: $K(x, x') = (x \\cdot x' + 1)^d, c \\gt 0$ 高斯核函数: $K(x, x') = \\exp\\left(-\\frac{\\|x-x'\\|^2}{2\\sigma^2}\\right), \\sigma \\gt 0$ 本节作业链接\n","date":"2025-02-28T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/vector-machine/","title":"机器学习基础(2) —— 支持向量机"},{"content":"本节课件链接\n对象间的运算和结构变量一样, 对象之间可以用 = 进行赋值, 但是不能用 ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;= 进行比较, 除非这些运算符经过了 \u0026ldquo;重载\u0026rdquo;. 运算符重载的实质是函数重载.\n重载为普通函数和成员函数均可. 重载为成员函数时, 重载函数的参数个数比运算符的操作数少一个.\n1 2 3 4 5 6 7 8 9 10 11 12 class Complex { public: double real, imag; Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} Complex operator-(const Complex \u0026amp;c); }; Complex operator+(const Complex \u0026amp;a, const Complex \u0026amp;b) { return Complex(a.real + b.real, a.imag + b.imag); } Complex Complex::operator-(const Complex \u0026amp;c) { return Complex(real - c.real, imag - c.imag); }; 重载 = 有时候希望赋值运算符两边的类型可以不匹配, 比如把一个 char * 类型的字符串赋值给一个字符串对象, 此时就需要重载赋值运算符 =. 赋值运算符 = 只能重载为成员函数, 通常返回类型为 *this 的引用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class String { char *str; public: String() : str(new char[1]) { str[0] = 0; } const char *c_str() { return str; } String \u0026amp;operator=(const char *s); ~String() { delete[] str; } }; String \u0026amp;String::operator=(const char*s) { delete[] str; str = new char[strlen(s) + 1]; strcpy(str, s); return *this; } int main() { String s; s = \u0026#34;Good Luck,\u0026#34;; // s.operator=(\u0026#34;Good Luck,\u0026#34;); cout \u0026lt;\u0026lt; s.c_str() \u0026lt;\u0026lt; endl; // String s2 = \u0026#34;hello!\u0026#34;; // 这是构造函数, 不是赋值 return 0; } 如不定义自己的赋值运算符, 那么 S1 = S2 实际上导致 S1.str 和 S2.str 指向同一地方. 为此要做深拷贝.\n1 2 3 4 5 6 7 8 9 10 11 12 13 String \u0026amp;operator=(const String \u0026amp;s) { // 防止自赋值导致问题 if (this == \u0026amp;s) return *this; delete[] str; str = new char[strlen(s.str) + 1]; strcpy(str, s.str); return *this } String(const String \u0026amp;s) { // 复制构造函数也要深拷贝 str = new char[strlen(s.str) + 1]; strcpy(str, s.str); } 也可以重载为友元函数.\n1 2 3 4 5 6 7 8 9 class Complex{ double real, imag; public: Complex(double r, double i) : real(r), imag(i) {} Complex operator+(double r); }; Complex Complex::operator+(double r) { return Complex(real + r, imag); } 这个可以解决 c = a + 2.5 的问题, 但是不能解决 c = 2.5 + a 的问题. 为此要重载为普通函数, 但普通函数又不能访问私有成员, 用友元函数.\n1 2 3 4 5 6 7 8 Complex operator+(double r, const Complex \u0026amp;c); class Complex { // --skip-- friend Complex operator+(double r, const Complex \u0026amp;c); }; Complex operator+(double r, const Complex \u0026amp;c) { return Complex(c.real + r, c.imag); } 重载流插入运算符 \u0026lt;\u0026lt; 和流提取运算符 \u0026gt;\u0026gt; cout 实际上是一个在 iostream 中定义的, ostream 类的对象.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Complex { double real, imag; public: Complex(double r = 0, double i = 0) : real(r), imag(i) {} friend ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Complex \u0026amp;c); friend istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;is, Complex \u0026amp;c); }; ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Complex \u0026amp;c) { os \u0026lt;\u0026lt; c.real \u0026lt;\u0026lt; \u0026#34;+\u0026#34; \u0026lt;\u0026lt; c.imag \u0026lt;\u0026lt; \u0026#34;i\u0026#34;; // 以\u0026#34;a+bi\u0026#34; 的形式输出 return os; } istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;is, Complex \u0026amp;c) { string s; is \u0026gt;\u0026gt; s; // 将 \u0026#34;a+bi\u0026#34; 作为字符串读入 int pos = s.find(\u0026#34;+\u0026#34;, 0); string sTmp = s.substr(0, pos); // 分离出代表实部的字符串 c.real = atof(sTmp.c_str()); // atof 库函数能将 const char* 指针指向的内容转换成 float sTmp = s.substr(pos + 1, s.length() - pos - 2); // 分离出代表虚部的字符串 c.imag = atof(sTmp.c_str()); return is; } 重载类型转换运算符 没有返回值, 因为转换函数的返回值类型就是要转换的类型.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { double real, imag; public: Complex(double r = 0, double i = 0) : real(r), imag(i) {} operator double() { return real; } }; int main() { Complex c(1.2, 3.4); cout \u0026lt;\u0026lt; (double)c \u0026lt;\u0026lt; endl; // 输出 1.2 double n = 2 + c; // double n = 2 + c.operator double() cout \u0026lt;\u0026lt; n; // 输出 3.2 return 0; } 重载自增自减运算符 ++ 和 -- 自增运算符 ++, 自减运算符 -- 有前置/后置之分, 为了区分所重载的是前置运算符还是后置运算符, C++ 规定前置运算符作为一元运算符重载, 后置运算符作为二元运算符重载, 多写一个没用的参数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 前置, 先加减再返回, 返回引用 // 重载为成员函数 T\u0026amp; operator++(); T\u0026amp; operator--(); // 重载为全局函数 T\u0026amp; operator++(T\u0026amp;); T\u0026amp; operator--(T\u0026amp;) // 后置, 先返回再加减, 返回值 // 重载为成员函数 T operator++(int); T operator--(int); // 重载为全局函数 T operator++(T\u0026amp;, int); T operator--(T\u0026amp;, int); 重载 -\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class A { private: int x; public: A() : x(5) {} int getX() { return x; } A *operator-\u0026gt;() { return this; } }; int main() { A a; cout \u0026lt;\u0026lt; a-\u0026gt;getX() \u0026lt;\u0026lt; endl; // a.operator-\u0026gt;()-\u0026gt;getX() return 0; } 看起来似乎没有什么用, 但是可以用来实现智能指针.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class client { public: int a; client(int x) : a(x) {} }; class proxy { client *target; public: proxy(client *t) : target(t) {} client *operator-\u0026gt;() const { return target; } }; class proxy2 { proxy *target; public: proxy2(proxy *t) : target(t) {} proxy \u0026amp;operator-\u0026gt;() const { return *target; } }; int main() { client x(3); proxy y(\u0026amp;x); proxy2 z(\u0026amp;y); cout \u0026lt;\u0026lt; x.a \u0026lt;\u0026lt; y-\u0026gt;a \u0026lt;\u0026lt; z-\u0026gt;a; // print \u0026#34;333\u0026#34; return 0; } 注意事项 运算符重载不改变运算符的优先级.\n以下运算符不能重载: ., .*(成员函数指针), ::, ?:(三目运算符), sizeof.\n重载运算符 (), [], -\u0026gt; 或者赋值运算符 = 时，运算符重载函数必须声明为类的成员函数.\n重载运算符是为了让它能作用于对象, 因此重载运算符不允许操作数都不是对象 (有一个是枚举类型也可以).\n1 void operator+(int a, char* b); // 错误 ","date":"2025-02-26T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/operator-overload/","title":"程序设计实习(2) —— 运算符重载"},{"content":"本节课件链接1 本节课件链接2 本节课件链接3\n面向对象的程序设计 面向对象的程序设计方法:\n将某类客观事物共同特点 (属性) 归纳出来, 形成一个数据结构 (可以用多个变量描述事物的属性); 将这类事物所能进行的行为也归纳出来, 形成一个个函数, 这些函数可以用来操作数据结构. 面向对象的特点有 抽象, 封装, 继承, 多态.\n一般来说, 对象所占用的内存空间的大小, 等于所有成员变量的大小之和.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class CRectangle { public: int w, h; int Area() { return w * h; } int Perimeter() { return 2 * (w + h); } void Init(int w_, int h_) { w = w_; h = h_; } }; // 必须有分号 int main() { int w, h; CRectangle r; // r 是一个对象 cin \u0026gt;\u0026gt; w \u0026gt;\u0026gt; h; r.Init(w, h); cout \u0026lt;\u0026lt; r.Area() \u0026lt;\u0026lt; endl \u0026lt;\u0026lt; r.Perimeter(); return 0; } 和结构变量一样, 对象之间可以用 = 进行赋值, 但是不能用 ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;= 进行比较, 除非这些运算符经过了 重载.\n对象名.成员名 1 2 3 CRectangle r1, r2; r1.w = 5; r2.Init(5, 4); 指针-\u0026gt;成员名 1 2 3 4 5 CRectangle r1, r2; CRectangle *p1 = \u0026amp;r1; CRectangle *p2 = \u0026amp;r2; p1-\u0026gt;w = 5; p2-\u0026gt;Init(5, 4); 引用名.成员名 1 2 3 CRectangle r1; CRectangle \u0026amp;rr = r1; rr.w = 5; 引用 引用名是对象名的别名, 指向同一个对象. 语法: 类名 \u0026amp;引用名 = 对象名; 引用的好处是可以减少指针的使用, 使得代码更加简洁.\n1 2 3 4 5 6 7 8 9 // 要用指针, 否则参数传递会产生拷贝 void swap(int *a, int *b) { int tmp; tmp = *a; *a = *b; *b = tmp; } int n1, n2; swap(\u0026amp;n1, \u0026amp;n2); 可以改写为:\n1 2 3 4 5 6 7 8 void swap(int \u0026amp;a, int \u0026amp;b) { int tmp; tmp = a; a = b; b = tmp; } int n1, n2; swap(n1, n2); 引用还可以作为函数的返回值.\n1 2 3 4 5 6 int \u0026amp;ref(int \u0026amp;a) { return a; } int n = 5; ref(n) = 6; // n = 6 常量, 常引用, 常量指针 定义引用时, 前面加 const 关键字, 表示 常引用. 不能通过常引用修改对象的值, 即只读引用.\n1 2 3 4 int n; const int \u0026amp;r = n; r = 5; // 错误 n = 5; // 正确 const T\u0026amp; 和 T\u0026amp; 是不同的类型. T\u0026amp; 类型的引用或 T 类型的变量可以用来初始化 const T\u0026amp; 类型的引用. const T 类型的常变量和 const T\u0026amp; 类型的引用则不能用来初始化 T\u0026amp; 类型的引用, 除非进行强制类型转换. 不可通过常量指针修改其指向的内容, 但可以修改指针的指向 (引用不可以).\n1 2 3 4 5 int n, m; const int *p = \u0026amp;n; *p = 5; // 错误 n = 5; // 正确 p = \u0026amp;m; // 正确 函数参数为常量指针时, 可避免函数内部不小心改变参数指针所指地方的内.\n1 2 3 4 void myPrintf(const int *p) { *p = 5; // 错误 p = \u0026amp;m; // 正确 } 类成员的访问控制 public: 公有成员, 可以在类的外部访问. private: 私有成员, 只能在类的内部访问, 缺省默认为 private. protected: 保护成员, 只能在类的内部和派生类中访问. 1 2 3 4 5 6 7 8 9 class className { // 这三个关键字可以出现多次, 没有顺序要求 private: // 私有属性和函数 public: // 公有属性和函数 protected: // 保护属性和函数 }; 类内部可以访问当前对象和同类其他对象的私有成员.\n\u0026ldquo;隐藏\u0026rdquo; 的目的是强制对成员变量的访问一定要通过成员函数进行, 那么以后成员变量的类 型等属性修改后, 只需要更改成员函数即可.\nstruct 和 class 的唯一区别是默认的访问控制权限不同, struct 默认为 public, class 默认为 private.\n函数重载和缺省参数 函数名相同, 参数个数或类型不同, 注意没有返回值类型不同.\n1 2 3 double _max(double f1, double f2); int _max(int n1, int n2); int _max(int n1, int n2, int n3); C++ 中, 定义函数的时候可以让 最右边 的连续若干个参数有缺省值, 那么调用函数的时候, 若相应位置不写参数, 参数就是缺省值.\n1 2 3 4 void func(int a, int b = 0, int c = 0); func(1); // a = 1, b = 0, c = 0 func(1, 2); // a = 1, b = 2, c = 0 func(1, , 8); // 错误 成员函数也可以重载或有缺省参数.\n构造函数 构造函数是一种特殊的成员函数: 名字与类名相同, 没有返回值 (void 也不行). 作用是初始化对象的数据成员.\n如果定义类时没有定义构造函数, 编译器会生成一个默认的无参构造函数. 如果定义了, 默认构造函数就不会生成. 对象生成时, 构造函数自动调用. 生成之后不能再执行构造函数. 一个类可以有多个构造函数, 可以重载.\n1 2 3 4 5 6 7 8 9 10 11 class Complex { private: double real; double imag; public: void Set(double r, double i); }; //编译器自动生成默认构造函数 // 两种写法均可. Complex c1; Complex *pc = new Complex; 手动加入构造函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Complex { private: double real; double imag; public: Complex(double r, double i = 0) { // 带缺省参数 real = r; imag = i; } }; Complex *pc1 = new Complex; // error, 没有参数 Complex c2(2); // OK Complex *pc2 = new Complex(3, 4); 构造当然也可以 private, 这样就不能用来生成对象, 但是可以用来实现单例模式.\n构造函数还可以用在数组.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class CSample { int x; public: CSample() {} CSample(int n) { x = n; } CSample(int m, int n) { x = m + n; } }; int main() { CSample array1[2]; // 两次无参 CSample array2[2] = {4, CSample(5, 3)}; // 两次有参 CSample array3[2] = {3}; // 一次有参一次无参 CSample *array4 = new CSample[2]; delete[] array4; return 0; } 复制构造函数只有一个参数, 且参数是本类的引用 (或常量引用). 如果没有定义, 编译器会生成一个默认的复制构造函数. 如果定义了, 默认复制构造函数就不会生成.\n1 2 3 4 5 6 7 8 class Complex { private: double real; double imag; }; Complex c1; Complex c2(c1); // 默认复制构造函数 1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real; double imag; Complex() {} // 必须要写, 否则编译器不会生成默认构造函数 Complex(const Complex \u0026amp;c) { real = c.real; imag = c.imag; cout \u0026lt;\u0026lt; \u0026#34;Copy Constructor called\u0026#34;; } }; Complex c1; Complex c2(c1); // 自定义复制构造函数 复制构造函数的调用时机:\n用一个对象去初始化另一个对象.\n1 2 Complex c1; Complex c2(c1); 一个对象作为函数参数传递给一个非引用类型的参数.\n1 2 3 void func(Complex c); Complex c1; func(c1); 一个对象作为函数返回值返回.\n1 2 Complex func(); Complex c1 = func(); 注意: 对象之间的赋值操作, 不会调用复制构造函数.\n1 2 Complex c1, c2; c1 = c2; // 不会调用复制构造函数 考虑到对象作为函数参数会掉用复制构造函数, 为了避免不必要的开销, 可以使用引用传递.\n手动写复制构造函数的目的一般是为了实现深拷贝.\n转换构造函数的目的是实现类型的自动转换. 不以说明符 explicit 声明 {且可以用单个参数调用 (C++11 前)} 的构造函数被称为转换构造函数.\n1 2 3 4 5 6 7 8 9 10 11 12 class Complex { public: double real, imag; Complex(int i) { // 类型转换构造函数 real = i; imag = 0; } }; int main() { Complex c1 = 9; // 隐式调用, 转换成一个临时 Complex 对象 return 0; } 如果加了 explicit 关键字, 则只能显式调用.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real, imag; explicit Complex(int i) { // 类型转换构造函数 real = i; imag = 0; } }; int main() { Complex c1 = 9; // 错误 Complex c2(9); // 正确 return 0; } 析构函数 析构函数是类的一个特殊成员函数, 名字由波浪号 ~ 加类名构成, 没有参数, 也没有返回值, 对象消亡时即自动被调用, 作用是释放对象所占用的资源.\n如果定义类时没写析构函数, 则编译器生成缺省析构函数.缺省析构函数什么也不做. 如果定义了析构函数, 缺省析构函数就不会生成. 一个类只有一个析构函数, 不能重载.\n在数组生命周期结束时, 编译器会自动调用数组中每个元素的析构函数. delete 一个对象时, 会调用对象的析构函数. (注意, new 数组要用 delete[])\n1 2 3 4 5 Ctest *pTest; pTest = new Ctest; // 构造函数调用 delete pTest; // 析构函数调用 pTest = new Ctest[3]; // 构造函数调用 3 次 delete[] pTest; // 析构函数调用 3 次 this 指针 this 是一个指向对象本身的指针. 把 car.foo() 翻译成 C 就是 foo(\u0026amp;car)\n1 2 3 4 5 6 7 8 9 class A { int i; public: void hello() { cout \u0026lt;\u0026lt; \u0026#34;hello\u0026#34; \u0026lt;\u0026lt; endl; } }; int main() { A *p = NULL; p-\u0026gt;hello(); } 能运行且输出, 但是是未定义行为. 而且一旦 hello() 中用到了 this, 就会出错.\n非静态成员函数中可以直接使用 this 来代表指向该函数作用的对象的指针.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Complex { public: double real, imag; void print() { cout \u0026lt;\u0026lt; real \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; imag; } Complex(double r, double i) : real(r), imag(i) {} Complex addOne() { this-\u0026gt;real++; // 等价于 real++; this-\u0026gt;print(); // 等价于 print() return *this; // 返回对象本身 } }; 静态成员 静态成员即加了 static 关键字的成员.\n静态成员变量是类的所有对象共享的. sizeof 不包括静态成员变量. 本质是全局变量. 静态成员函数是类的所有对象共享的函数, 静态成员函数只能访问静态成员变量和静态成员函数, 不能访问普通成员变量和普通成员函数, 也不可以用 this 指针. 本质是全局函数.\n访问静态成员可以不通过对象访问. 类名::静态成员名.\n1 2 int Rectangle::edges = 4; Rectangle::printTotal(); 即使类的对象不存在, 静态成员变量也存在.\n成员对象和封闭类 有成员对象的类叫封闭类.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CTyre { private: int radius; int width; public: CTyre(int r, int w) : radius(r), width(w) {} }; class CEngine {}; class CCar { private: int price; // 价格 CTyre tyre; CEngine engine; public: CCar(int p, int tr, int tw); }; CCar::CCar(int p, int tr, int w) : price(p), tyre(tr, w) {} 这个例子 CCar 必须有构造函数, 因为 CTyre 和没有默认构造函数.\n封闭类对象生成时, 先执行所有对象成员的构造函数, 然后才执行封闭类的构造函数. 对象成员的构造函数调用次序和对象成员在类中的说明次序一致, 与它们在成员初始化列表中出现的次序无关. 当封闭类的对象消亡时, 先执行封闭类的析构函数, 然后再执行成员对象的析构函数. 次序和构造函数的调用次序相反. 封闭类的对象, 如果是用默认复制构造函数初始化的, 那么它里面包含的成员对象也会用复制构造函数初始化. 友元 友元分为友元函数和友元类两种.\n友元函数: 一个类的友元函数可以访问该类的私有成员.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CCar; // 提前声明 CCar 类, 以便后面的 CDriver 类使用 class CDriver { public: void modifyCar(CCar *pCar); }; class CCar { private: int price; friend int mostExpensiveCar(CCar cars[], int total); // 声明友元 friend void CDriver::modifyCar(CCar *pCar); // 声明友元 // 可以将一个类的成员函数 (包括构造/析构函数) 说明为另一个类的友元。 }; void CDriver::modifyCar(CCar *pCar) { pCar-\u0026gt;price += 1000; } int mostExpensiveCar(CCar cars[], int total) { int tmpMax = -1; for (int i = 0; i \u0026lt; total; ++i) if (cars[i].price \u0026gt; tmpMax) tmpMax = cars[i].price; return tmpMax; } 如果 A 是 B 的友元类, 那么 A 的成员函数可以访问 B 的私有成员.\n1 2 3 4 5 6 7 8 9 10 11 12 class CCar { private: int price; friend class CDriver; // 声明 CDriver 为友元类 }; class CDriver { public: CCar myCar; void modifyCar() { myCar.price += 1000; // 因 CDriver 是 CCar 的友元类, 故此处可以访问其私有成员 } }; 友元类之间的关系不能传递, 不能继承.\n常量对象, 常量成员函数 如果不希望某个对象的值被改变, 则定义该对象的时候可以在前面加 const 关键字变为常量对象.\n在类的成员函数说明后面可以加 const 关键字, 则该成员函数成为常量成员函数. 常量成员函数内部不能改变属性的值, 也不能调用非常量成员函数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Sample { private: int value; public: void func() {}; Sample() {} void SetValue() const { value = 0; // wrong func(); // wrong } }; const Sample Obj; Obj.SetValue(); // 常量对象上可以使用常量成员函数 对于\n1 2 3 4 5 6 int getValue() const { return n; } int getValue() { return 2 * n; } 两个函数, 名字和参数表都一样, 但是一个是 const, 一个不是, 算重载.\n加上 mutable 关键字的成员变量, 即使在常量成员函数中也可以被修改.\n1 2 3 4 5 6 7 8 9 10 class CTest { public: bool getData() const { m_n1++; return m_b2; } private: mutable int m_n1; bool m_b2; }; ","date":"2025-02-19T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/program-practice/class-and-object/","title":"程序设计实习(1) —— 类和对象"},{"content":"本节课件链接\n基础数学工具 定义\n随机变量 $X$ 的 期望 $E[X]$ 定义为\n$$ E[X] = \\sum_{x} x \\cdot P(X=x) $$随机变量 $X$ 的 方差 $\\text{Var}(X)$ 定义为\n$$ \\text{Var}(X) = E[(X - E[X])^2] $$标准差 $\\sigma(X)$ 定义为\n$$ \\sigma(X) = \\sqrt{\\text{Var}(X)} $$ 定理Markov 不等式\n设 $X$ 是一个非负随机变量, 期望存在, 那么对于任意 $t \u003e 0$ 有\n$$ P(X \\geq t) \\leq \\frac{E[X]}{t} $$ 定理Chebyshev 不等式\n设 $X$ 是一个随机变量, 期望和方差都存在, 那么对于任意 $t \u003e 0$ 有\n$$ P(|X - E[X]| \\geq t) \\leq \\frac{\\text{Var}(X)}{t^2} $$ 定义\n随机变量 $X$ 和 $Y$ 的 协方差 $\\text{Cov}(X, Y)$ 定义为\n$$ \\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] $$如果 $\\text{Cov}(X, Y) = 0$, 则称 $X$ 和 $Y$ 不相关.\n协方差具有对称性, 双线性.\n定义\n随机向量 $X=(X_1, X_2, \\ldots, X_n)$ 的 协方差矩阵 $C(X)$ 定义为\n$$ C(X) = E[(X - E[X])(X - E[X])^T] = (\\text{Cov}(X_i, X_j))_{ij} $$ 定义\nGauss 分布 (正态分布) 的概率密度函数为\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) $$Laplace 分布 的概率密度函数为\n$$ f(x) = \\frac{1}{2b} \\exp(-\\frac{|x-\\mu|}{b}) $$ 最优化问题\n$$ \\begin{aligned} \u0026 \\min f(x) \\\\ \\text{s.t. } \u0026 c_i(x) \\leq 0, i = 1, 2, \\dots, k \\\\ \u0026 h_j(x) = 0, j = 1, 2, \\dots, l \\end{aligned} $$构造 Lagrange 函数\n$$ L(x, \\alpha, \\beta) = f(x) + \\sum_{i=1}^{k} \\alpha_i c_i(x) + \\sum_{j=1}^{l} \\beta_j h_j(x) $$引入 Karush-Kuhn-Tucker (KKT) 条件\n$$ \\begin{aligned} \u0026 \\nabla_x L(x, \\alpha, \\beta) = 0 \\\\ \u0026 c_i(x) \\leq 0, i = 1, 2, \\dots, k \\\\ \u0026 h_j(x) = 0, j = 1, 2, \\dots, l \\\\ \u0026 \\alpha_i c_i(x) = 0, i = 1, 2, \\dots, k \\\\ \u0026 \\alpha_i \\geq 0, i = 1, 2, \\dots, k \\end{aligned} $$基本概念和术语 定义\n监督学习: 基于标记数据 $T=\\{ (x_i,y_i) \\}_{i=1}^N$, 学习一个从输入空间到输出空间的映射 $f: \\mathcal{X} \\mapsto \\mathcal{Y}$. 利用此对未见数据进行预测. 通常分为 回归 和 分类 两类.\n无监督学习: 基于未标记数据 $T=\\{ x_i \\}_{i=1}^N$, 发现其中隐含的知识模式. 聚类 是典型的无监督学习任务.\n半监督学习: 既有标记数据又有未标记数据 (通常占比较大).\n强化学习: 通过观察环境的反馈, 学习如何选择动作以获得最大的奖励.\n模型评估与选择 损失函数 模型基于算法按照一定策略给出假设 $h \\in \\mathcal{H}$, 通过 损失函数 $L(h(x), y)$ 衡量假设的好坏.\n0-1 损失函数: $$L(h(x), y) = \\mathbb{I}(h(x) \\neq y) = \\begin{cases} 0, \u0026 h(x) = y \\\\ 1, \u0026 h(x) \\neq y \\end{cases}$$ 平方损失函数: $$L(h(x), y) = (h(x) - y)^2$$平均损失 $R(h) = E_{x \\sim D} [L(h(x), y)]$ 称为 泛化误差.\n容易验证, 对于 0-1 损失函数, 准确率 $a = 1-R(h)$.\n二分类 对于二分类问题, 样本预测结果有四种情况:\n真正例 (True Positive, TP): 预测为正例, 实际为正例. 假正例 (False Positive, FP): 预测为正例, 实际为负例. 真负例 (True Negative, TN): 预测为负例, 实际为负例. 假负例 (False Negative, FN): 预测为负例, 实际为正例. 由此引入\n准确率(查准率): $P = \\frac{TP}{TP+FP}$. 召回率(查全率): $R = \\frac{TP}{TP+FN}$. $F_1$ 度量: 考虑到二者抵触, 引入调和均值 $F_1 = \\frac{2PR}{P+R}$. 过拟合和正则化 为了防止由于模型过于复杂而导致的过拟合, 可以通过 正则化 方法来限制模型的复杂度.\n$$ \\min \\sum_{i=1}^{N} L(h(x_i), y_i) + \\lambda J(h) $$其中 $J(h)$ 是随着模型复杂度增加而增加的函数. $\\lambda$ 是正则化参数.\n怎么选取合适的 $\\lambda$ ? 一般是先给出若干候选, 在验证集上进行评估, 选取泛化误差最小的.\n数据集划分 一般将数据集划分为 训练集 $T$ 和 测试(验证)集 $T^\\prime$.\n留出法 (hold-out): 分层无放回地随机采样. 也叫简单交叉验证. $k$ 折交叉验证 ($k$-fold cross validation): 将数据集分为 $k$ 个大小相等的子集, 每次取其中一个作为验证集, 其余作为训练集, 最后以这 $k$ 次的平均误差作为泛化误差的估计. 当 $k=|D|$ 时称为留一 (leave-one-out) 验证法. 自助法 (bootstrapping): 从数据集中有放回地采样 $|D|$ 个数据作为训练集, 没抽中的作为验证集. 因而训练集 $T$ 和原始数据集 $D$ 的分布未必一致, 对数据分布敏感的模型不适用. 偏差-方差分解 为什么泛化误差会随着模型复杂度的增加而先减小后增大?\n定义\n偏差 (bias): 模型预测值的期望与真实值之间的差异. 体现了模型的拟合能力.\n$$\\text{Bias}(x) = E_T[h_T(x)-c(x)] = \\bar{h}(x) - c(x)$$方差 (variance): 模型预测值的方差. 体现了模型的对数据扰动的稳定性.\n$$\\text{Var}(x) = E[(h(x) - \\bar{h}(x))^2]$$ 现在对泛化误差进行分解:\n$$ \\begin{aligned} R(h) \u0026= E_T[(h_T(x) - c(x))^2] \\\\ \u0026= E_T[h_T^2(x) - 2h_T(x)c(x) + c^2(x)] \\\\ \u0026= E_T[h_T^2(x)] - 2c(x)E_T[h_T(x)] + c^2(x) \\\\ \u0026= E_T[h_T^2(x)] - \\bar{h}^2(x) + \\bar{h}^2(x) - 2\\bar{h}(x)c(x) + c^2(x) \\\\ \u0026= E_T[(h_T(x) - \\bar{h}(x))^2] + (\\bar{h}(x) - c(x))^2 \\\\ \u0026= \\text{Var}(x) + \\text{Bias}^2(x) \\end{aligned} $$当然, 由于噪声存在, $y$ 未必一定等于 $c(x)$, 不妨设 $y=c(x)+\\varepsilon$, 其中 $\\varepsilon \\sim \\Epsilon$ 期望为 $0$. 可以证明\n定理偏差-方差分解\n$$ E_{T \\sim D^{|T|}, \\varepsilon \\sim \\Epsilon} [(h_T(x)-y)^2] = \\text{Bias}^2(x) + \\text{Var}(x) + E[\\varepsilon^2] $$即泛化误差可以分解为偏差、方差和噪声三部分.\n起初, 模型较为简单, 偏差在泛化误差起主导作用. 随着模型复杂度的增加, 拟合能力增强, 偏差减小, 但带来过拟合风险, 算法对数据扰动敏感, 方差增大. 方差占比逐渐增大, 最终导致泛化误差增大.\n本节作业链接\n","date":"2025-02-18T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/machine-learning/intro/","title":"机器学习基础(1) —— 概述"},{"content":"本节课件链接\n凸优化 凸问题的可行集都是凸集.\n定理\n凸优化问题的任意局部极小点都是全局最优点.\n证明\n假设 $x$ 是局部极小, $y$ 全局最优且 $f(y) \u003c f(x)$.\n考虑 $z = \\theta x + (1-\\theta) y$, 则由于 $z$ 是可行点的凸组合, 也是可行点. 由于 $f$ 是凸函数, 有\n$$ f(z) \\leq \\theta f(x) + (1-\\theta) f(y) \u003c f(x) $$取 $\\theta \\to 1$, 则 $f(z) \\to f(x)$, 与局部最小性矛盾.\n线性规划 所谓 线性规划(LP) 问题是指目标函数和约束条件都是线性的优化问题. 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 c^T x \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 Gx \\le e \\end{aligned} $$$\\ell_1$ 和 $\\ell_\\infty$ 范数实际上也是线性的.\n凸多边形\n$$P = \\{ x \\mid a_i^Tx \\le b_i \\}$$的切比雪夫中心是最大半径内切球的中心. 代入得\n$$ \\sup \\{a_i^T (x_c + u) \\mid \\Vert u \\Vert_2 \\le r \\} = a_i^Tx_c + r \\Vert a_i \\Vert_2 \\le b_i $$这也变成了一个线性规划问题.\n二次规划 二次规划问题是指目标函数是二次的的优化问题.\n例如, 对于线性约束条件的问题, 一般形式如下:\n$$ \\begin{aligned} \\min \\quad \u0026 \\frac{1}{2} x^T P x + q^T x + r \\\\ \\text{s.t.} \\quad \u0026 Ax = b \\\\ \u0026 Gx \\le e \\end{aligned} $$也有 带二次约束的二次规划 (QCQP).\n我们归结为 二次锥规划 (SOCP):\n$$ \\begin{aligned} \\min \\quad \u0026 f^T x \\\\ \\text{s.t.} \\quad \u0026 \\Vert A_i x + b_i \\Vert_2 \\le c_i^T x + d_i, \\quad i = 1, \\ldots, m \\\\ \u0026 Fx = g \\end{aligned} $$最小范数问题: 令 $\\bar{v}_i = A_ix+b_i \\in \\mathbb{R}^{n_i}$, 则\n$\\min_x \\sum_i \\Vert \\bar{v}_i \\Vert_2$ 等价于 $$ \\begin{aligned} \\min \\quad \u0026 \\sum_i v_{i0} \\\\ \\text{s.t.} \\quad \u0026\\bar{v}_i = A_i x + b_i \\\\ \u0026(v_{i0}, \\bar{v}_i) \\succeq_\\mathcal{Q} 0 \\end{aligned} $$其中 $\\mathcal{Q}$ 是二次锥.\n最大 $k$ 范数和问题: 设 $\\Vert \\bar{v}_{[i]} \\Vert$ 是 $\\bar{v}_i$ 的非递增方式的排序. 则 $\\min_x \\sum_{i=1}^m \\Vert \\bar{v}_{[i]} \\Vert$ 等价于\n$$ \\begin{aligned} \\min \\quad \u0026 \\sum_{i=1}^m u_i + kt \\\\ \\text{s.t.} \\quad \u0026 \\bar{v}_i = A_i x + b_i \\\\ \u0026 \\Vert \\bar{v}_i \\Vert \\le u_i + t \\\\ \u0026 u_i \\ge 0 \\end{aligned} $$","date":"2025-02-15T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/opt-problem/","title":"最优化方法(4) —— 优化问题"},{"content":"本节课件链接\n基本线性代数知识 定义\n给定函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$, 且 $f$ 在 $x$ 一个邻域内有定义, 若存在 $g \\in \\mathbb{R}^n$, 使得\n$$ \\lim_{p \\to 0} \\frac{f(x+p)-f(x)-g^Tp}{\\Vert p \\Vert} = 0 $$其中 $\\Vert \\cdot \\Vert$ 是向量范数, 则称 $f$ 在 $x$ 处 可微. 此时, $g$ 称为 $f$ 在 $x$ 处的 梯度, 记为 $\\nabla f(x)$.\n显然, 如果梯度存在, 令 $p = \\varepsilon e_i$, 易得\n$$ \\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right) $$ 定义\n如果函数 $f(x): \\mathbb{R}^n \\mapsto \\mathbb{R}$ 在点 $x$ 处的二阶偏导数 $\\dfrac{\\partial^2 f}{\\partial x_i \\partial x_j}$ 存在, 则称 $f$ 在 $x$ 处 二次可微. 此时, $n \\times n$ 矩阵\n$$ \\nabla^2 f(x) = \\begin{pmatrix} \\dfrac{\\partial^2 f}{\\partial x_1^2} \u0026 \\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\dfrac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026 \\dfrac{\\partial^2 f}{\\partial x_2^2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\dfrac{\\partial^2 f}{\\partial x_n \\partial x_1} \u0026 \\dfrac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026 \\cdots \u0026 \\dfrac{\\partial^2 f}{\\partial x_n^2} \\end{pmatrix} $$称为 $f$ 在 $x$ 处的 Hessian 矩阵. 若 $\\nabla^2 f(x)$ 在 $D$ 上连续, 则称 $f$ 在 $D$ 上 二次连续可微.\n可以证明, 若 $f$ 在 $D$ 上二次连续可微, 则 $\\nabla^2 f(x)$ 为对称矩阵.\n多元函数的梯度可以推广到变量是矩阵的情形.\n定义\n给定函数 $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$, 且 $f$ 在 $X$ 一个邻域内有定义, 若存在 $G \\in \\mathbb{R}^{m \\times n}$, 使得\n$$ \\lim_{V \\to 0} \\frac{f(X+V)-f(X)-\\langle G, V \\rangle}{\\Vert V \\Vert} = 0 $$其中 $\\Vert \\cdot \\Vert$ 是矩阵范数, 则称 $f$ 在 $X$ 处 (Fréchet)可微. 此时, $G$ 称为 $f$ 在 $X$ 处的 梯度, 记为 $\\nabla f(X)$.\n矩阵的可微有另一种较为简单常用的定义.\n定义\n给定函数 $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$, 若存在矩阵 $G \\in \\mathbb{R}^{m \\times n}$, 使得\n$$ \\lim_{t \\to 0} \\frac{f(X+tV)-f(X)}{t} = \\langle G, V \\rangle $$则称 $f$ 在 $X$ 处 (Gâteaux)可微.\n例如:\n$f(X) = \\text{tr}(AX^TB)$, 此时 $\\nabla f(X) = BA$.\n$f(X, Y)=\\frac{1}{2} \\Vert XY-A \\Vert_F^2$. 此时\n$$ \\begin{aligned} \u0026f(X,Y+tV)-f(X,Y) \\\\ \u0026= \\frac{1}{2} \\Vert X(Y+tV)-A \\Vert_F^2 - \\frac{1}{2} \\Vert XY-A \\Vert_F^2 \\\\ \u0026= \\frac{1}{2} \\Vert XY - A + tVX \\Vert_F^2 - \\frac{1}{2} \\Vert XY - A \\Vert_F^2 \\\\ \u0026= \\frac{1}{2} \\Vert tVX \\Vert_F^2 + \\langle XY-A, tVX \\rangle \\\\ \u0026= t \\langle X^T(XY-A), V \\rangle + o(t) \\end{aligned} $$所以 $\\frac{\\partial f}{\\partial Y} = X^T(XY-A)$, 类似地, $\\frac{\\partial f}{\\partial X} = (XY-A)Y^T$.\n$f(X)=\\ln\\text{det}(X)$, $X$ 为正定矩阵. 此时\n$$ \\begin{aligned} \u0026f(X+tV)-f(X) \\\\ \u0026= \\ln\\text{det}(X+tV) - \\ln\\text{det}(X) \\\\ \u0026= \\ln\\text{det}(I+tX^{-1/2}VX^{-1/2}) \\end{aligned} $$考虑 $X^{-1/2}VX^{-1/2}$ 的特征值 $\\lambda_i$, 则由特征值之和为迹, 有\n$$ \\begin{aligned} \u0026= \\ln\\text{det}\\prod_{i=1}^n (1+t\\lambda_i) \\\\ \u0026= \\sum_{i=1}^n \\ln(1+t\\lambda_i) \\\\ \u0026= \\sum_{i=1}^n t\\lambda_i + o(t) \\\\ \u0026= t\\text{tr}(X^{-1/2}VX^{-1/2}) + o(t) \\\\ \u0026= t\\text{tr}(X^{-1}V) + o(t) \\\\ \u0026= t\\langle X^{-T}, V \\rangle + o(t) \\end{aligned} $$所以 $\\nabla f(X) = X^{-T}$.\n定义\n广义实数 是一种扩充实数域的数, 记为 $\\bar{\\mathbb{R}} = \\mathbb{R} \\cup \\{ \\pm \\infty \\}$. 映射 $f: \\mathbb{R}^n \\mapsto \\bar{\\mathbb{R}}$ 称为 广义实值函数.\n定义\n给定广义实值函数 $f$ 和非空集合 $X$. 如果存在 $x \\in X$ 使得 $f(x) \u003c +\\infty$, 并且对任意的 $x \\in X$, 都有 $f(x) \u003e -\\infty$, 那么称函数 $f$ 关于集合 $X$ 是 适当的．\n定义\n对于广义实值函数 $f: \\mathbb{R}^n \\mapsto \\bar{\\mathbb{R}}$,\n$C_\\alpha = \\{x \\mid f(x) \\le \\alpha \\}$ 称为 $f$ 的 $\\alpha$-下水平集. $\\text{epi} f = \\{ (x, t) \\mid f(x) \\le t \\}$ 称为 $f$ 的 上方图. 若 $\\text{epi} f$ 为闭集, 则称 $f$ 为闭函数. 若对任意的 $x \\in \\mathbb{R}^n$, 有 $\\liminf_{y \\to x} f(y) \\ge f(x)$, 则称 $f$ 为 下半连续函数. 定理\n对于广义实值函数 $f$, 以下命题等价:\n$f(x)$ 的任意 $\\alpha$-下水平集都是闭集; $f(x)$ 是下半连续的; $f(x)$ 是闭函数. 证明\n(1) $\\Rightarrow$ (2): 反证, 假设 $x_k \\to \\bar{x}$ 但 $\\liminf_{k \\to \\infty} f(x_k) \u003c f(\\bar{x})$. 取 $t$ 介于二者之间.\n考虑到 $\\liminf_{k \\to \\infty} f(x_k) \u003c t$, 则有无穷多 $x_k$ 使得 $f(x_k) \\le t$, 即这些 $x_k$ 在 $C_t$ 中. 由于 $C_t$ 是闭集, 则 $\\bar{x} \\in C_t$, 即 $f(\\bar{x}) \\le t$, 矛盾.\n(2) $\\Rightarrow$ (3): 考虑 $(x_k,y_k) \\in \\text{epi} f \\to (\\bar{x},\\bar{y})$, 由于 $f$ 下半连续, 则\n$$ f(\\bar{x}) \\le \\liminf_{k \\to \\infty} f(x_k) = \\liminf_{k \\to \\infty} y_k = \\bar{y} $$即 $(\\bar{x}, \\bar{y}) \\in \\text{epi} f$.\n(3) $\\Rightarrow$ (1): 考虑 $x_k \\in C_\\alpha \\to \\bar{x}$, 则 $(x_k, \\alpha) \\in \\text{epi} f \\to (\\bar{x}, \\alpha)$, 所以 $(\\bar{x}, \\alpha) \\in \\text{epi} f$, 即 $f(\\bar{x}) \\le \\alpha$, 所以 $\\bar{x} \\in C_\\alpha$.\n适当闭函数的和, 复合, 逐点上确界仍然是闭函数.\n凸函数 定义\n适当函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 凸函数, 如果 $\\text{dom} f$ 是凸集, 且对任意的 $x, y \\in \\text{dom} f$ 和 $\\theta \\in [0,1]$, 有\n$$ f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y) $$ 易知仿射函数既是凸函数又是凹函数. 所有的范数都是凸函数.\n定义\n若存在常数 $m \u003e 0$, 使得 $g(x) = f(x) - \\frac{m}{2} \\Vert x \\Vert^2$ 是凸函数, 则称 $f$ 是 强凸函数, $m$ 称为 强凸参数.\n定理凸函数判定定理\n适当函数 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 是凸函数的充要条件是, 对任意的 $x \\in \\text{dom} f$, 函数 $g: \\mathbb{R} \\mapsto \\mathbb{R}$ 是凸函数, 其中\n$$g(t) = f(x+tv), \\quad \\text{dom}g = \\{ t \\mid x + tv \\in \\text{dom} f \\}$$ 定理一阶条件\n对于定义在凸集上的可微函数 $f$, $f$ 是凸函数当且仅当\n$$ f(y) \\ge f(x) + \\nabla f(x)^T(y-x), \\quad \\forall x, y \\in \\text{dom} f $$证明\n必要性: 设 $f$ 凸, 则 $\\forall x, y \\in \\text{dom} f, t \\in [0,1]$, 有\n$$tf(y)+(1-t)f(x) \\ge f(x+t(y-x))$$令 $t \\to 0$, 即\n$$f(y)-f(x) \\ge \\frac{f(x+t(y-x))-f(x)}{t} \\to \\nabla f(x)^T(y-x)$$充分性: $\\forall x, y \\in \\text{dom}f, t\\in (0,1)$, 取 $z = tx+(1-t)y$, 则\n$$ \\begin{aligned} f(x) \u0026\\ge f(z) + \\nabla f(z)^T(x-z) \\\\ f(y) \u0026\\ge f(z) + \\nabla f(z)^T(y-z) \\end{aligned} $$一式乘以 $t$, 二式乘以 $1-t$, 相加即得.\n定理梯度单调性\n设 $f$ 为可微函数, 则 $f$ 为凸函数当且仅当 $\\text{dom} f$ 为凸集且 $\\nabla f$ 为单调映射.\n$$(\\nabla f(x) - \\nabla f(y))^T(x-y) \\ge 0$$证明\n必要性: 根据一阶条件, 有\n$$ \\begin{aligned} f(y) \u0026\\ge f(x) + \\nabla f(x)^T(y-x) \\\\ f(x) \u0026\\ge f(y) + \\nabla f(y)^T(x-y) \\end{aligned} $$相加即可.\n充分性: 考虑 $g(t)=f(x+t(y-x))$, 则 $g^\\prime(t)=\\nabla f(x+t(y-x))^T (y-x)$, 从而 $g^\\prime (t) \\ge g^\\prime (0)$.\n$$ \\begin{aligned} f(y) \u0026= g(1) = g(0) + \\int_{0}^1 g^\\prime(t) dt \\\\ \u0026\\ge g(0) + \\int_{0}^1 g^\\prime(0) dt = g(0) + g^\\prime(0) \\\\ \u0026= f(x) + \\nabla f(x)^T(y-x) \\end{aligned} $$ 定理\n函数 $f(x)$ 是凸函数当且仅当 $\\text{epi}f$ 是凸集.\n定理二阶条件\n设 $f$ 为定义在凸集上的二阶连续可微函数, $f$ 是凸函数当且仅当 $\\nabla^2 f(x) \\succeq 0, \\forall x \\in \\text{dom} f$. 若不取等, 则为严格凸函数.\n证明\n必要性: 反设 $f(x)$ 在 $x$ 处 $\\nabla^2 f(x) \\prec 0$, 则存在 $v \\in \\mathbb{R}^n$, 使得 $v^T \\nabla^2 f(x) v \u003c 0$, 考虑 Peano 余项\n$$ f(x+tv)=f(x)+t\\nabla f(x)^Tv+\\frac{t^2}{2}v^T\\nabla^2 f(x+tv)v + o(t^2) $$取 $t$ 充分小,\n$$ \\frac{f(x+tv)-f(x)-t\\nabla f(x)^T v}{t^2}=\\frac{1}{2}v^T\\nabla^2 f(x+tv)v + o(1) \u003c 0 $$这和一阶条件矛盾.\n充分性: 对于任意的 $x, y \\in \\text{dom} f$, 有\n$$ \\begin{aligned} f(y) \u0026= f(x)+\\nabla f(x)^T(y-x)+\\frac{1}{2}(y-x)^T\\nabla^2 f(z)(y-x) \\\\ \u0026\\ge f(x)+\\nabla f(x)^T(y-x) \\end{aligned} $$由一阶条件, $f$ 为凸函数.\n保凸运算 下面举一些重要的例子.\n逐点取上界: 若对每个 $y \\in A$, $f(x,y)$ 都是关于 $x$ 的凸函数, 则\n$$g(x)=\\sup_{y \\in A} f(x,y)$$也是凸函数.\n$C$ 的支撑函数 $f(x)=\\sup_{y \\in C} y^Tx$ 是凸函数. $C$ 到 $x$ 的最远距离 $f(x)=\\sup_{y \\in C} \\Vert x-y \\Vert$ 是凸函数. 对称阵 $X \\in \\mathbb{S}^n$ 的最大特征值 $\\lambda_{\\max}(X)=\\sup_{\\Vert x \\Vert=1} x^TXx$ 是凸函数. 标量函数的复合: 若 $g: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 是凸函数, $h: \\mathbb{R} \\mapsto \\mathbb{R}$ 是单调不减的凸函数, 则\n$$f(x) = h(g(x))$$也是凸函数. 凹同理.\n如果 $g$ 凸, 则 $f(x) = \\exp(g(x))$ 也是凸函数. 如果 $g$ 凹, 则 $f(x) = 1/g(x)$ 也是凸函数. 取下确界: 若 $f(x, y)$ 关于 $(x, y)$ 整体是凸函数, $C$ 是凸集, 则\n$$g(x) = \\inf_{y \\in C} f(x, y)$$也是凸函数.\n凸集 $C$ 到 $x$ 的距离 $f(x)=\\inf_{y \\in C} \\Vert x-y \\Vert$ 是凸函数. 透视函数: 若 $f: \\mathbb{R}^{n} \\mapsto \\mathbb{R}$ 是凸函数, 则\n$$g(x, t) = tf(x/t), \\quad \\text{dom} g = \\{ (x, t) \\mid x / t \\in \\text{dom} f, t \u003e 0 \\}$$也是凸函数.\n相对熵函数 $g(x,t)=t\\log t-t\\log x$ 是凸函数. 若 $f$ 凸, 则 $g(x)=(c^T+d)f((Ax+b)/(c^T+d))$ 也是凸函数. 共轭函数: 任意适当函数 $f$ 的共轭函数\n$$f^\\ast(y)=\\sup_{x \\in \\text{dom} f} (\\langle x, y \\rangle - f(x))$$是凸函数.\n凸函数的推广 定义\n$f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 拟凸的, 如果 $\\text{dom} f$ 是凸集, 且对任意 $\\alpha$, 下水平集 $C_\\alpha$ 是凸集.\n若 $f$ 是拟凸的, 则称 $-f$ 是 拟凹的. 若 $f$ 既是拟凸又是拟凹的, 则称 $f$ 是 拟线性的.\n注意: 拟凸函数的和不一定是拟凸函数.\n定理\n拟凸函数满足类 Jenson 不等式: 对拟凸函数 $f$ 和 $\\forall x, y \\in \\text{dom} f, \\theta \\in [0,1]$, 有 $$f(\\theta x + (1-\\theta)y) \\le \\max\\left\\{f(x),f(y)\\right\\}$$ 拟凸函数满足一阶条件: 定义在凸集上的可微函数 $f$ 拟凸当且仅当 $$f(y) \\le f(x) \\Rightarrow \\nabla f(x)^T(y-x) \\le 0$$ 定义\n如果正值函数 $f$ 满足 $\\log f$ 是凸函数, 则 $f$ 称为 对数凸函数; 若为凹函数, 则 $f$ 称为 对数凹函数.\n例如, 正态分布\n$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\text{det} \\Sigma}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$是对数凹函数.\n对数凹函数的乘积, 积分都是对数凹的, 但加和不一定是对数凹的.\n在广义不等式下, 也可以定义凸凹性.\n定义\n$$f(\\theta x+(1-\\theta)y \\preceq_K \\theta f(x)+(1-\\theta)f(y))$$ 对任意 $x,y \\in \\text{dom} f, 0 \\le \\theta \\le 1$ 成立.\n例如, $f: \\mathbb{S}^m \\mapsto \\mathbb{S}^m$, $f(X)=X^2$ 是 $\\mathbb{S}^m_+$-凸函数. 这点利用 $z^TX^2z=\\Vert Xz \\Vert^2$ 是关于 $X$ 的凸函数即可得知.\n","date":"2025-01-25T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-function/","title":"最优化方法(3) —— 凸函数"},{"content":"本节课件链接\n范数 定义\n记号 $\\Vert \\cdot \\Vert: \\mathbb{R}^n \\mapsto \\mathbb{R}$ 称为 向量范数, 若满足:\n正定性: $\\Vert x \\Vert \\geq 0$, 且 $\\Vert x \\Vert = 0 \\Leftrightarrow x = 0$; 齐次性: $\\Vert \\alpha x \\Vert = \\vert \\alpha \\vert \\Vert x \\Vert$; 三角不等式: $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$. $\\ell_p$ 范数是最常见的向量范数\n$$ \\Vert x \\Vert_p = \\left( \\sum_{i=1}^n \\vert x_i \\vert^p \\right) ^{\\frac{1}{p}} $$特别地, 当 $p = \\infty$ 时, $\\Vert x \\Vert_\\infty = \\max_i \\vert x_i \\vert$.\n向量范数可以自然地推广到矩阵范数. 常见的矩阵范数有:\n和范数: $\\Vert A \\Vert_1 = \\sum_{i,j} \\vert A_{ij} \\vert$; Frobenius 范数: $\\Vert A \\Vert_F = \\sqrt{\\sum_{i,j} A_{ij} ^2} = \\sqrt{\\text{tr}(A^T A)}$; 算子范数: $\\Vert A \\Vert_{(m,n)}=\\max_{\\Vert x \\Vert_n = 1} \\Vert Ax \\Vert_m$. 特别地, 当 $m = n = p$ 时: $p=1$ 时, $\\Vert A \\Vert_{p=1} = \\max_j \\sum_i \\vert A_{ij} \\vert$; $p=2$ 时, $\\Vert A \\Vert_{p=2} = \\sqrt{\\lambda_{\\max}(A^T A)}$, 亦称为 谱范数. $p=\\infty$ 时, $\\Vert A \\Vert_{p=\\infty} = \\max_i \\sum_j \\vert A_{ij} \\vert$. 核范数: $\\Vert A \\Vert_\\ast = \\sum_i \\sigma_i$, 其中 $\\sigma_i$ 为 $A$ 的奇异值. 定理Cauchy 不等式\n$$\\vert \\langle X, Y \\rangle \\vert \\leq \\Vert X \\Vert \\Vert Y \\Vert$$等号成立当且仅当 $X$ 与 $Y$ 线性相关.\n凸集 定义\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in \\mathbb{R}$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 仿射集.\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in [0, 1]$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 凸集.\n换言之, 仿射集要求过任意两点的直线都在集合内, 而凸集要求过任意两点的线段都在集合内. 显然, 仿射集都是凸集. 线性方程组的解集是一个仿射集, 而线性规划问题的可行域是一个凸集. 可以证明, 仿射集均可表示为某个线性方程组的解集.\n定理\n若 $S$ 是凸集, 则 $kS = \\left\\{ ks \\mid k \\in \\mathbb{R}, s \\in S \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S + T = \\left\\{ s + t \\mid s \\in S, t \\in T \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S \\cap T$ 也是凸集. 凸集的内部和闭包均为凸集. 可以证明, 任意多个凸集的交集仍为凸集.\n定义\n形如 $x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_kx_k$, 其中 $\\theta_i \\geq 0$ 且 $\\sum_i \\theta_i = 1$, 的表达式称为 $x$ 的 凸组合. 集合 $S$ 的所有点的凸组合构成的集合称为 $S$ 的 凸包, 记为 $\\text{conv}(S)$.\n定理\n若 $\\text{conv} S \\subseteq S$, 则 $S$ 为凸集, 反之亦然.\n证明\n先证明正方向. 对任意 $x,y \\in S, \\theta \\in [0,1]$, 有 $\\theta x + (1-\\theta) y \\in \\text{conv} S \\subseteq S$, 故 $S$ 为凸集.\n再证明反方向, 对凸组合的维数 $k$ 采用数学归纳法证明之.\n若 $k=1$, 显然成立. 假设对于 $k-1$ 成立, 则对于 $k$, 考虑\n$$ \\begin{aligned} x \u0026= \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_k x_k \\\\ \u0026= (1-\\theta_k)\\left(\\frac{\\theta_1}{1-\\theta_k} x_1 + \\frac{\\theta_2}{1-\\theta_k} x_2 + \\cdots + \\frac{\\theta_{k-1}}{1-\\theta_k} x_{k-1}\\right) + \\theta_k x_k \\end{aligned} $$前面大括号内的表达式为 $k-1$ 个凸组合, 故在 $S$ 中. 于是 $x$ 又成为两个点的凸组合, 由于 $S$ 为凸集, 故 $x \\in S$. 则 $\\text{conv} S \\subseteq S$.\n定理\n$\\text{conv}S$ 是包含 $S$ 的最小凸集; $\\text{conv}S$ 是所有包含 $S$ 的凸集的交集. 证明\n显然第一个是第二个的推论, 只证明第二个.\n已知凸集的交是凸集, 从而所有包含 $S$ 的凸集的交集 $X$ 是凸集. 且 $\\text{conv} S$ 是包含 $S$ 的凸集, 则 $X \\subseteq \\text{conv} S$.\n另一方面, $S \\subseteq X$, 则 $\\text{conv} S \\subseteq \\text{conv}X$, 而 $X$ 是凸集, 则 $\\text{conv}X = X$, 即 $\\text{conv}S \\subseteq X$. 综上, $\\text{conv}S = X$.\n仿照凸组合和凸包, 也可以定义仿射组合和仿射包 $\\text{affine} S$, 不再赘述.\n定义\n形如 $x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_kx_k$, 其中 $\\theta_i \\geq 0$ 的表达式称为 $x$ 的 锥组合. 若集合 $S$ 中任意点的锥组合都在 $S$ 中, 则称 $S$ 为凸锥.\n常见凸集 定义\n任取非零向量 $a\\in \\mathbb{R}^n$, 形如\n$$ \\left\\{ x \\mid a^Tx =b \\right\\} $$的集合称为 超平面, 形如\n$$ \\left\\{ x \\mid a^Tx \\le b \\right\\} $$的集合称为 半空间.\n定义\n满足线性等式和不等式组的点的集合称为 多面体, 即\n$$ \\left\\{x \\mid Ax \\le b, Cx = d\\right\\} $$其中 $A \\in \\mathbb{R}^{m \\times n}, C \\in \\mathbb{R}^{p \\times n}$.\n定义\n对中心 $x_c$ 和半径 $r$, 形如\n$$ B(x_c, r) = \\left\\{ x \\mid \\Vert x - x_c \\Vert \\le r \\right\\} = \\left\\{ x_c + ru \\mid \\Vert u \\Vert \\le 1 \\right\\} $$的集合称为 球.\n对中心 $x_c$ 和对称正定矩阵 $P$, 非奇异矩阵 $A$, 形如\n$$ \\left\\{ x \\mid (x-x_c)^TP(x-x_c) \\le 1 \\right\\} = \\left\\{ x_c + Au \\mid \\Vert u \\Vert \\le 1 \\right\\} $$的集合称为 椭球.\n定义\n形如\n$$ \\left\\{(x,t) \\mid \\Vert x \\Vert \\le t \\right\\} $$的集合称为 (范数)锥.\n保凸运算 定理\n仿射运算保凸, 即对 $f(x)=Ax+b$, 则凸集在 $f$ 下的像是凸集, 凸集在 $f$ 下的原像是凸集.\n考虑双曲锥\n$$ \\left\\{ x \\mid x^TPx \\le \\left( c^Tx \\right)^2, c^Tx \\ge 0, P \\in \\mathbb{S}_+^n \\right\\} $$$\\mathbb{S}_+^n$ 表示半正定矩阵. 双曲锥可以表示为二阶锥\n$$ \\left\\{ x \\mid \\Vert Ax \\Vert_2 \\le c^Tx, c^Tx \\ge 0, A^TA = P \\right\\} $$这个可以由二次范数锥得到.\n透视变换 $P: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n$:\n$$ P(x,t) = \\frac{x}{t}, \\quad \\text{dom} P = \\left\\{ (x,t) \\mid t \u003e 0 \\right\\} $$保凸.\n分式线性变换 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}^m$:\n$$ f(x) = \\frac{Ax+b}{c^Tx+d}, \\quad \\text{dom} f = \\left\\{ x \\mid c^Tx+d \u003e 0 \\right\\} $$保凸.\n广义不等式和对偶锥 定义\n我们称一个凸锥 $K \\subseteq \\mathbb{R}^n$ 为 适当锥, 当其还满足\n$K$ 是闭集; $K$ 是实心的, 即 $\\text{int} K \\neq \\emptyset$; $K$ 是尖的, 即内部不包含直线: 若 $x \\in \\text{int} K, -x \\in \\text{int} K$. 则一定有 $x = 0$. 例如\n非负卦限 $K=\\mathbb{R}_+^n=\\left\\{ x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\right\\}$ 是适当锥. 半正定锥 $K=\\mathbb{S}_+^n$ 是适当锥. $[0,1]$ 上的有限非负多项式 $K=\\left\\{ x \\in \\mathbb{R}^n \\mid x_1 + x_2t + \\cdots + x_nt^{n-1} \\ge 0, t \\in [0,1] \\right\\}$ 是适当锥. 可以在 适当锥 上定义广义不等式.\n定义\n对于适当锥 $K$ , 定义偏序 广义不等式 为\n$$x \\preceq_K y \\Leftrightarrow y - x \\in K$$严格版本:\n$$x \\prec_K y \\Leftrightarrow y - x \\in \\text{int} K$$ 广义不等式是一个偏序关系, 具有自反性, 反对称性, 传递性, 可加性, 非负缩放性, 不再赘述.\n定义\n令锥 $K$ 为全空间 $\\Omega$ 的子集, 则 $K$ 的 对偶锥 为\n$$ K^\\ast = \\left\\{ y \\mid \\langle x, y \\rangle \\ge 0, \\forall x \\in K \\right\\} $$ 例如\n非负卦限是自对偶锥. 半正定锥是自对偶锥. 定理\n设 $K$ 是一锥, $K^\\ast$ 是其对偶锥, 则满足:\n$K^\\ast$ 是锥 (即使 $K$ 不是锥); $K^\\ast$ 是凸且闭的; 若 $\\text{int} \\neq \\emptyset$, 则 $K^\\ast$ 是尖的. 若 $K$ 是尖的, 则 $\\text{int} K^\\ast \\neq \\emptyset$. 若 $K$ 是适当锥, 则 $K^\\ast$ 是适当锥. $K^{\\ast\\ast}$ 是 $K$ 的凸包. 特别地, 若 $K$ 是凸且闭的, 则 $K^\\ast=K$. 适当锥的对偶锥仍是适当锥, 则适当锥 $K$ 的对偶锥 $K^\\ast$ 也可以诱导广义不等式.\n定义\n对于适当锥 $K$, 定义其对偶锥 $K^\\ast$ 上的 对偶广义不等式 为:\n$$x \\preceq_{K^\\ast} y \\Leftrightarrow y - x \\in K^\\ast$$其满足\n$x \\preceq_{K} y \\Leftrightarrow \\lambda^Tx \\le \\lambda^Ty, \\forall \\lambda \\succeq_{K^\\ast} K^\\ast$. $y \\succeq_{K^\\ast} 0 \\Leftrightarrow y^Tx \\ge 0, \\forall x \\succeq_K 0$. 分离超平面定理 定理分离超平面定理\n如果 $C$ 和 $D$ 是不相交的凸集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 分开, 即存在 $a \\neq 0, b$ 使得\n$$ \\begin{aligned} a^Tx \u0026\\le b, \\quad \\forall x \\in C \\\\ a^Tx \u0026\\ge b, \\quad \\forall x \\in D \\end{aligned} $$ 简要想法是找距离最近的一对点, 以这两点的中点为中心, 以两点的连线为法向量构造超平面.\n定理严格分离定理\n如果 $C$ 和 $D$ 是不相交的凸集, 且 $C$ 是闭集, $D$ 是紧集, 则存在一个超平面 $H$ 将 $C$ 和 $D$ 严格分开, 即存在 $a \\neq 0, b$ 使得\n$$ \\begin{aligned} a^Tx \u0026\\lt b, \\quad \\forall x \\in C \\\\ a^Tx \u0026\\gt b, \\quad \\forall x \\in D \\end{aligned} $$ 定义\n给定集合 $C$ 和边界点 $x_0$, 如果 $a\\ne 0$ 满足 $a^Tx \\le a^T x_0, \\forall x \\in C$, 则称\n$$ \\left\\{ x \\mid a^Tx = a^T x_0 \\right\\} $$为 $C$ 的 支撑超平面.\n由分离超平面的特殊情况 ($D$ 为单点集) 可以得到支撑超平面的存在性.\n定理支撑超平面定理\n若 $C$ 是凸集, 则 $C$ 的任意边界点处存在支撑超平面.\n","date":"2025-01-16T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-set/","title":"最优化方法(2) —— 凸集"},{"content":"本节课件链接\n概要 最优化问题的一般形式:\n$$ \\begin{aligned} \\min_{x} \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 g_i(x) \\leq 0, \\quad i = 1, 2, \\ldots, m \\\\ \u0026 h_j(x) = 0, \\quad j = 1, 2, \\ldots, p \\end{aligned} $$稀疏优化 考虑线性方程组 $Ax = b$, 优化函数 $\\min_{x \\in R^n} {\\Vert x \\Vert}_0, {\\Vert x \\Vert}_1, {\\Vert x \\Vert}_2$, 分别指代 $x$ 的非零元个数, $l_1, l_2$ 范数. LASSO(least absolute shrinkage and selection operator) 问题:\n$$ \\min_{x \\in \\mathbb{R}^n} \\mu {\\Vert x \\Vert}_1 + \\frac{1}{2} {\\Vert Ax - b \\Vert}_2^2 $$低秩矩阵优化 考虑矩阵 $M$, 希望 $X$ 在描述 $M$ 有效特征元素的同时, 尽可能保证 $X$ 的低秩性质. 低秩矩阵问题:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\text{rank}(X) \\quad \\text{s.t.} \\quad X_{ij} = M_{ij}, \\quad (i, j) \\in \\Omega $$核范数 ${\\Vert X \\Vert}_*$ 为所有奇异值的和. 也有二次罚函数的形式:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\mu {\\Vert X \\Vert}_* + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} (X_{ij} - M_{ij})^2 $$对于低秩情形, $X=LR^T$, 其中 $L \\in \\mathbb{R}^{m \\times r}, R \\in \\mathbb{R}^{n \\times r}$, $r \\ll m,n$ 为秩. 优化问题可写为:\n$$ \\min_{L,R} \\alpha {\\Vert L \\Vert}^2_F + \\beta {\\Vert R \\Vert}^2_F + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} ([LR^T]_{ij} - M_{ij})^2 $$引入正则化系数 $\\alpha, \\beta$ 来消除 $L,R$ 在常数缩放下的不确定性.\n深度学习 机器学习的问题通常形如\n$$ \\min_{x \\in W} \\frac{1}{N} \\sum_{i=1}^N \\ell(f(a_i, x), b_i) + \\lambda R(x) $$ 基本概念 定义\n设 $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$, $x \\in \\mathbb{R}^n$ 的可行区域为 $S$. 若存在一个邻域 $N(x)$, 使得 $\\forall x \\in N(x) \\cap S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的局部极小点. 若 $\\forall x \\in S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的全局极小点.\n大多数的问题是不能显式求解的, 通常要使用迭代算法.\n定义\n称算法是 Q-线性收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} \\le a, \\quad a \\in (0, 1) $$称算法是 Q-超线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 0 $$称算法是 Q-次线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 1 $$称算法是 Q-二次收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert^2}} \\le a, \\quad a \u003e 0 $$ 定义\n设 $x_k$ 是迭代算法产生的序列且收敛到 $x^\\ast$, 如果存在 Q-线性收敛于 $0$ 的非负序列 $t_k$, 且\n$$ \\Vert x_k - x^\\ast \\Vert \\le t_k $$则称 $x_k$ 是 R-线性收敛 的.\n一般来说, 收敛准则可以是\n$$ \\frac{f(x_k) - f^\\ast}{\\max\\left\\{\\left|f^\\ast \\right|, 1\\right\\}} \\le \\varepsilon $$也可以是\n$$ \\nabla f(x_k) \\le \\varepsilon $$如果有约束要求, 还要同时考虑到约束违反度. 对于实际的计算机算法, 会设计适当的停机准则, 例如\n$$ \\frac{{\\Vert x_{k+1} - x_k \\Vert}}{\\max\\left\\{\\Vert x_k \\Vert, 1\\right\\}} \\le \\varepsilon $$","date":"2025-01-12T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/intro/","title":"最优化方法(1) —— 简介"}]