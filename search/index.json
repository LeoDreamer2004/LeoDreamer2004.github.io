[{"content":" 本节课件链接\n范数 定义\n记号 $\\Vert \\cdot \\Vert: \\mathbb{R}^n \\to \\mathbb{R}$ 称为 向量范数, 若满足:\n正定性: $\\Vert x \\Vert \\geq 0$, 且 $\\Vert x \\Vert = 0 \\Leftrightarrow x = 0$; 齐次性: $\\Vert \\alpha x \\Vert = \\vert \\alpha \\vert \\Vert x \\Vert$; 三角不等式: $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$. $\\ell_p$ 范数是最常见的向量范数\n$$ \\Vert x \\Vert_p = \\left( \\sum_{i=1}^n \\vert x_i \\vert^p \\right) ^{\\frac{1}{p}} $$特别地, 当 $p = \\infty$ 时, $\\Vert x \\Vert_\\infty = \\max_i \\vert x_i \\vert$.\n向量范数可以自然地推广到矩阵范数. 常见的矩阵范数有:\n和范数: $\\Vert A \\Vert_1 = \\sum_{i,j} \\vert A_{ij} \\vert$; Frobenius 范数: $\\Vert A \\Vert_F = \\sqrt{\\sum_{i,j} A_{ij} ^2} = \\sqrt{\\text{tr}(A^T A)}$; 算子范数: $\\Vert A \\Vert_{(m,n)}=\\max_{\\Vert x \\Vert_n = 1} \\Vert Ax \\Vert_m$. 特别地, 当 $m = n = p$ 时: $p=1$ 时, $\\Vert A \\Vert_{p=1} = \\max_j \\sum_i \\vert A_{ij} \\vert$; $p=2$ 时, $\\Vert A \\Vert_{p=2} = \\sqrt{\\lambda_{\\max}(A^T A)}$, 亦称为 谱范数. $p=\\infty$ 时, $\\Vert A \\Vert_{p=\\infty} = \\max_i \\sum_j \\vert A_{ij} \\vert$. 核范数: $\\Vert A \\Vert_\\ast = \\sum_i \\sigma_i$, 其中 $\\sigma_i$ 为 $A$ 的奇异值. 定理 柯西不等式\n$$\\vert \\langle X, Y \\rangle \\vert \\leq \\Vert X \\Vert \\Vert Y \\Vert$$等号成立当且仅当 $X$ 与 $Y$ 线性相关.\n凸集 定义\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in \\mathbb{R}$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 仿射集.\n如果对于任意 $x, y \\in C$ 和 $\\theta \\in [0, 1]$, 都有 $\\theta x + (1-\\theta) y \\in C$, 则称 $C$ 为 凸集.\n换言之, 仿射集要求过任意两点的直线都在集合内, 而凸集要求过任意两点的线段都在集合内. 显然, 仿射集都是凸集. 线性方程组的解集是一个仿射集, 而线性规划问题的可行域是一个凸集. 可以证明, 仿射集均可表示为某个线性方程组的解集.\n定理\n若 $S$ 是凸集, 则 $kS = \\left\\{ ks|k \\in \\mathbb{R}, s \\in S \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S + T = \\left\\{ s + t|s \\in S, t \\in T \\right\\}$ 也是凸集; 若 $S, T$ 是凸集, 则 $S \\cap T$ 也是凸集. 凸集的内部和闭包均为凸集. 可以证明, 任意多个凸集的交集仍为凸集.\n定义\n集合 $S$ 的所有点的凸组合构成的集合称为 $S$ 的 凸包, 记为 $\\text{conv}(S)$.\n定理\n若 $\\text{conv} S \\subset S$, 则 $S$ 为凸集, 反之亦然.\n证明\n","date":"2025-01-16T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/convex-set/","title":"最优化方法(2) —— 凸集"},{"content":" 本节课件链接\n概要 最优化问题的一般形式:\n$$ \\begin{aligned} \\min_{x} \\quad \u0026 f(x) \\\\ \\text{s.t.} \\quad \u0026 g_i(x) \\leq 0, \\quad i = 1, 2, \\ldots, m \\\\ \u0026 h_j(x) = 0, \\quad j = 1, 2, \\ldots, p \\end{aligned} $$稀疏优化 考虑线性方程组 $Ax = b$, 优化函数 $\\min_{x \\in R^n} {\\Vert x \\Vert}_0, {\\Vert x \\Vert}_1, {\\Vert x \\Vert}_2$, 分别指代 $x$ 的非零元个数, $l_1, l_2$ 范数. LASSO(least absolute shrinkage and selection operator) 问题:\n$$ \\min_{x \\in \\mathbb{R}^n} \\mu {\\Vert x \\Vert}_1 + \\frac{1}{2} {\\Vert Ax - b \\Vert}_2^2 $$低秩矩阵优化 考虑矩阵 $M$, 希望 $X$ 在描述 $M$ 有效特征元素的同时, 尽可能保证 $X$ 的低秩性质. 低秩矩阵问题:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\text{rank}(X) \\quad \\text{s.t.} \\quad X_{ij} = M_{ij}, \\quad (i, j) \\in \\Omega $$核范数 ${\\Vert X \\Vert}_*$ 为所有奇异值的和. 也有二次罚函数的形式:\n$$ \\min_{X \\in \\mathbb{R}^{m \\times n}} \\mu {\\Vert X \\Vert}_* + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} (X_{ij} - M_{ij})^2 $$对于低秩情形, $X=LR^T$, 其中 +$L \\in \\mathbb{R}^{m \\times r}, R \\in \\mathbb{R}^{n \\times r}$, $r \\ll m,n$ 为秩. 优化问题可写为:\n$$ \\min_{L,R} \\alpha {\\Vert L \\Vert}^2_F + \\beta {\\Vert R \\Vert}^2_F + \\frac{1}{2} \\sum_{(i,j)\\in \\Omega} ([LR^T]_{ij} - M_{ij})^2 $$引入正则化系数 $\\alpha, \\beta$ 来消除 $L,R$ 在常数缩放下的不确定性.\n深度学习 机器学习的问题通常形如\n$$ \\min_{x \\in W} \\frac{1}{N} \\sum_{i=1}^N \\ell(f(a_i, x), b_i) + \\lambda R(x) $$ 基本概念 定义\n设 $f: \\mathbb{R}^n \\to \\mathbb{R}$, $x \\in \\mathbb{R}^n$ 的可行区域为 $S$. 若存在一个邻域 $N(x)$, 使得 $\\forall x \\in N(x) \\cap S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的局部极小点. 若 $\\forall x \\in S$, 有 $f(x^\\ast) \\leq f(x)$, 则称 $x^\\ast$ 为 $f$ 的全局极小点.\n大多数的问题是不能显式求解的, 通常要使用迭代算法.\n定义\n称算法是 Q-线性收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} \\le a, \\quad a \\in (0, 1) $$称算法是 Q-超线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 0 $$称算法是 Q-次线性收敛 的, 若对充分大的 $k$ 有\n$$ \\lim_{k \\to \\infty} \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert}} = 1 $$称算法是 Q-二次收敛 的, 若对充分大的 $k$ 有\n$$ \\frac{{\\Vert x_{k+1} - x^\\ast \\Vert}}{{\\Vert x_k - x^\\ast \\Vert^2}} \\le a, \\quad a \u003e 0 $$ 定义\n设 $x_k$ 是迭代算法产生的序列且收敛到 $x^\\ast$, 如果存在 Q-线性收敛于 $0$ 的非负序列 $t_k$, 且\n$$ \\Vert x_k - x^\\ast \\Vert \\le t_k $$则称 $x_k$ 是 R-线性收敛 的.\n一般来说, 收敛准则可以是\n$$ \\frac{f(x_k) - f^\\ast}{\\max\\left\\{\\left|f^\\ast \\right|, 1\\right\\}} \\le \\varepsilon $$也可以是\n$$ \\nabla f(x_k) \\le \\varepsilon $$如果有约束要求, 还要同时考虑到约束违反度. 对于实际的计算机算法, 会设计适当的停机准则, 例如\n$$ \\frac{{\\Vert x_{k+1} - x_k \\Vert}}{\\max\\left\\{\\Vert x_k \\Vert, 1\\right\\}} \\le \\varepsilon $$","date":"2025-01-12T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/intro/","title":"最优化方法(1) —— 简介"},{"content":"本系列是关于北大文再文老师最优化方法课程的随性笔记。\n导航\n最优化方法(1) —— 简介 最优化方法(2) —— 凸集 ","date":"2025-01-09T00:00:00Z","permalink":"https://LeoDreamer2004.github.io/p/opt-method/","title":"最优化方法"}]