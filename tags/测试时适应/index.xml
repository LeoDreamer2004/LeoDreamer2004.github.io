<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>测试时适应 on LeoDreamer</title>
        <link>https://LeoDreamer2004.github.io/tags/%E6%B5%8B%E8%AF%95%E6%97%B6%E9%80%82%E5%BA%94/</link>
        <description>Recent content in 测试时适应 on LeoDreamer</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>LeoDreamer</copyright>
        <lastBuildDate>Thu, 29 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://LeoDreamer2004.github.io/tags/%E6%B5%8B%E8%AF%95%E6%97%B6%E9%80%82%E5%BA%94/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>论文阅读 - 测试时强化学习</title>
        <link>https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/</link>
        <pubDate>Thu, 29 May 2025 00:00:00 +0000</pubDate>
        
        <guid>https://LeoDreamer2004.github.io/p/paper-reading/test-time-reinforcement-learning/</guid>
        <description>&lt;h2 id=&#34;测试时强化学习&#34;&gt;测试时强化学习
&lt;/h2&gt;&lt;p&gt;通常情况下, 深度学习模型在训练完成后就固定了参数, 在测试或部署阶段不再更新. 但在实际应用中, 测试数据可能与训练数据的分布存在差异, 导致模型性能下降. 因此后续的微调显得非常重要.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-def&#34;&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试时适应 (Test-time Adaption, TTA)&lt;/strong&gt; 算法指在不使用真实标签的前提下, 利用当前测试样本或其增强版本来在线微调模型, 使其更适应当前的输入分布.&lt;/p&gt;
&lt;p&gt;常见的测试时适应算法包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应批归一化&lt;/strong&gt;: 在测试阶段对批归一化层的均值和方差进行调整, 使其更适应当前输入分布, 同时不修改学习参数 &lt;code&gt;gamma&lt;/code&gt; 和 &lt;code&gt;beta&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;熵最小化&lt;/strong&gt;: 在测试阶段通过最小化模型输出的熵来提高模型的自信度.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id=&#34;基于熵最小化的强化学习&#34;&gt;基于熵最小化的强化学习
&lt;/h2&gt;&lt;p&gt;过往的 TTA 算法一般基于无监督学习, 即便是强化学习算法, 需要辛苦设计奖励函数, RLHF 需要人工标注数据, 成本高昂.&lt;/p&gt;
&lt;p&gt;论文 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 给出基于最小化熵的强化学习算法 (Reinforcement Learning via Entropy Minimization, RENT). 基于 GRPO 框架测试, 认为可以在 &lt;strong&gt;无监督&lt;/strong&gt; 的情况下, 把奖励函数设置为负熵, 只通过最小化输出的熵, 即可提高模型推理能力.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22660/x1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RENT&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;内部反馈的强化学习&#34;&gt;内部反馈的强化学习
&lt;/h2&gt;&lt;p&gt;除了 KL 正则化等等项之外, 我们关心奖励函数的设计. 这个奖励要与任务无关, 而由模型内部的反馈来决定. 与 &lt;a class=&#34;link cite-2&#34;&gt;[2]
    
&lt;/a&gt; 提出的负熵奖励不同, 论文 &lt;a class=&#34;link cite-3&#34;&gt;[3]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 给出了另一个置信度函数:&lt;/p&gt;
$$
S(o) = \frac{1}{|o|}\sum_{i=1}^{|o|}KL(U \| p_{\pi_{\theta}}(\cdot|o_{\lt i})) = -\frac{1}{|o| \cdot |V|} \sum_{i=1}^{|o|}\sum_{j=1}^{|V|} \log \left( |V| \cdot p_{\pi_{\theta}} (j|o_{\lt i}) \right)
$$&lt;p&gt;其中 $o$ 是 token 序列, $U$ 表示均匀分布.&lt;/p&gt;
&lt;h2 id=&#34;带有-clip-反馈的强化学习&#34;&gt;带有 CLIP 反馈的强化学习
&lt;/h2&gt;&lt;p&gt;对于一般任务, 传统的测试时适应算法要最小化熵, 但很显然这个方式容易陷入错误的模型预测中. 与带有反馈的学习模型相比, 监督微调模型有更好的泛化能力.&lt;/p&gt;
&lt;h3 id=&#34;clip&#34;&gt;CLIP
&lt;/h3&gt;&lt;p&gt;文章 &lt;a class=&#34;link cite-4&#34;&gt;[4]
    &lt;span class=&#34;material-name&#34;&gt;Unknown-material&lt;/span&gt; 
&lt;/a&gt; 通过引入 CLIP 反馈来解决置信度过高问题, 称为 RLCF(如下图).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arxiv.org/html/2305.18010/x1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;tpt-vs-rlcf&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;除了分类任务外, 通过特定任务的采样策略和适当的选择奖励基线, RLCF 可以很容易地扩展到不仅仅是检索这样的区分任务, 还可以扩展到图像字幕这样的泛化任务.&lt;/p&gt;
&lt;p&gt;我们现在关心视觉语言模型 (VLM), 因此要衡量跨模态的相似性. 论文 &lt;a class=&#34;link cite-6&#34;&gt;[6]
    
&lt;/a&gt; 提出了 &lt;strong&gt;对比语言-图像预训练 (Contrastive Language-Image Pre-training, CLIP)&lt;/strong&gt; 模型, 这个模型通过对图像和文本进行编码, 使得它们在同一个共享的向量空间中具有相似的表示.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;CLIP&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 图像 $v$ 和文本 $t$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 图像和文本的相似度分数 $s(v,t)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CLIP 训练两个编码器: 图像编码器 $g$ 和文本编码器 $h$.&lt;/li&gt;
&lt;li&gt;二者的输出分别为 $g(v)$ 和 $h(t)$.&lt;/li&gt;
&lt;li&gt;计算相似度分数, 常用的是余弦相似度:
$$s(v,t) = \frac{g(v) \cdot h(t)}{\|g(v)\| \|h(t)\|}$$&lt;/li&gt;
&lt;li&gt;返回相似度分数 $s(v,t)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h3 id=&#34;rlcf-算法&#34;&gt;RLCF 算法
&lt;/h3&gt;&lt;p&gt;对于 VLM, 训练集 $\mathcal{D}_\text{train}$ 和测试集 $\mathcal{D}_\text{test}$ 都是图像和文本对 $(v,t)$ 的集合. 需要注意, 算法的微调是在 &lt;strong&gt;单个&lt;/strong&gt; 测试样本上进行的.&lt;/p&gt;
&lt;p&gt;对于奖励函数 $R$, 我们希望学习到最好的概率分布 $f_{\theta}(v) = [p(t|v,\theta)]_{t \in T}$ 使得其能最大化奖励:&lt;/p&gt;
$$\max_{\theta} \mathbb{E}_{t \sim f_{\theta}(v)}R(t,v)$$&lt;p&gt;我们正式引入 &lt;strong&gt;带有 CLIP 反馈的强化学习 (Reinforcement Learning with CLIP Feedback, RLCF)&lt;/strong&gt; 算法.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;RLCF (分类任务)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 一个已经训练好的 VLM 模型 $f_{\theta}$, 测试样本 $v$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 微调后的模型 $f_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对测试样本 $v$ 进行数据增强, 生成多个增强样本 $\tau_i(v)$.&lt;/li&gt;
&lt;li&gt;按照 CLIP 的编码器编码 $v$ 和 $\tau_i(v)$, 计算当前模型的预测 $P(t|v,\theta)$. 注意此时训练文本应当是类似于 prompt + label 的形式, 如 &amp;ldquo;a photo of a cat&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;做置信度筛选, 只保留预测熵足够低的样本 $\tau_i(v)$. 在这些样本中, 按照 top-K 策略选择预测结果, 得到 K 对文本和图像 $(\tau_i(v), t_j)_{j=1}^K$. 暂记为 $(v,t)$ 以进行后续计算.&lt;/li&gt;
&lt;li&gt;按照先前的工作, 根据 CLIP 模型计算 CLIPScore:
$$
    \text{CLIP-S}(t,v) = w \times \max(\text{CLIP}(t,v), 0)
    $$
其中 $w=2.5$ 是一个常数.&lt;/li&gt;
&lt;li&gt;由于 CLIPScore 永远是非负的, 加入一个奖励基线增加稳定性:
$$
    R(t,v) = \text{CLIP-S}(t,v) - \mathbb{E}_{t&#39; \sim f_{\theta}(v)}[\text{CLIP-S}(t&#39;,v)]
    $$&lt;/li&gt;
&lt;li&gt;通过 REINFORCE 策略梯度更新模型参数 $\theta$ 为 $\theta&#39;$, 使得模型能够最大化奖励, 注意此时 &lt;strong&gt;只&lt;/strong&gt; 更新图像编码器 $g$ 的参数:
$$
    \nabla_{\theta} \mathbb{E}_{t \sim f_{\theta}(v)}[R(t,v)] = \mathbb{E}_{t \sim f_{\theta}(v)}[R(t,v) \nabla_{\theta} \log f_{\theta}(t|v, \theta)]
    $$&lt;/li&gt;
&lt;li&gt;返回微调后的模型 $f_{\theta&#39;}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://arxiv.org/html/2305.18010/x3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;rlcf-algo&#34;
	
	
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;相较于监督学习, 基于反馈的强化学习更加通用, 例如可以进行图像描述的任务.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;RLCF (图文转换)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;基本可以从上面的 RLCF 算法中直接泛化修改. 只需要注意如果是文本生成图片时, 应该固定图像编码编码器 $g$ 而微调文本编码器 $h$, 且此时不做数据增强.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arxiv.org/html/2305.18010/x4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;rlcf-algo-general&#34;
	
	
&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;技巧和变体&#34;&gt;技巧和变体
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;使用多个奖励模型及权重&lt;/strong&gt;: 默认情况下, 使用单个 CLIP-ViT-L/14. 可以使用多个 CLIP 模型, 并对它们的输出进行加权平均, 以获得更好的奖励信号.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;片段式测试时适应 (Episodic TTA)&lt;/strong&gt;: 假定模型泛化能力很强, 测试时只在测试集上微调, 随后丢弃重置为原参数 $\theta^*$, 防止污染大模型.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动量缓冲 (Momentum Buffer)&lt;/strong&gt;: 尽管片段式测试时适应确保可靠性, 但影响了模型增量学习能力. 因此引入一个动量缓冲, 在每次 TTA 中, 按照移动平均的方式更新缓冲 $\xi \leftarrow m\xi + (1-m)\theta$, 每经过若干次样本后, 再将缓冲 $\eta$ 作为新的参数 $\theta$ 进行更新.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验&#34;&gt;实验
&lt;/h3&gt;&lt;p&gt;RLCF 方法可以通用地建立在常用的架构上. 在零样本分类任务, 零样本图文检索和图像描述任务上, RLCF 都能显著提升模型的性能.&lt;/p&gt;
&lt;h2 id=&#34;引入协方差正则化的强化学习&#34;&gt;引入协方差正则化的强化学习
&lt;/h2&gt;&lt;p&gt;与论文 &lt;a class=&#34;link cite-4&#34;&gt;[4]
    
&lt;/a&gt; 不同, 论文 &lt;a class=&#34;link cite-5&#34;&gt;[5]
    
&lt;/a&gt; 通过熵动力学来研究熵崩溃的问题, 最终的目的依然是控制熵.&lt;/p&gt;
&lt;h3 id=&#34;熵崩溃&#34;&gt;熵崩溃
&lt;/h3&gt;&lt;p&gt;强化学习过程中对于高置信度的策略会愈发增强其使用概率, 导致熵变得更加降低. 以下图揭示了熵崩溃和性能饱和的关系. 当熵下降到某个阈值时, 性能会达到饱和点.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;熵崩溃和性能饱和&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;论文定量分析认为, 如果没有像熵损失或者 KL 散度这样的正则化, 下游性能完全可以通过策略熵来预测, 精确来说可以拟合成指数函数:&lt;/p&gt;
$$
R = -a \exp(\mathcal{H}) + b
$$&lt;p&gt;$R$ 是验证集的性能, $\mathcal{H}$ 是策略的熵.&lt;/p&gt;
&lt;h3 id=&#34;熵-性能函数&#34;&gt;熵-性能函数
&lt;/h3&gt;&lt;p&gt;这个函数可以用来分析模型的性能和熵之间的关系, 有几个特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;系数与算法无关&lt;/strong&gt;: 下面这个图几个算法得到的曲线是类似的, 这表明 $a,b$  可能是模型和数据的固有属性.
&lt;img src=&#34;https://arxiv.org/html/2505.22617/x11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;熵-性能函数&#34;
	
	
&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测不同模型的函数系数&lt;/strong&gt;: 显然 $a$ 是模型将熵转化为下游性能的速度. $−a+b$ 是当熵归零时模型可以达到的最大验证性能. 理论上个更大的性能应该对应更大的 $a$ 和 $b$. 此外不同的任务也会有不同的系数
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;系数 $a$&lt;/th&gt;
          &lt;th&gt;系数 $b$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;数学任务&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x12.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;a-math&#34;
	
	
&gt;&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x13.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;b-math&#34;
	
	
&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;代码任务&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x14.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;a-code&#34;
	
	
&gt;&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x15.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;b-code&#34;
	
	
&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结, 在策略熵减少过程中, 性能天花板不仅存在, 而且可以被预测.&lt;/p&gt;
&lt;h3 id=&#34;熵动力学&#34;&gt;熵动力学
&lt;/h3&gt;&lt;p&gt;我们主要关注相邻两次迭代的熵变化 $\mathcal{H}(\pi_{\theta}^{k+1}) - \mathcal{H}(\pi_{\theta}^{k})$.&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;策略梯度下的熵变化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令行为策略 $\pi_{\theta}$ 为一个 softmax 策略，并通过标准策略梯度更新，两个连续步骤中给定状态 $s$ 的策略熵之差满足:&lt;/p&gt;
$$
\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx -\eta \text{Cov}_{a \sim \pi_{\theta}^{k}(\cdot|s)} \left( \log \pi_{\theta}^{k}(a|s), \pi_{\theta}^k(a|s) \cdot A(s,a) \right)
$$&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-thm&#34;&gt;定理&lt;span class=&#34;math-subtitle&#34;&gt;自然策略梯度下的熵变化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令行为策略 $\pi_{\theta}$ 为一个 softmax 策略，并通过标准策略梯度更新，两个连续步骤中给定状态 $s$ 的策略熵之差满足:&lt;/p&gt;
$$
\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx -\eta \text{Cov}_{a \sim \pi_{\theta}^{k}(\cdot|s)} \left( \log \pi_{\theta}^{k}(a|s), A(s,a) \right)
$$&lt;/div&gt;
&lt;p&gt;揭示了当前策略下的动作概率 $P(a)$ 与相应的优势函数 $A(a)$ 之间的强正相关性. 作者做了实验验证了这个定理估计的正确性.&lt;/p&gt;
&lt;h3 id=&#34;协方差正则化&#34;&gt;协方差正则化
&lt;/h3&gt;&lt;p&gt;论文认为直接采用传统强化学习中的熵正则化技术难以解决 LLMs 的熵瓶颈问题, 过高的熵正则化甚至会导致熵爆炸.&lt;/p&gt;
&lt;p&gt;实验表明, 小部分 token 的协方差极高, 在触发熵崩溃中占据了主导地位. 受到 PPO 策略的启发, 论文提出两种协方差感知方法: &lt;strong&gt;Clip-Cov&lt;/strong&gt; 和 &lt;strong&gt;KL-Cov&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对于 token $y_i$ 的协方差, 定义为:&lt;/p&gt;
$$
\text{Cov}(y_i) = \left( \log \pi_{\theta}(y_i) - \mathbb{E}_{i \in [N]}\left[ \log \pi_{\theta}(y_i) \right] \right) \left(A(y_i) - \mathbb{E}_{i \in [N]}\left[A(y_i)\right]\right)
$$&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;Clip-Cov&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 策略 $\pi_{\theta}$, 协方差阈值 $\omega_l, \omega_h$ (两个都远超均值), 剔除比例 $r$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 更新后的策略 $\pi_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算每个 token 的协方差 $\text{Cov}(y_i)$.&lt;/li&gt;
&lt;li&gt;从 $y_i$ 中随机选取 $r \cdot N$ 个满足 $\omega_l \le \text{Cov}(y_i) \le \omega_h $ 的 token, 设索引集为 $I_{\text{clip}}$.&lt;/li&gt;
&lt;li&gt;将选择的这些 token 从策略梯度中移除, 其余仍然正常更新:
$$
    L_{\text{clip}}(\theta) = \begin{cases}
    \mathbb{E}\left[ \frac{\pi_{\theta&#39;}(y_i)}{\pi_{\theta}(y_i)} A(y_i) \right] &amp; \text{if } i \notin I_{\text{clip}} \\
    0 &amp; \text{if } i \in I_{\text{clip}}
    \end{cases}
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;math-block&#34;&gt;
    &lt;p class=&#34;math-block-title math-algo&#34;&gt;算法&lt;span class=&#34;math-subtitle&#34;&gt;KL-Cov&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;: 策略 $\pi_{\theta}$, 剔除比例 $k\ll 1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;: 更新后的策略 $\pi_{\theta&#39;}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算每个 token 的协方差 $\text{Cov}(y_i)$.&lt;/li&gt;
&lt;li&gt;从 $y_i$ 选取方差最大的 $k \cdot N$ 个 token, 设索引集为 $I_{\text{KL}}$.&lt;/li&gt;
&lt;li&gt;将选择的这些 token 在策略梯度中施加 KL 惩罚:
$$
    L_{\text{KL}}(\theta) = \begin{cases}
    \mathbb{E}\left[ \frac{\pi_{\theta&#39;}(y_i)}{\pi_{\theta}(y_i)} A(y_i) \right] &amp; \text{if } i \notin I_{\text{KL}} \\
    \mathbb{E}\left[ \frac{\pi_{\theta&#39;}(y_i)}{\pi_{\theta}(y_i)} A(y_i) \right] - \beta KL(\pi_{\theta}(y_i) || \pi_{\theta&#39;}(y_i)) &amp; \text{if } i \in I_{\text{KL}}
    \end{cases}
    $$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h3 id=&#34;实验-1&#34;&gt;实验
&lt;/h3&gt;&lt;p&gt;与一般的熵正则化方法相比, 协方差正则化方法在多个任务上都能显著提升模型性能. 且能一定程度上避免瓶颈问题.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;策略熵&lt;/th&gt;
          &lt;th&gt;LLM 响应长度&lt;/th&gt;
          &lt;th&gt;准确率&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Qwen-7B&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x22.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;7B-entropy&#34;
	
	
&gt;&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x23.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;7B-response&#34;
	
	
&gt;&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x24.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;7B-accuracy&#34;
	
	
&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Qwen-32B&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x25.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;32B-entropy&#34;
	
	
&gt;&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x26.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;32B-response&#34;
	
	
&gt;&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://arxiv.org/html/2505.22617/x27.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;32B-accuracy&#34;
	
	
&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        
    </channel>
</rss>
